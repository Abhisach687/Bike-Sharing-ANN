{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'\n",
    "data = pd.read_csv('hour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizations before one-hot encoding\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data['cnt'], kde=True)\n",
    "plt.title('Distribution of Bike Rentals')\n",
    "plt.xlabel('Bike Rentals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data, x='season', y='cnt')\n",
    "plt.title('Bike Rentals by Season')\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Bike Rentals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data, x='hr', y='cnt')\n",
    "plt.title('Bike Rentals by Hour')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Bike Rentals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "data = data.drop(['instant', 'dteday', 'casual', 'registered'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical variables\n",
    "data = pd.get_dummies(data, columns=['season', 'weathersit', 'mnth', 'hr', 'weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data.drop('cnt', axis=1))\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=data.drop('cnt', axis=1).columns)\n",
    "scaled_data['cnt'] = data['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(scaled_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to PyTorch tensors\n",
    "def to_tensor(df):\n",
    "    inputs = torch.tensor(df.drop('cnt', axis=1).values, dtype=torch.float32)\n",
    "    targets = torch.tensor(df['cnt'].values, dtype=torch.float32).view(-1, 1)\n",
    "    return inputs, targets\n",
    "\n",
    "X_train, y_train = to_tensor(train_data)\n",
    "X_val, y_val = to_tensor(val_data)\n",
    "X_test, y_test = to_tensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for RNN/LSTM/GRU [batch_size, seq_length, input_dim]\n",
    "X_train = X_train.view(X_train.size(0), 1, X_train.size(1))\n",
    "X_val = X_val.view(X_val.size(0), 1, X_val.size(1))\n",
    "X_test = X_test.view(X_test.size(0), 1, X_test.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super(RNNNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GRU model\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = X_train.size(2)\n",
    "num_epochs = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning function\n",
    "def train_model(model_class, hidden_dim, num_layers, learning_rate, dropout):\n",
    "    model = model_class(input_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train.to(device))\n",
    "        loss = criterion(outputs, y_train.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_outputs = model(X_val.to(device))\n",
    "        val_loss = criterion(val_outputs, y_val.to(device))\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_model(params, model_class):\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    for hidden_dim in params['hidden_dim']:\n",
    "        for num_layers in params['num_layers']:\n",
    "            for lr in params['learning_rate']:\n",
    "                for dropout in params['dropout']:\n",
    "                    print(f\"Trying params: hidden_dim={hidden_dim}, num_layers={num_layers}, lr={lr}, dropout={dropout}\")\n",
    "                    _, _, val_losses = train_model(model_class, hidden_dim, num_layers, lr, dropout)\n",
    "                    if val_losses[-1] < best_score:\n",
    "                        best_score = val_losses[-1]\n",
    "                        best_params = {'hidden_dim': hidden_dim, 'num_layers': num_layers, 'learning_rate': lr, 'dropout': dropout}\n",
    "                        print(f\"New best score: {best_score} with params: {best_params}\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning RNN Model\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68847.0703, Val Loss: 68593.4453\n",
      "Epoch [20/100], Train Loss: 67653.5469, Val Loss: 67379.7422\n",
      "Epoch [30/100], Train Loss: 66255.1484, Val Loss: 65944.4844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\Downloads\\Documents\\Bike Sharing ANN\\ann\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Train Loss: 64733.4961, Val Loss: 64391.3086\n",
      "Epoch [50/100], Train Loss: 63224.7773, Val Loss: 62867.1719\n",
      "Epoch [60/100], Train Loss: 61793.3008, Val Loss: 61428.5391\n",
      "Epoch [70/100], Train Loss: 60432.9180, Val Loss: 60063.9648\n",
      "Epoch [80/100], Train Loss: 59135.0000, Val Loss: 58761.6992\n",
      "Epoch [90/100], Train Loss: 57897.0977, Val Loss: 57519.3281\n",
      "Epoch [100/100], Train Loss: 56714.5664, Val Loss: 56332.4805\n",
      "New best score: 56332.48046875 with params: {'hidden_dim': 32, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68831.5312, Val Loss: 68584.8516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\Downloads\\Documents\\Bike Sharing ANN\\ann\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Train Loss: 67678.4688, Val Loss: 67409.6406\n",
      "Epoch [30/100], Train Loss: 66301.3203, Val Loss: 65995.8125\n",
      "Epoch [40/100], Train Loss: 64821.9922, Val Loss: 64488.2617\n",
      "Epoch [50/100], Train Loss: 63334.6914, Val Loss: 62981.5234\n",
      "Epoch [60/100], Train Loss: 61916.2578, Val Loss: 61555.2109\n",
      "Epoch [70/100], Train Loss: 60566.8164, Val Loss: 60200.8555\n",
      "Epoch [80/100], Train Loss: 59278.9961, Val Loss: 58907.7305\n",
      "Epoch [90/100], Train Loss: 58044.0742, Val Loss: 57667.0938\n",
      "Epoch [100/100], Train Loss: 56862.4805, Val Loss: 56480.7305\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68539.7031, Val Loss: 68280.1953\n",
      "Epoch [20/100], Train Loss: 67289.9688, Val Loss: 67015.6094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\Downloads\\Documents\\Bike Sharing ANN\\ann\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Train Loss: 65875.0391, Val Loss: 65560.0000\n",
      "Epoch [40/100], Train Loss: 64357.1953, Val Loss: 64010.8633\n",
      "Epoch [50/100], Train Loss: 62881.8867, Val Loss: 62525.1797\n",
      "Epoch [60/100], Train Loss: 61481.9883, Val Loss: 61121.1172\n",
      "Epoch [70/100], Train Loss: 60136.5664, Val Loss: 59773.0430\n",
      "Epoch [80/100], Train Loss: 58849.9375, Val Loss: 58484.2734\n",
      "Epoch [90/100], Train Loss: 57623.3438, Val Loss: 57254.3750\n",
      "Epoch [100/100], Train Loss: 56446.2344, Val Loss: 56072.3672\n",
      "New best score: 56072.3671875 with params: {'hidden_dim': 32, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.3}\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69489.2500, Val Loss: 69332.6797\n",
      "Epoch [20/100], Train Loss: 69415.1562, Val Loss: 69258.2266\n",
      "Epoch [30/100], Train Loss: 69335.3594, Val Loss: 69177.6016\n",
      "Epoch [40/100], Train Loss: 69245.1562, Val Loss: 69086.2734\n",
      "Epoch [50/100], Train Loss: 69141.7734, Val Loss: 68981.8359\n",
      "Epoch [60/100], Train Loss: 69024.8203, Val Loss: 68864.2500\n",
      "Epoch [70/100], Train Loss: 68895.1406, Val Loss: 68734.3984\n",
      "Epoch [80/100], Train Loss: 68753.6328, Val Loss: 68592.9922\n",
      "Epoch [90/100], Train Loss: 68601.1562, Val Loss: 68440.7422\n",
      "Epoch [100/100], Train Loss: 68439.3828, Val Loss: 68279.2812\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69479.6016, Val Loss: 69321.1875\n",
      "Epoch [20/100], Train Loss: 69402.7500, Val Loss: 69243.7578\n",
      "Epoch [30/100], Train Loss: 69315.6797, Val Loss: 69155.8438\n",
      "Epoch [40/100], Train Loss: 69213.8906, Val Loss: 69053.2031\n",
      "Epoch [50/100], Train Loss: 69096.4297, Val Loss: 68935.1953\n",
      "Epoch [60/100], Train Loss: 68964.6641, Val Loss: 68803.3281\n",
      "Epoch [70/100], Train Loss: 68821.0781, Val Loss: 68659.9766\n",
      "Epoch [80/100], Train Loss: 68667.9453, Val Loss: 68507.1953\n",
      "Epoch [90/100], Train Loss: 68507.1484, Val Loss: 68346.7656\n",
      "Epoch [100/100], Train Loss: 68340.6797, Val Loss: 68180.5781\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69468.0859, Val Loss: 69314.2344\n",
      "Epoch [20/100], Train Loss: 69392.1719, Val Loss: 69237.9297\n",
      "Epoch [30/100], Train Loss: 69307.7422, Val Loss: 69153.3359\n",
      "Epoch [40/100], Train Loss: 69210.5312, Val Loss: 69056.3359\n",
      "Epoch [50/100], Train Loss: 69098.1094, Val Loss: 68944.5000\n",
      "Epoch [60/100], Train Loss: 68970.3438, Val Loss: 68817.5625\n",
      "Epoch [70/100], Train Loss: 68828.7578, Val Loss: 68676.8984\n",
      "Epoch [80/100], Train Loss: 68675.6953, Val Loss: 68524.6562\n",
      "Epoch [90/100], Train Loss: 68513.7969, Val Loss: 68363.3828\n",
      "Epoch [100/100], Train Loss: 68345.7188, Val Loss: 68195.5547\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69585.5391, Val Loss: 69430.4141\n",
      "Epoch [20/100], Train Loss: 69582.9609, Val Loss: 69428.0469\n",
      "Epoch [30/100], Train Loss: 69581.8125, Val Loss: 69427.0469\n",
      "Epoch [40/100], Train Loss: 69581.4688, Val Loss: 69426.7344\n",
      "Epoch [50/100], Train Loss: 69581.3594, Val Loss: 69426.6406\n",
      "Epoch [60/100], Train Loss: 69581.3203, Val Loss: 69426.6094\n",
      "Epoch [70/100], Train Loss: 69581.3125, Val Loss: 69426.6016\n",
      "Epoch [80/100], Train Loss: 69581.2969, Val Loss: 69426.5938\n",
      "Epoch [90/100], Train Loss: 69581.2969, Val Loss: 69426.5938\n",
      "Epoch [100/100], Train Loss: 69581.2969, Val Loss: 69426.5938\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69460.6719, Val Loss: 69307.8281\n",
      "Epoch [20/100], Train Loss: 69457.9453, Val Loss: 69305.3125\n",
      "Epoch [30/100], Train Loss: 69457.0312, Val Loss: 69304.5469\n",
      "Epoch [40/100], Train Loss: 69456.7266, Val Loss: 69304.2812\n",
      "Epoch [50/100], Train Loss: 69456.6328, Val Loss: 69304.1953\n",
      "Epoch [60/100], Train Loss: 69456.5938, Val Loss: 69304.1641\n",
      "Epoch [70/100], Train Loss: 69456.5938, Val Loss: 69304.1641\n",
      "Epoch [80/100], Train Loss: 69456.5859, Val Loss: 69304.1641\n",
      "Epoch [90/100], Train Loss: 69456.5859, Val Loss: 69304.1641\n",
      "Epoch [100/100], Train Loss: 69456.5859, Val Loss: 69304.1641\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69537.6016, Val Loss: 69388.1016\n",
      "Epoch [20/100], Train Loss: 69533.6719, Val Loss: 69384.3672\n",
      "Epoch [30/100], Train Loss: 69532.0781, Val Loss: 69382.8984\n",
      "Epoch [40/100], Train Loss: 69531.5391, Val Loss: 69382.4297\n",
      "Epoch [50/100], Train Loss: 69531.3672, Val Loss: 69382.2812\n",
      "Epoch [60/100], Train Loss: 69531.3125, Val Loss: 69382.2266\n",
      "Epoch [70/100], Train Loss: 69531.3047, Val Loss: 69382.2109\n",
      "Epoch [80/100], Train Loss: 69531.2969, Val Loss: 69382.2109\n",
      "Epoch [90/100], Train Loss: 69531.2969, Val Loss: 69382.2109\n",
      "Epoch [100/100], Train Loss: 69531.2969, Val Loss: 69382.2109\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68465.1875, Val Loss: 68152.5234\n",
      "Epoch [20/100], Train Loss: 66916.0703, Val Loss: 66576.2344\n",
      "Epoch [30/100], Train Loss: 65470.9766, Val Loss: 65136.1562\n",
      "Epoch [40/100], Train Loss: 64097.0000, Val Loss: 63765.1250\n",
      "Epoch [50/100], Train Loss: 62816.7578, Val Loss: 62480.7812\n",
      "Epoch [60/100], Train Loss: 61594.5000, Val Loss: 61251.3359\n",
      "Epoch [70/100], Train Loss: 60425.3867, Val Loss: 60074.6211\n",
      "Epoch [80/100], Train Loss: 59302.9531, Val Loss: 58945.3906\n",
      "Epoch [90/100], Train Loss: 58221.0781, Val Loss: 57856.3125\n",
      "Epoch [100/100], Train Loss: 57162.8516, Val Loss: 56789.4375\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68214.3438, Val Loss: 67882.9922\n",
      "Epoch [20/100], Train Loss: 66601.6016, Val Loss: 66257.0547\n",
      "Epoch [30/100], Train Loss: 65130.8477, Val Loss: 64798.1016\n",
      "Epoch [40/100], Train Loss: 63790.5586, Val Loss: 63455.5742\n",
      "Epoch [50/100], Train Loss: 62525.0117, Val Loss: 62187.8398\n",
      "Epoch [60/100], Train Loss: 61314.9297, Val Loss: 60968.9023\n",
      "Epoch [70/100], Train Loss: 60156.8633, Val Loss: 59802.8438\n",
      "Epoch [80/100], Train Loss: 59044.7812, Val Loss: 58683.0820\n",
      "Epoch [90/100], Train Loss: 57970.8281, Val Loss: 57600.9961\n",
      "Epoch [100/100], Train Loss: 56923.1992, Val Loss: 56543.4922\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68284.9141, Val Loss: 67945.9219\n",
      "Epoch [20/100], Train Loss: 66709.1328, Val Loss: 66367.7500\n",
      "Epoch [30/100], Train Loss: 65267.0000, Val Loss: 64931.3672\n",
      "Epoch [40/100], Train Loss: 63898.7695, Val Loss: 63561.8281\n",
      "Epoch [50/100], Train Loss: 62628.7227, Val Loss: 62291.2891\n",
      "Epoch [60/100], Train Loss: 61416.3086, Val Loss: 61071.6328\n",
      "Epoch [70/100], Train Loss: 60252.4492, Val Loss: 59897.6953\n",
      "Epoch [80/100], Train Loss: 59132.0312, Val Loss: 58767.6094\n",
      "Epoch [90/100], Train Loss: 58048.4648, Val Loss: 57673.5742\n",
      "Epoch [100/100], Train Loss: 56995.5430, Val Loss: 56612.6484\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69451.6016, Val Loss: 69287.8594\n",
      "Epoch [20/100], Train Loss: 69360.8672, Val Loss: 69194.3828\n",
      "Epoch [30/100], Train Loss: 69244.1641, Val Loss: 69073.3828\n",
      "Epoch [40/100], Train Loss: 69085.8594, Val Loss: 68910.4141\n",
      "Epoch [50/100], Train Loss: 68882.6797, Val Loss: 68704.0078\n",
      "Epoch [60/100], Train Loss: 68648.3828, Val Loss: 68466.2734\n",
      "Epoch [70/100], Train Loss: 68397.7344, Val Loss: 68213.7656\n",
      "Epoch [80/100], Train Loss: 68144.1719, Val Loss: 67954.7578\n",
      "Epoch [90/100], Train Loss: 67886.5000, Val Loss: 67692.9375\n",
      "Epoch [100/100], Train Loss: 67635.2031, Val Loss: 67437.3594\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69518.1562, Val Loss: 69351.7891\n",
      "Epoch [20/100], Train Loss: 69397.1562, Val Loss: 69225.7422\n",
      "Epoch [30/100], Train Loss: 69240.8047, Val Loss: 69060.8906\n",
      "Epoch [40/100], Train Loss: 69032.7969, Val Loss: 68848.8594\n",
      "Epoch [50/100], Train Loss: 68788.3203, Val Loss: 68598.6172\n",
      "Epoch [60/100], Train Loss: 68523.8516, Val Loss: 68329.5312\n",
      "Epoch [70/100], Train Loss: 68253.4766, Val Loss: 68057.8750\n",
      "Epoch [80/100], Train Loss: 67985.0078, Val Loss: 67789.8594\n",
      "Epoch [90/100], Train Loss: 67730.9453, Val Loss: 67528.2422\n",
      "Epoch [100/100], Train Loss: 67481.1172, Val Loss: 67277.1719\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69403.3828, Val Loss: 69232.3984\n",
      "Epoch [20/100], Train Loss: 69275.2734, Val Loss: 69095.0703\n",
      "Epoch [30/100], Train Loss: 69104.7344, Val Loss: 68915.2344\n",
      "Epoch [40/100], Train Loss: 68889.4141, Val Loss: 68692.4453\n",
      "Epoch [50/100], Train Loss: 68643.4688, Val Loss: 68440.0391\n",
      "Epoch [60/100], Train Loss: 68383.4453, Val Loss: 68174.8203\n",
      "Epoch [70/100], Train Loss: 68118.9375, Val Loss: 67909.1406\n",
      "Epoch [80/100], Train Loss: 67863.2891, Val Loss: 67650.3047\n",
      "Epoch [90/100], Train Loss: 67613.0938, Val Loss: 67403.3984\n",
      "Epoch [100/100], Train Loss: 67385.8828, Val Loss: 67170.6641\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69474.2969, Val Loss: 69321.8281\n",
      "Epoch [20/100], Train Loss: 69470.3906, Val Loss: 69317.8359\n",
      "Epoch [30/100], Train Loss: 69469.2656, Val Loss: 69316.7031\n",
      "Epoch [40/100], Train Loss: 69468.7266, Val Loss: 69316.3203\n",
      "Epoch [50/100], Train Loss: 69468.3203, Val Loss: 69316.1797\n",
      "Epoch [60/100], Train Loss: 69468.0234, Val Loss: 69316.1562\n",
      "Epoch [70/100], Train Loss: 69467.9688, Val Loss: 69316.1406\n",
      "Epoch [80/100], Train Loss: 69468.9844, Val Loss: 69316.1328\n",
      "Epoch [90/100], Train Loss: 69467.9531, Val Loss: 69316.1328\n",
      "Epoch [100/100], Train Loss: 69468.9453, Val Loss: 69316.1328\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69468.1719, Val Loss: 69311.5781\n",
      "Epoch [20/100], Train Loss: 69463.7578, Val Loss: 69307.7891\n",
      "Epoch [30/100], Train Loss: 69462.5469, Val Loss: 69306.5938\n",
      "Epoch [40/100], Train Loss: 69462.3516, Val Loss: 69306.2031\n",
      "Epoch [50/100], Train Loss: 69461.5703, Val Loss: 69306.0703\n",
      "Epoch [60/100], Train Loss: 69461.7188, Val Loss: 69306.0391\n",
      "Epoch [70/100], Train Loss: 69462.6484, Val Loss: 69306.0312\n",
      "Epoch [80/100], Train Loss: 69462.3828, Val Loss: 69306.0312\n",
      "Epoch [90/100], Train Loss: 69462.3281, Val Loss: 69306.0234\n",
      "Epoch [100/100], Train Loss: 69462.7578, Val Loss: 69306.0234\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69603.5469, Val Loss: 69453.1719\n",
      "Epoch [20/100], Train Loss: 69600.1719, Val Loss: 69449.4141\n",
      "Epoch [30/100], Train Loss: 69599.0703, Val Loss: 69448.3438\n",
      "Epoch [40/100], Train Loss: 69598.1797, Val Loss: 69447.9766\n",
      "Epoch [50/100], Train Loss: 69597.4141, Val Loss: 69447.8594\n",
      "Epoch [60/100], Train Loss: 69596.9062, Val Loss: 69447.8203\n",
      "Epoch [70/100], Train Loss: 69599.3125, Val Loss: 69447.8047\n",
      "Epoch [80/100], Train Loss: 69599.1328, Val Loss: 69447.8047\n",
      "Epoch [90/100], Train Loss: 69598.4766, Val Loss: 69447.8047\n",
      "Epoch [100/100], Train Loss: 69598.0859, Val Loss: 69447.8047\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 67837.6719, Val Loss: 67507.5547\n",
      "Epoch [20/100], Train Loss: 66462.3438, Val Loss: 66152.4688\n",
      "Epoch [30/100], Train Loss: 65176.6055, Val Loss: 64859.3672\n",
      "Epoch [40/100], Train Loss: 63931.6719, Val Loss: 63608.1172\n",
      "Epoch [50/100], Train Loss: 62733.7461, Val Loss: 62404.6602\n",
      "Epoch [60/100], Train Loss: 61580.6133, Val Loss: 61245.1133\n",
      "Epoch [70/100], Train Loss: 60466.2539, Val Loss: 60122.0742\n",
      "Epoch [80/100], Train Loss: 59385.1016, Val Loss: 59027.3242\n",
      "Epoch [90/100], Train Loss: 58335.2930, Val Loss: 57969.6328\n",
      "Epoch [100/100], Train Loss: 57315.5742, Val Loss: 56946.3828\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68182.2891, Val Loss: 67841.4297\n",
      "Epoch [20/100], Train Loss: 66771.1875, Val Loss: 66460.8672\n",
      "Epoch [30/100], Train Loss: 65463.4141, Val Loss: 65146.0664\n",
      "Epoch [40/100], Train Loss: 64196.5625, Val Loss: 63873.0820\n",
      "Epoch [50/100], Train Loss: 62978.9961, Val Loss: 62650.5000\n",
      "Epoch [60/100], Train Loss: 61812.3984, Val Loss: 61477.0039\n",
      "Epoch [70/100], Train Loss: 60690.5156, Val Loss: 60348.8828\n",
      "Epoch [80/100], Train Loss: 59610.3359, Val Loss: 59262.4219\n",
      "Epoch [90/100], Train Loss: 58567.5312, Val Loss: 58214.0938\n",
      "Epoch [100/100], Train Loss: 57558.0508, Val Loss: 57194.1953\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67678.9141, Val Loss: 67314.8281\n",
      "Epoch [20/100], Train Loss: 66251.8281, Val Loss: 65936.7109\n",
      "Epoch [30/100], Train Loss: 64956.1445, Val Loss: 64636.1758\n",
      "Epoch [40/100], Train Loss: 63707.7539, Val Loss: 63381.6250\n",
      "Epoch [50/100], Train Loss: 62508.3555, Val Loss: 62176.2227\n",
      "Epoch [60/100], Train Loss: 61355.7344, Val Loss: 61017.8516\n",
      "Epoch [70/100], Train Loss: 60246.7891, Val Loss: 59901.6680\n",
      "Epoch [80/100], Train Loss: 59176.3945, Val Loss: 58822.6055\n",
      "Epoch [90/100], Train Loss: 58137.5664, Val Loss: 57774.1406\n",
      "Epoch [100/100], Train Loss: 57122.6406, Val Loss: 56737.0703\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69472.3359, Val Loss: 69300.1406\n",
      "Epoch [20/100], Train Loss: 69281.5859, Val Loss: 69099.7812\n",
      "Epoch [30/100], Train Loss: 69018.1797, Val Loss: 68824.0625\n",
      "Epoch [40/100], Train Loss: 68701.0000, Val Loss: 68498.9688\n",
      "Epoch [50/100], Train Loss: 68380.9531, Val Loss: 68176.6172\n",
      "Epoch [60/100], Train Loss: 68088.9141, Val Loss: 67885.6328\n",
      "Epoch [70/100], Train Loss: 67835.7109, Val Loss: 67634.8906\n",
      "Epoch [80/100], Train Loss: 67615.7734, Val Loss: 67418.4922\n",
      "Epoch [90/100], Train Loss: 67420.9609, Val Loss: 67226.0078\n",
      "Epoch [100/100], Train Loss: 67243.1094, Val Loss: 67049.3594\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69452.4297, Val Loss: 69281.3281\n",
      "Epoch [20/100], Train Loss: 69295.6953, Val Loss: 69114.9219\n",
      "Epoch [30/100], Train Loss: 69078.9766, Val Loss: 68883.6484\n",
      "Epoch [40/100], Train Loss: 68793.3672, Val Loss: 68583.0859\n",
      "Epoch [50/100], Train Loss: 68473.3750, Val Loss: 68257.2422\n",
      "Epoch [60/100], Train Loss: 68170.3203, Val Loss: 67954.5000\n",
      "Epoch [70/100], Train Loss: 67904.6641, Val Loss: 67693.4766\n",
      "Epoch [80/100], Train Loss: 67672.5156, Val Loss: 67465.6641\n",
      "Epoch [90/100], Train Loss: 67460.7812, Val Loss: 67256.5625\n",
      "Epoch [100/100], Train Loss: 67265.4141, Val Loss: 67063.7188\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69512.7812, Val Loss: 69340.1250\n",
      "Epoch [20/100], Train Loss: 69347.6016, Val Loss: 69157.7656\n",
      "Epoch [30/100], Train Loss: 69112.6484, Val Loss: 68901.6875\n",
      "Epoch [40/100], Train Loss: 68805.7109, Val Loss: 68576.7344\n",
      "Epoch [50/100], Train Loss: 68471.4375, Val Loss: 68233.7109\n",
      "Epoch [60/100], Train Loss: 68153.9062, Val Loss: 67918.3281\n",
      "Epoch [70/100], Train Loss: 67877.4375, Val Loss: 67649.0781\n",
      "Epoch [80/100], Train Loss: 67639.3047, Val Loss: 67419.7812\n",
      "Epoch [90/100], Train Loss: 67431.7734, Val Loss: 67216.3750\n",
      "Epoch [100/100], Train Loss: 67238.6484, Val Loss: 67027.7891\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69491.7188, Val Loss: 69337.4062\n",
      "Epoch [20/100], Train Loss: 69478.1406, Val Loss: 69323.7109\n",
      "Epoch [30/100], Train Loss: 69465.1094, Val Loss: 69309.6016\n",
      "Epoch [40/100], Train Loss: 69449.7812, Val Loss: 69294.9219\n",
      "Epoch [50/100], Train Loss: 69435.1641, Val Loss: 69279.5469\n",
      "Epoch [60/100], Train Loss: 69419.8750, Val Loss: 69263.3438\n",
      "Epoch [70/100], Train Loss: 69402.0312, Val Loss: 69246.1719\n",
      "Epoch [80/100], Train Loss: 69384.0078, Val Loss: 69227.9141\n",
      "Epoch [90/100], Train Loss: 69364.8672, Val Loss: 69208.4766\n",
      "Epoch [100/100], Train Loss: 69344.7109, Val Loss: 69187.7422\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69598.9531, Val Loss: 69445.5547\n",
      "Epoch [20/100], Train Loss: 69584.6641, Val Loss: 69431.0391\n",
      "Epoch [30/100], Train Loss: 69569.9531, Val Loss: 69416.1953\n",
      "Epoch [40/100], Train Loss: 69555.0234, Val Loss: 69400.8906\n",
      "Epoch [50/100], Train Loss: 69540.7734, Val Loss: 69384.9922\n",
      "Epoch [60/100], Train Loss: 69523.7812, Val Loss: 69368.3281\n",
      "Epoch [70/100], Train Loss: 69508.0781, Val Loss: 69350.7891\n",
      "Epoch [80/100], Train Loss: 69490.0625, Val Loss: 69332.2422\n",
      "Epoch [90/100], Train Loss: 69471.8359, Val Loss: 69312.5859\n",
      "Epoch [100/100], Train Loss: 69450.5625, Val Loss: 69291.6641\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69548.3906, Val Loss: 69396.7266\n",
      "Epoch [20/100], Train Loss: 69544.0078, Val Loss: 69392.8203\n",
      "Epoch [30/100], Train Loss: 69543.0391, Val Loss: 69391.7031\n",
      "Epoch [40/100], Train Loss: 69543.0000, Val Loss: 69391.3203\n",
      "Epoch [50/100], Train Loss: 69542.7734, Val Loss: 69391.2031\n",
      "Epoch [60/100], Train Loss: 69542.2891, Val Loss: 69391.1641\n",
      "Epoch [70/100], Train Loss: 69541.8438, Val Loss: 69391.1484\n",
      "Epoch [80/100], Train Loss: 69542.6328, Val Loss: 69391.1484\n",
      "Epoch [90/100], Train Loss: 69543.3906, Val Loss: 69391.1406\n",
      "Epoch [100/100], Train Loss: 69541.8750, Val Loss: 69391.1406\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 67909.0938, Val Loss: 67595.0781\n",
      "Epoch [20/100], Train Loss: 66599.1094, Val Loss: 66291.9297\n",
      "Epoch [30/100], Train Loss: 65321.8477, Val Loss: 65006.4688\n",
      "Epoch [40/100], Train Loss: 64081.9609, Val Loss: 63760.1484\n",
      "Epoch [50/100], Train Loss: 62888.0859, Val Loss: 62560.1367\n",
      "Epoch [60/100], Train Loss: 61739.2891, Val Loss: 61405.1367\n",
      "Epoch [70/100], Train Loss: 60632.6211, Val Loss: 60292.1367\n",
      "Epoch [80/100], Train Loss: 59565.1602, Val Loss: 59218.2383\n",
      "Epoch [90/100], Train Loss: 58534.4062, Val Loss: 58180.9922\n",
      "Epoch [100/100], Train Loss: 57538.3984, Val Loss: 57178.4414\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 67734.8828, Val Loss: 67429.0312\n",
      "Epoch [20/100], Train Loss: 66456.2188, Val Loss: 66148.6641\n",
      "Epoch [30/100], Train Loss: 65189.4492, Val Loss: 64873.8203\n",
      "Epoch [40/100], Train Loss: 63958.9766, Val Loss: 63636.8555\n",
      "Epoch [50/100], Train Loss: 62773.1758, Val Loss: 62444.8281\n",
      "Epoch [60/100], Train Loss: 61631.2812, Val Loss: 61296.6719\n",
      "Epoch [70/100], Train Loss: 60530.5859, Val Loss: 60189.5938\n",
      "Epoch [80/100], Train Loss: 59468.3672, Val Loss: 59120.9102\n",
      "Epoch [90/100], Train Loss: 58442.3242, Val Loss: 58088.3320\n",
      "Epoch [100/100], Train Loss: 57450.5469, Val Loss: 57090.0117\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67861.1094, Val Loss: 67541.7266\n",
      "Epoch [20/100], Train Loss: 66551.5781, Val Loss: 66243.2734\n",
      "Epoch [30/100], Train Loss: 65272.6211, Val Loss: 64956.3867\n",
      "Epoch [40/100], Train Loss: 64031.3945, Val Loss: 63708.4688\n",
      "Epoch [50/100], Train Loss: 62835.9062, Val Loss: 62507.1680\n",
      "Epoch [60/100], Train Loss: 61686.1953, Val Loss: 61351.2969\n",
      "Epoch [70/100], Train Loss: 60579.0078, Val Loss: 60237.7773\n",
      "Epoch [80/100], Train Loss: 59511.2891, Val Loss: 59163.6523\n",
      "Epoch [90/100], Train Loss: 58480.5430, Val Loss: 58126.4219\n",
      "Epoch [100/100], Train Loss: 57484.7227, Val Loss: 57124.0625\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69448.7891, Val Loss: 69280.9375\n",
      "Epoch [20/100], Train Loss: 69280.9531, Val Loss: 69099.5938\n",
      "Epoch [30/100], Train Loss: 69003.8516, Val Loss: 68804.2812\n",
      "Epoch [40/100], Train Loss: 68645.0547, Val Loss: 68435.9531\n",
      "Epoch [50/100], Train Loss: 68310.3516, Val Loss: 68103.7266\n",
      "Epoch [60/100], Train Loss: 68044.2188, Val Loss: 67846.5469\n",
      "Epoch [70/100], Train Loss: 67828.9531, Val Loss: 67636.0312\n",
      "Epoch [80/100], Train Loss: 67642.5469, Val Loss: 67451.4609\n",
      "Epoch [90/100], Train Loss: 67471.0781, Val Loss: 67280.7500\n",
      "Epoch [100/100], Train Loss: 67309.1641, Val Loss: 67118.8203\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69379.3203, Val Loss: 69200.3516\n",
      "Epoch [20/100], Train Loss: 69146.3047, Val Loss: 68943.0000\n",
      "Epoch [30/100], Train Loss: 68801.9531, Val Loss: 68579.9219\n",
      "Epoch [40/100], Train Loss: 68439.2500, Val Loss: 68214.9062\n",
      "Epoch [50/100], Train Loss: 68134.5312, Val Loss: 67916.0938\n",
      "Epoch [60/100], Train Loss: 67886.4219, Val Loss: 67678.6172\n",
      "Epoch [70/100], Train Loss: 67680.5781, Val Loss: 67479.6797\n",
      "Epoch [80/100], Train Loss: 67498.5391, Val Loss: 67301.1094\n",
      "Epoch [90/100], Train Loss: 67330.2422, Val Loss: 67134.3906\n",
      "Epoch [100/100], Train Loss: 67170.7891, Val Loss: 66975.5781\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69453.8125, Val Loss: 69277.8984\n",
      "Epoch [20/100], Train Loss: 69256.9844, Val Loss: 69057.0078\n",
      "Epoch [30/100], Train Loss: 68945.6094, Val Loss: 68714.0078\n",
      "Epoch [40/100], Train Loss: 68567.3594, Val Loss: 68319.3594\n",
      "Epoch [50/100], Train Loss: 68219.2422, Val Loss: 67972.3516\n",
      "Epoch [60/100], Train Loss: 67929.3281, Val Loss: 67701.0000\n",
      "Epoch [70/100], Train Loss: 67698.7656, Val Loss: 67482.2500\n",
      "Epoch [80/100], Train Loss: 67501.2891, Val Loss: 67293.2734\n",
      "Epoch [90/100], Train Loss: 67324.4688, Val Loss: 67120.3203\n",
      "Epoch [100/100], Train Loss: 67159.4453, Val Loss: 66957.1094\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69588.4531, Val Loss: 69434.6484\n",
      "Epoch [20/100], Train Loss: 69574.8516, Val Loss: 69420.8281\n",
      "Epoch [30/100], Train Loss: 69562.0391, Val Loss: 69407.1953\n",
      "Epoch [40/100], Train Loss: 69548.1797, Val Loss: 69393.5625\n",
      "Epoch [50/100], Train Loss: 69534.5000, Val Loss: 69379.7656\n",
      "Epoch [60/100], Train Loss: 69520.7812, Val Loss: 69365.5547\n",
      "Epoch [70/100], Train Loss: 69507.0078, Val Loss: 69350.6875\n",
      "Epoch [80/100], Train Loss: 69491.5859, Val Loss: 69334.9453\n",
      "Epoch [90/100], Train Loss: 69475.8672, Val Loss: 69318.0547\n",
      "Epoch [100/100], Train Loss: 69457.5312, Val Loss: 69299.7656\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69543.1953, Val Loss: 69389.0000\n",
      "Epoch [20/100], Train Loss: 69531.0156, Val Loss: 69376.3750\n",
      "Epoch [30/100], Train Loss: 69518.6016, Val Loss: 69363.6797\n",
      "Epoch [40/100], Train Loss: 69505.7969, Val Loss: 69350.6484\n",
      "Epoch [50/100], Train Loss: 69492.4922, Val Loss: 69337.0234\n",
      "Epoch [60/100], Train Loss: 69478.8438, Val Loss: 69322.5781\n",
      "Epoch [70/100], Train Loss: 69464.1562, Val Loss: 69307.0391\n",
      "Epoch [80/100], Train Loss: 69448.4609, Val Loss: 69290.1953\n",
      "Epoch [90/100], Train Loss: 69430.5781, Val Loss: 69271.7969\n",
      "Epoch [100/100], Train Loss: 69412.2734, Val Loss: 69251.5938\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69515.2422, Val Loss: 69361.2109\n",
      "Epoch [20/100], Train Loss: 69503.7422, Val Loss: 69348.1328\n",
      "Epoch [30/100], Train Loss: 69490.7344, Val Loss: 69334.9531\n",
      "Epoch [40/100], Train Loss: 69478.5234, Val Loss: 69321.5000\n",
      "Epoch [50/100], Train Loss: 69465.1484, Val Loss: 69307.6250\n",
      "Epoch [60/100], Train Loss: 69450.9453, Val Loss: 69293.1328\n",
      "Epoch [70/100], Train Loss: 69437.2500, Val Loss: 69277.8203\n",
      "Epoch [80/100], Train Loss: 69422.5859, Val Loss: 69261.5156\n",
      "Epoch [90/100], Train Loss: 69405.0547, Val Loss: 69244.0156\n",
      "Epoch [100/100], Train Loss: 69388.0938, Val Loss: 69225.1562\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68134.7500, Val Loss: 67796.2031\n",
      "Epoch [20/100], Train Loss: 65918.8281, Val Loss: 65527.7500\n",
      "Epoch [30/100], Train Loss: 63289.3086, Val Loss: 62829.3750\n",
      "Epoch [40/100], Train Loss: 60532.7578, Val Loss: 60032.0234\n",
      "Epoch [50/100], Train Loss: 57896.2734, Val Loss: 57382.1328\n",
      "Epoch [60/100], Train Loss: 55409.7070, Val Loss: 54887.9688\n",
      "Epoch [70/100], Train Loss: 53033.9648, Val Loss: 52508.3125\n",
      "Epoch [80/100], Train Loss: 50793.9375, Val Loss: 50266.3203\n",
      "Epoch [90/100], Train Loss: 48702.1719, Val Loss: 48174.2031\n",
      "Epoch [100/100], Train Loss: 46768.3867, Val Loss: 46242.2852\n",
      "New best score: 46242.28515625 with params: {'hidden_dim': 64, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68032.6875, Val Loss: 67693.7500\n",
      "Epoch [20/100], Train Loss: 65762.9375, Val Loss: 65377.4688\n",
      "Epoch [30/100], Train Loss: 63127.0391, Val Loss: 62663.5352\n",
      "Epoch [40/100], Train Loss: 60354.9336, Val Loss: 59846.8633\n",
      "Epoch [50/100], Train Loss: 57729.9258, Val Loss: 57212.7656\n",
      "Epoch [60/100], Train Loss: 55271.0508, Val Loss: 54752.1562\n",
      "Epoch [70/100], Train Loss: 52943.3594, Val Loss: 52424.0195\n",
      "Epoch [80/100], Train Loss: 50742.3711, Val Loss: 50222.8359\n",
      "Epoch [90/100], Train Loss: 48690.8555, Val Loss: 48173.8320\n",
      "Epoch [100/100], Train Loss: 46784.3125, Val Loss: 46269.1055\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68133.4375, Val Loss: 67796.6641\n",
      "Epoch [20/100], Train Loss: 65885.6328, Val Loss: 65503.8398\n",
      "Epoch [30/100], Train Loss: 63260.2383, Val Loss: 62800.1562\n",
      "Epoch [40/100], Train Loss: 60491.7578, Val Loss: 59988.2344\n",
      "Epoch [50/100], Train Loss: 57851.6484, Val Loss: 57336.3242\n",
      "Epoch [60/100], Train Loss: 55365.3477, Val Loss: 54849.7656\n",
      "Epoch [70/100], Train Loss: 53015.1953, Val Loss: 52496.2578\n",
      "Epoch [80/100], Train Loss: 50798.3438, Val Loss: 50276.1484\n",
      "Epoch [90/100], Train Loss: 48722.3203, Val Loss: 48199.0508\n",
      "Epoch [100/100], Train Loss: 46800.8359, Val Loss: 46277.4062\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69414.6797, Val Loss: 69254.6484\n",
      "Epoch [20/100], Train Loss: 69286.0312, Val Loss: 69125.0312\n",
      "Epoch [30/100], Train Loss: 69135.2031, Val Loss: 68972.8047\n",
      "Epoch [40/100], Train Loss: 68954.9766, Val Loss: 68791.3828\n",
      "Epoch [50/100], Train Loss: 68743.4219, Val Loss: 68579.1094\n",
      "Epoch [60/100], Train Loss: 68502.9688, Val Loss: 68338.5234\n",
      "Epoch [70/100], Train Loss: 68238.5859, Val Loss: 68074.5312\n",
      "Epoch [80/100], Train Loss: 67956.0312, Val Loss: 67792.6953\n",
      "Epoch [90/100], Train Loss: 67660.4141, Val Loss: 67497.6641\n",
      "Epoch [100/100], Train Loss: 67355.8203, Val Loss: 67193.1953\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69447.8359, Val Loss: 69280.8672\n",
      "Epoch [20/100], Train Loss: 69328.8125, Val Loss: 69160.9844\n",
      "Epoch [30/100], Train Loss: 69190.3438, Val Loss: 69020.5938\n",
      "Epoch [40/100], Train Loss: 69021.0547, Val Loss: 68849.0391\n",
      "Epoch [50/100], Train Loss: 68816.3672, Val Loss: 68642.4297\n",
      "Epoch [60/100], Train Loss: 68577.7734, Val Loss: 68402.6562\n",
      "Epoch [70/100], Train Loss: 68310.6406, Val Loss: 68135.1562\n",
      "Epoch [80/100], Train Loss: 68021.5547, Val Loss: 67846.3828\n",
      "Epoch [90/100], Train Loss: 67716.3828, Val Loss: 67541.8594\n",
      "Epoch [100/100], Train Loss: 67399.8984, Val Loss: 67225.9062\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69428.8516, Val Loss: 69265.1641\n",
      "Epoch [20/100], Train Loss: 69287.2891, Val Loss: 69122.8047\n",
      "Epoch [30/100], Train Loss: 69123.7656, Val Loss: 68958.0547\n",
      "Epoch [40/100], Train Loss: 68931.5547, Val Loss: 68764.6953\n",
      "Epoch [50/100], Train Loss: 68709.0391, Val Loss: 68541.3828\n",
      "Epoch [60/100], Train Loss: 68458.7031, Val Loss: 68290.7578\n",
      "Epoch [70/100], Train Loss: 68185.4141, Val Loss: 68017.5625\n",
      "Epoch [80/100], Train Loss: 67894.3594, Val Loss: 67726.7891\n",
      "Epoch [90/100], Train Loss: 67590.2422, Val Loss: 67422.8516\n",
      "Epoch [100/100], Train Loss: 67277.1406, Val Loss: 67109.6016\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69570.5078, Val Loss: 69423.2656\n",
      "Epoch [20/100], Train Loss: 69558.7734, Val Loss: 69411.6328\n",
      "Epoch [30/100], Train Loss: 69547.0703, Val Loss: 69400.0469\n",
      "Epoch [40/100], Train Loss: 69535.3750, Val Loss: 69388.4531\n",
      "Epoch [50/100], Train Loss: 69523.6250, Val Loss: 69376.8125\n",
      "Epoch [60/100], Train Loss: 69511.7734, Val Loss: 69365.0625\n",
      "Epoch [70/100], Train Loss: 69499.7656, Val Loss: 69353.1641\n",
      "Epoch [80/100], Train Loss: 69487.5703, Val Loss: 69341.0859\n",
      "Epoch [90/100], Train Loss: 69475.1484, Val Loss: 69328.7734\n",
      "Epoch [100/100], Train Loss: 69462.4688, Val Loss: 69316.1953\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69485.7891, Val Loss: 69333.0859\n",
      "Epoch [20/100], Train Loss: 69473.7188, Val Loss: 69321.0938\n",
      "Epoch [30/100], Train Loss: 69461.6250, Val Loss: 69309.0312\n",
      "Epoch [40/100], Train Loss: 69449.4219, Val Loss: 69296.8828\n",
      "Epoch [50/100], Train Loss: 69437.0938, Val Loss: 69284.5938\n",
      "Epoch [60/100], Train Loss: 69424.5938, Val Loss: 69272.1406\n",
      "Epoch [70/100], Train Loss: 69411.8906, Val Loss: 69259.4844\n",
      "Epoch [80/100], Train Loss: 69398.9297, Val Loss: 69246.5781\n",
      "Epoch [90/100], Train Loss: 69385.7109, Val Loss: 69233.4141\n",
      "Epoch [100/100], Train Loss: 69372.1797, Val Loss: 69219.9375\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69489.7188, Val Loss: 69334.0781\n",
      "Epoch [20/100], Train Loss: 69485.3672, Val Loss: 69330.0156\n",
      "Epoch [30/100], Train Loss: 69483.9922, Val Loss: 69328.8594\n",
      "Epoch [40/100], Train Loss: 69483.5469, Val Loss: 69328.4609\n",
      "Epoch [50/100], Train Loss: 69483.4062, Val Loss: 69328.3359\n",
      "Epoch [60/100], Train Loss: 69483.3594, Val Loss: 69328.2969\n",
      "Epoch [70/100], Train Loss: 69483.3438, Val Loss: 69328.2891\n",
      "Epoch [80/100], Train Loss: 69483.3438, Val Loss: 69328.2734\n",
      "Epoch [90/100], Train Loss: 69483.3438, Val Loss: 69328.2734\n",
      "Epoch [100/100], Train Loss: 69483.3281, Val Loss: 69328.2734\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 66981.7891, Val Loss: 66517.9141\n",
      "Epoch [20/100], Train Loss: 64208.8477, Val Loss: 63744.9766\n",
      "Epoch [30/100], Train Loss: 61671.5312, Val Loss: 61207.6797\n",
      "Epoch [40/100], Train Loss: 59359.7070, Val Loss: 58891.3789\n",
      "Epoch [50/100], Train Loss: 57176.1328, Val Loss: 56695.6406\n",
      "Epoch [60/100], Train Loss: 55145.8398, Val Loss: 54656.6836\n",
      "Epoch [70/100], Train Loss: 53249.7031, Val Loss: 52752.1094\n",
      "Epoch [80/100], Train Loss: 51412.6680, Val Loss: 50892.9258\n",
      "Epoch [90/100], Train Loss: 49618.2891, Val Loss: 49107.1484\n",
      "Epoch [100/100], Train Loss: 47923.6953, Val Loss: 47432.1602\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 67060.1406, Val Loss: 66581.6406\n",
      "Epoch [20/100], Train Loss: 64229.8242, Val Loss: 63764.2383\n",
      "Epoch [30/100], Train Loss: 61696.4258, Val Loss: 61224.7617\n",
      "Epoch [40/100], Train Loss: 59346.0000, Val Loss: 58877.3984\n",
      "Epoch [50/100], Train Loss: 57164.7227, Val Loss: 56682.6914\n",
      "Epoch [60/100], Train Loss: 55113.1133, Val Loss: 54620.8086\n",
      "Epoch [70/100], Train Loss: 53170.7461, Val Loss: 52665.3164\n",
      "Epoch [80/100], Train Loss: 51324.5234, Val Loss: 50806.1719\n",
      "Epoch [90/100], Train Loss: 49533.0352, Val Loss: 49031.3828\n",
      "Epoch [100/100], Train Loss: 47843.2500, Val Loss: 47350.8672\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67062.1875, Val Loss: 66588.9844\n",
      "Epoch [20/100], Train Loss: 64297.9844, Val Loss: 63824.9102\n",
      "Epoch [30/100], Train Loss: 61759.8398, Val Loss: 61287.0547\n",
      "Epoch [40/100], Train Loss: 59356.9688, Val Loss: 58891.3242\n",
      "Epoch [50/100], Train Loss: 57170.2891, Val Loss: 56689.3281\n",
      "Epoch [60/100], Train Loss: 55122.5703, Val Loss: 54629.1133\n",
      "Epoch [70/100], Train Loss: 53195.9961, Val Loss: 52683.8477\n",
      "Epoch [80/100], Train Loss: 51337.2695, Val Loss: 50816.0820\n",
      "Epoch [90/100], Train Loss: 49522.6094, Val Loss: 49020.3008\n",
      "Epoch [100/100], Train Loss: 47840.7461, Val Loss: 47343.3203\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69347.0547, Val Loss: 69162.3281\n",
      "Epoch [20/100], Train Loss: 69027.8594, Val Loss: 68826.0859\n",
      "Epoch [30/100], Train Loss: 68585.8125, Val Loss: 68374.9609\n",
      "Epoch [40/100], Train Loss: 68094.2266, Val Loss: 67885.1094\n",
      "Epoch [50/100], Train Loss: 67614.4844, Val Loss: 67403.0547\n",
      "Epoch [60/100], Train Loss: 67149.1172, Val Loss: 66932.0234\n",
      "Epoch [70/100], Train Loss: 66706.7734, Val Loss: 66486.9297\n",
      "Epoch [80/100], Train Loss: 66299.4688, Val Loss: 66076.7188\n",
      "Epoch [90/100], Train Loss: 65918.1875, Val Loss: 65693.2188\n",
      "Epoch [100/100], Train Loss: 65554.4609, Val Loss: 65329.1719\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69382.8984, Val Loss: 69207.9062\n",
      "Epoch [20/100], Train Loss: 69109.2422, Val Loss: 68921.0391\n",
      "Epoch [30/100], Train Loss: 68715.7500, Val Loss: 68514.4453\n",
      "Epoch [40/100], Train Loss: 68245.6406, Val Loss: 68043.0547\n",
      "Epoch [50/100], Train Loss: 67768.9531, Val Loss: 67558.5000\n",
      "Epoch [60/100], Train Loss: 67293.6172, Val Loss: 67074.4219\n",
      "Epoch [70/100], Train Loss: 66843.4531, Val Loss: 66616.2188\n",
      "Epoch [80/100], Train Loss: 66422.1406, Val Loss: 66191.0469\n",
      "Epoch [90/100], Train Loss: 66026.2734, Val Loss: 65793.7734\n",
      "Epoch [100/100], Train Loss: 65656.9922, Val Loss: 65426.1914\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69350.8516, Val Loss: 69170.8516\n",
      "Epoch [20/100], Train Loss: 69067.5859, Val Loss: 68866.3438\n",
      "Epoch [30/100], Train Loss: 68630.9844, Val Loss: 68412.9766\n",
      "Epoch [40/100], Train Loss: 68108.6953, Val Loss: 67887.1328\n",
      "Epoch [50/100], Train Loss: 67584.7344, Val Loss: 67363.5703\n",
      "Epoch [60/100], Train Loss: 67087.8750, Val Loss: 66852.4297\n",
      "Epoch [70/100], Train Loss: 66608.0859, Val Loss: 66366.7031\n",
      "Epoch [80/100], Train Loss: 66165.5234, Val Loss: 65921.9922\n",
      "Epoch [90/100], Train Loss: 65756.8906, Val Loss: 65513.8281\n",
      "Epoch [100/100], Train Loss: 65381.4375, Val Loss: 65140.1016\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69556.2266, Val Loss: 69402.5312\n",
      "Epoch [20/100], Train Loss: 69538.1016, Val Loss: 69384.2188\n",
      "Epoch [30/100], Train Loss: 69519.8281, Val Loss: 69365.3594\n",
      "Epoch [40/100], Train Loss: 69499.9844, Val Loss: 69345.5859\n",
      "Epoch [50/100], Train Loss: 69479.1797, Val Loss: 69324.6094\n",
      "Epoch [60/100], Train Loss: 69457.3516, Val Loss: 69302.0938\n",
      "Epoch [70/100], Train Loss: 69432.9922, Val Loss: 69277.7734\n",
      "Epoch [80/100], Train Loss: 69407.2031, Val Loss: 69251.3750\n",
      "Epoch [90/100], Train Loss: 69378.2812, Val Loss: 69222.6016\n",
      "Epoch [100/100], Train Loss: 69348.2031, Val Loss: 69191.2344\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69578.6484, Val Loss: 69425.1484\n",
      "Epoch [20/100], Train Loss: 69557.7109, Val Loss: 69404.0938\n",
      "Epoch [30/100], Train Loss: 69536.9922, Val Loss: 69382.6875\n",
      "Epoch [40/100], Train Loss: 69515.4766, Val Loss: 69360.5938\n",
      "Epoch [50/100], Train Loss: 69493.2109, Val Loss: 69337.4688\n",
      "Epoch [60/100], Train Loss: 69468.4531, Val Loss: 69313.0156\n",
      "Epoch [70/100], Train Loss: 69444.2578, Val Loss: 69286.9141\n",
      "Epoch [80/100], Train Loss: 69415.8672, Val Loss: 69258.9297\n",
      "Epoch [90/100], Train Loss: 69386.5391, Val Loss: 69228.7891\n",
      "Epoch [100/100], Train Loss: 69355.2891, Val Loss: 69196.2578\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69592.8203, Val Loss: 69437.3047\n",
      "Epoch [20/100], Train Loss: 69573.0156, Val Loss: 69417.4688\n",
      "Epoch [30/100], Train Loss: 69553.7891, Val Loss: 69397.3438\n",
      "Epoch [40/100], Train Loss: 69534.9766, Val Loss: 69376.5547\n",
      "Epoch [50/100], Train Loss: 69513.1875, Val Loss: 69354.7109\n",
      "Epoch [60/100], Train Loss: 69492.2188, Val Loss: 69331.4688\n",
      "Epoch [70/100], Train Loss: 69468.7266, Val Loss: 69306.5312\n",
      "Epoch [80/100], Train Loss: 69444.6641, Val Loss: 69279.6484\n",
      "Epoch [90/100], Train Loss: 69415.5938, Val Loss: 69250.5547\n",
      "Epoch [100/100], Train Loss: 69385.8828, Val Loss: 69218.9609\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 66597.1250, Val Loss: 66155.0000\n",
      "Epoch [20/100], Train Loss: 64127.2422, Val Loss: 63686.1328\n",
      "Epoch [30/100], Train Loss: 61775.4727, Val Loss: 61324.4844\n",
      "Epoch [40/100], Train Loss: 59557.3633, Val Loss: 59099.9609\n",
      "Epoch [50/100], Train Loss: 57483.4531, Val Loss: 57019.9336\n",
      "Epoch [60/100], Train Loss: 55547.7344, Val Loss: 55078.3008\n",
      "Epoch [70/100], Train Loss: 53730.2148, Val Loss: 53249.4492\n",
      "Epoch [80/100], Train Loss: 51932.1094, Val Loss: 51422.3672\n",
      "Epoch [90/100], Train Loss: 50266.8945, Val Loss: 49749.7031\n",
      "Epoch [100/100], Train Loss: 48479.4023, Val Loss: 47980.3281\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 66470.6484, Val Loss: 66030.1641\n",
      "Epoch [20/100], Train Loss: 64015.9805, Val Loss: 63573.6484\n",
      "Epoch [30/100], Train Loss: 61669.4141, Val Loss: 61218.4219\n",
      "Epoch [40/100], Train Loss: 59457.7852, Val Loss: 58999.6562\n",
      "Epoch [50/100], Train Loss: 57390.4648, Val Loss: 56924.8828\n",
      "Epoch [60/100], Train Loss: 55459.0078, Val Loss: 54988.5039\n",
      "Epoch [70/100], Train Loss: 53659.1719, Val Loss: 53182.2227\n",
      "Epoch [80/100], Train Loss: 51981.7461, Val Loss: 51497.7969\n",
      "Epoch [90/100], Train Loss: 50418.9102, Val Loss: 49927.7656\n",
      "Epoch [100/100], Train Loss: 48963.8047, Val Loss: 48465.4258\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 66599.3828, Val Loss: 66152.5000\n",
      "Epoch [20/100], Train Loss: 64121.3008, Val Loss: 63680.0117\n",
      "Epoch [30/100], Train Loss: 61766.3359, Val Loss: 61315.1133\n",
      "Epoch [40/100], Train Loss: 59545.4102, Val Loss: 59087.0781\n",
      "Epoch [50/100], Train Loss: 57468.3008, Val Loss: 57004.2383\n",
      "Epoch [60/100], Train Loss: 55531.4531, Val Loss: 55061.0234\n",
      "Epoch [70/100], Train Loss: 53726.0391, Val Loss: 53248.8867\n",
      "Epoch [80/100], Train Loss: 52043.5430, Val Loss: 51559.3828\n",
      "Epoch [90/100], Train Loss: 50476.2695, Val Loss: 49984.8750\n",
      "Epoch [100/100], Train Loss: 49017.3242, Val Loss: 48518.5586\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69326.9922, Val Loss: 69137.9531\n",
      "Epoch [20/100], Train Loss: 68863.7188, Val Loss: 68639.0469\n",
      "Epoch [30/100], Train Loss: 68190.5703, Val Loss: 67955.6172\n",
      "Epoch [40/100], Train Loss: 67577.9141, Val Loss: 67346.8828\n",
      "Epoch [50/100], Train Loss: 67100.8672, Val Loss: 66875.3672\n",
      "Epoch [60/100], Train Loss: 66696.3984, Val Loss: 66477.3828\n",
      "Epoch [70/100], Train Loss: 66343.0469, Val Loss: 66126.8984\n",
      "Epoch [80/100], Train Loss: 66023.7891, Val Loss: 65808.0156\n",
      "Epoch [90/100], Train Loss: 65723.8516, Val Loss: 65507.9414\n",
      "Epoch [100/100], Train Loss: 65437.3516, Val Loss: 65220.5000\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69322.0156, Val Loss: 69122.0781\n",
      "Epoch [20/100], Train Loss: 68795.0391, Val Loss: 68552.5703\n",
      "Epoch [30/100], Train Loss: 68075.1719, Val Loss: 67819.6250\n",
      "Epoch [40/100], Train Loss: 67444.7266, Val Loss: 67198.8047\n",
      "Epoch [50/100], Train Loss: 66959.7188, Val Loss: 66724.7891\n",
      "Epoch [60/100], Train Loss: 66563.1719, Val Loss: 66339.3047\n",
      "Epoch [70/100], Train Loss: 66224.8047, Val Loss: 66005.0312\n",
      "Epoch [80/100], Train Loss: 65914.7500, Val Loss: 65696.4297\n",
      "Epoch [90/100], Train Loss: 65621.6953, Val Loss: 65403.2188\n",
      "Epoch [100/100], Train Loss: 65340.2891, Val Loss: 65121.1914\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69340.9531, Val Loss: 69150.0703\n",
      "Epoch [20/100], Train Loss: 68922.1328, Val Loss: 68686.7266\n",
      "Epoch [30/100], Train Loss: 68298.4844, Val Loss: 68046.2031\n",
      "Epoch [40/100], Train Loss: 67712.7188, Val Loss: 67463.0859\n",
      "Epoch [50/100], Train Loss: 67224.4688, Val Loss: 66984.2578\n",
      "Epoch [60/100], Train Loss: 66814.7500, Val Loss: 66584.0469\n",
      "Epoch [70/100], Train Loss: 66458.9062, Val Loss: 66235.7188\n",
      "Epoch [80/100], Train Loss: 66137.4844, Val Loss: 65917.2422\n",
      "Epoch [90/100], Train Loss: 65834.6172, Val Loss: 65615.1484\n",
      "Epoch [100/100], Train Loss: 65543.2656, Val Loss: 65322.7539\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69538.6875, Val Loss: 69383.9609\n",
      "Epoch [20/100], Train Loss: 69516.0000, Val Loss: 69361.1250\n",
      "Epoch [30/100], Train Loss: 69492.4453, Val Loss: 69337.0703\n",
      "Epoch [40/100], Train Loss: 69466.6250, Val Loss: 69310.9375\n",
      "Epoch [50/100], Train Loss: 69438.0312, Val Loss: 69281.6953\n",
      "Epoch [60/100], Train Loss: 69405.1484, Val Loss: 69248.3047\n",
      "Epoch [70/100], Train Loss: 69367.6484, Val Loss: 69209.7422\n",
      "Epoch [80/100], Train Loss: 69323.5781, Val Loss: 69165.0156\n",
      "Epoch [90/100], Train Loss: 69272.8203, Val Loss: 69113.2422\n",
      "Epoch [100/100], Train Loss: 69214.5469, Val Loss: 69053.9375\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69502.0156, Val Loss: 69347.2031\n",
      "Epoch [20/100], Train Loss: 69479.6094, Val Loss: 69324.0781\n",
      "Epoch [30/100], Train Loss: 69456.0078, Val Loss: 69299.4609\n",
      "Epoch [40/100], Train Loss: 69430.1172, Val Loss: 69272.5469\n",
      "Epoch [50/100], Train Loss: 69400.8359, Val Loss: 69242.5234\n",
      "Epoch [60/100], Train Loss: 69368.4766, Val Loss: 69208.4844\n",
      "Epoch [70/100], Train Loss: 69331.3281, Val Loss: 69169.5391\n",
      "Epoch [80/100], Train Loss: 69287.8281, Val Loss: 69124.8359\n",
      "Epoch [90/100], Train Loss: 69238.6562, Val Loss: 69073.5156\n",
      "Epoch [100/100], Train Loss: 69181.6719, Val Loss: 69014.9297\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69538.7188, Val Loss: 69382.9375\n",
      "Epoch [20/100], Train Loss: 69517.5078, Val Loss: 69360.1016\n",
      "Epoch [30/100], Train Loss: 69494.9453, Val Loss: 69336.4609\n",
      "Epoch [40/100], Train Loss: 69469.7734, Val Loss: 69311.1797\n",
      "Epoch [50/100], Train Loss: 69443.7188, Val Loss: 69283.2188\n",
      "Epoch [60/100], Train Loss: 69412.0547, Val Loss: 69251.5000\n",
      "Epoch [70/100], Train Loss: 69379.4844, Val Loss: 69214.8672\n",
      "Epoch [80/100], Train Loss: 69338.0234, Val Loss: 69172.2656\n",
      "Epoch [90/100], Train Loss: 69292.8438, Val Loss: 69122.6328\n",
      "Epoch [100/100], Train Loss: 69238.1719, Val Loss: 69065.0312\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 66366.9375, Val Loss: 65948.3438\n",
      "Epoch [20/100], Train Loss: 63980.6406, Val Loss: 63541.8398\n",
      "Epoch [30/100], Train Loss: 61665.3633, Val Loss: 61217.1641\n",
      "Epoch [40/100], Train Loss: 59479.0078, Val Loss: 59023.9297\n",
      "Epoch [50/100], Train Loss: 57430.3516, Val Loss: 56968.6914\n",
      "Epoch [60/100], Train Loss: 55515.3320, Val Loss: 55046.9609\n",
      "Epoch [70/100], Train Loss: 53729.3477, Val Loss: 53256.5547\n",
      "Epoch [80/100], Train Loss: 52058.1641, Val Loss: 51573.9531\n",
      "Epoch [90/100], Train Loss: 50498.7695, Val Loss: 50008.9570\n",
      "Epoch [100/100], Train Loss: 49046.9883, Val Loss: 48549.7930\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 66847.0938, Val Loss: 66430.2109\n",
      "Epoch [20/100], Train Loss: 64441.8906, Val Loss: 64004.5234\n",
      "Epoch [30/100], Train Loss: 62098.3555, Val Loss: 61650.2266\n",
      "Epoch [40/100], Train Loss: 59882.3828, Val Loss: 59428.4414\n",
      "Epoch [50/100], Train Loss: 57807.4570, Val Loss: 57347.0781\n",
      "Epoch [60/100], Train Loss: 55868.4102, Val Loss: 55401.4648\n",
      "Epoch [70/100], Train Loss: 54057.7188, Val Loss: 53583.9492\n",
      "Epoch [80/100], Train Loss: 52367.7266, Val Loss: 51886.8281\n",
      "Epoch [90/100], Train Loss: 50791.1641, Val Loss: 50303.0508\n",
      "Epoch [100/100], Train Loss: 49321.7422, Val Loss: 48826.2422\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 66639.9453, Val Loss: 66221.0547\n",
      "Epoch [20/100], Train Loss: 64246.6836, Val Loss: 63809.2266\n",
      "Epoch [30/100], Train Loss: 61919.0664, Val Loss: 61472.0469\n",
      "Epoch [40/100], Train Loss: 59720.1719, Val Loss: 59266.1641\n",
      "Epoch [50/100], Train Loss: 57659.3047, Val Loss: 57198.7578\n",
      "Epoch [60/100], Train Loss: 55732.4844, Val Loss: 55265.2617\n",
      "Epoch [70/100], Train Loss: 53932.6094, Val Loss: 53458.3750\n",
      "Epoch [80/100], Train Loss: 52251.9961, Val Loss: 51770.7188\n",
      "Epoch [90/100], Train Loss: 50684.0508, Val Loss: 50195.4805\n",
      "Epoch [100/100], Train Loss: 49222.4375, Val Loss: 48726.4492\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69344.4375, Val Loss: 69149.4297\n",
      "Epoch [20/100], Train Loss: 68788.6953, Val Loss: 68548.3672\n",
      "Epoch [30/100], Train Loss: 68105.5312, Val Loss: 67872.3203\n",
      "Epoch [40/100], Train Loss: 67619.5938, Val Loss: 67402.8672\n",
      "Epoch [50/100], Train Loss: 67248.6953, Val Loss: 67040.1328\n",
      "Epoch [60/100], Train Loss: 66925.8594, Val Loss: 66718.3281\n",
      "Epoch [70/100], Train Loss: 66623.6484, Val Loss: 66415.6484\n",
      "Epoch [80/100], Train Loss: 66334.9609, Val Loss: 66125.9062\n",
      "Epoch [90/100], Train Loss: 66056.0703, Val Loss: 65845.6406\n",
      "Epoch [100/100], Train Loss: 65784.9531, Val Loss: 65572.8516\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69328.2812, Val Loss: 69119.9844\n",
      "Epoch [20/100], Train Loss: 68672.2109, Val Loss: 68392.2109\n",
      "Epoch [30/100], Train Loss: 67828.6172, Val Loss: 67560.1562\n",
      "Epoch [40/100], Train Loss: 67254.8906, Val Loss: 67019.1875\n",
      "Epoch [50/100], Train Loss: 66858.4609, Val Loss: 66639.5938\n",
      "Epoch [60/100], Train Loss: 66530.1094, Val Loss: 66315.7734\n",
      "Epoch [70/100], Train Loss: 66227.5078, Val Loss: 66013.7969\n",
      "Epoch [80/100], Train Loss: 65939.1250, Val Loss: 65724.7188\n",
      "Epoch [90/100], Train Loss: 65660.8906, Val Loss: 65445.3047\n",
      "Epoch [100/100], Train Loss: 65390.3125, Val Loss: 65173.5234\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69243.3594, Val Loss: 69027.4219\n",
      "Epoch [20/100], Train Loss: 68619.0312, Val Loss: 68333.5859\n",
      "Epoch [30/100], Train Loss: 67865.3281, Val Loss: 67589.9219\n",
      "Epoch [40/100], Train Loss: 67349.2656, Val Loss: 67111.5547\n",
      "Epoch [50/100], Train Loss: 66974.6875, Val Loss: 66754.5078\n",
      "Epoch [60/100], Train Loss: 66653.7812, Val Loss: 66438.8984\n",
      "Epoch [70/100], Train Loss: 66355.3438, Val Loss: 66141.2812\n",
      "Epoch [80/100], Train Loss: 66069.7812, Val Loss: 65855.3359\n",
      "Epoch [90/100], Train Loss: 65793.7188, Val Loss: 65578.3984\n",
      "Epoch [100/100], Train Loss: 65525.3203, Val Loss: 65308.5898\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69556.2734, Val Loss: 69400.9141\n",
      "Epoch [20/100], Train Loss: 69531.3828, Val Loss: 69375.3203\n",
      "Epoch [30/100], Train Loss: 69504.8047, Val Loss: 69347.9609\n",
      "Epoch [40/100], Train Loss: 69475.1641, Val Loss: 69317.2969\n",
      "Epoch [50/100], Train Loss: 69440.3828, Val Loss: 69281.6953\n",
      "Epoch [60/100], Train Loss: 69400.1250, Val Loss: 69239.4531\n",
      "Epoch [70/100], Train Loss: 69351.8047, Val Loss: 69188.7656\n",
      "Epoch [80/100], Train Loss: 69292.0781, Val Loss: 69127.8672\n",
      "Epoch [90/100], Train Loss: 69222.3672, Val Loss: 69055.3516\n",
      "Epoch [100/100], Train Loss: 69140.6719, Val Loss: 68970.5625\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69502.2969, Val Loss: 69346.5625\n",
      "Epoch [20/100], Train Loss: 69473.8203, Val Loss: 69317.7344\n",
      "Epoch [30/100], Train Loss: 69444.3125, Val Loss: 69286.9766\n",
      "Epoch [40/100], Train Loss: 69411.7266, Val Loss: 69252.8594\n",
      "Epoch [50/100], Train Loss: 69373.8359, Val Loss: 69213.6406\n",
      "Epoch [60/100], Train Loss: 69330.3906, Val Loss: 69167.4297\n",
      "Epoch [70/100], Train Loss: 69277.6484, Val Loss: 69112.0703\n",
      "Epoch [80/100], Train Loss: 69213.0781, Val Loss: 69045.3047\n",
      "Epoch [90/100], Train Loss: 69138.5781, Val Loss: 68964.9844\n",
      "Epoch [100/100], Train Loss: 69048.5547, Val Loss: 68869.7109\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69488.5000, Val Loss: 69332.3203\n",
      "Epoch [20/100], Train Loss: 69460.7891, Val Loss: 69304.1016\n",
      "Epoch [30/100], Train Loss: 69432.9922, Val Loss: 69274.0391\n",
      "Epoch [40/100], Train Loss: 69401.9141, Val Loss: 69240.7031\n",
      "Epoch [50/100], Train Loss: 69365.7266, Val Loss: 69202.4375\n",
      "Epoch [60/100], Train Loss: 69323.0000, Val Loss: 69157.3516\n",
      "Epoch [70/100], Train Loss: 69272.1016, Val Loss: 69103.2266\n",
      "Epoch [80/100], Train Loss: 69212.6875, Val Loss: 69037.7656\n",
      "Epoch [90/100], Train Loss: 69140.6016, Val Loss: 68958.8438\n",
      "Epoch [100/100], Train Loss: 69051.9297, Val Loss: 68864.9609\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 67086.7969, Val Loss: 66580.1562\n",
      "Epoch [20/100], Train Loss: 62828.8281, Val Loss: 62214.1484\n",
      "Epoch [30/100], Train Loss: 57998.7656, Val Loss: 57278.3320\n",
      "Epoch [40/100], Train Loss: 53252.1094, Val Loss: 52490.2656\n",
      "Epoch [50/100], Train Loss: 48820.3398, Val Loss: 48066.8086\n",
      "Epoch [60/100], Train Loss: 44712.2109, Val Loss: 43969.3320\n",
      "Epoch [70/100], Train Loss: 40967.4766, Val Loss: 40255.9453\n",
      "Epoch [80/100], Train Loss: 37733.9492, Val Loss: 37059.6406\n",
      "Epoch [90/100], Train Loss: 34969.7148, Val Loss: 34332.8203\n",
      "Epoch [100/100], Train Loss: 32554.6426, Val Loss: 31957.2617\n",
      "New best score: 31957.26171875 with params: {'hidden_dim': 128, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 67201.3203, Val Loss: 66699.2188\n",
      "Epoch [20/100], Train Loss: 62972.3633, Val Loss: 62364.6094\n",
      "Epoch [30/100], Train Loss: 58144.9961, Val Loss: 57427.0156\n",
      "Epoch [40/100], Train Loss: 53398.0000, Val Loss: 52638.1953\n",
      "Epoch [50/100], Train Loss: 48967.3320, Val Loss: 48213.4883\n",
      "Epoch [60/100], Train Loss: 44821.6719, Val Loss: 44071.7656\n",
      "Epoch [70/100], Train Loss: 41026.3672, Val Loss: 40306.7656\n",
      "Epoch [80/100], Train Loss: 37763.0508, Val Loss: 37081.4727\n",
      "Epoch [90/100], Train Loss: 34981.1367, Val Loss: 34337.5664\n",
      "Epoch [100/100], Train Loss: 32552.8379, Val Loss: 31951.1172\n",
      "New best score: 31951.1171875 with params: {'hidden_dim': 128, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67103.1094, Val Loss: 66597.0312\n",
      "Epoch [20/100], Train Loss: 62844.5078, Val Loss: 62232.6719\n",
      "Epoch [30/100], Train Loss: 57998.3516, Val Loss: 57275.5430\n",
      "Epoch [40/100], Train Loss: 53217.6875, Val Loss: 52451.9883\n",
      "Epoch [50/100], Train Loss: 48765.5469, Val Loss: 48006.2500\n",
      "Epoch [60/100], Train Loss: 44624.0195, Val Loss: 43873.5430\n",
      "Epoch [70/100], Train Loss: 40883.3594, Val Loss: 40165.7461\n",
      "Epoch [80/100], Train Loss: 37656.8711, Val Loss: 36977.7227\n",
      "Epoch [90/100], Train Loss: 34902.0508, Val Loss: 34261.3086\n",
      "Epoch [100/100], Train Loss: 32493.9316, Val Loss: 31891.4297\n",
      "New best score: 31891.4296875 with params: {'hidden_dim': 128, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.3}\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69377.1562, Val Loss: 69209.2422\n",
      "Epoch [20/100], Train Loss: 69167.1172, Val Loss: 68995.2422\n",
      "Epoch [30/100], Train Loss: 68898.9844, Val Loss: 68722.0234\n",
      "Epoch [40/100], Train Loss: 68562.1094, Val Loss: 68380.6484\n",
      "Epoch [50/100], Train Loss: 68158.5469, Val Loss: 67973.9922\n",
      "Epoch [60/100], Train Loss: 67697.4844, Val Loss: 67511.2812\n",
      "Epoch [70/100], Train Loss: 67190.5625, Val Loss: 67003.7188\n",
      "Epoch [80/100], Train Loss: 66649.4531, Val Loss: 66462.3047\n",
      "Epoch [90/100], Train Loss: 66084.2812, Val Loss: 65896.4141\n",
      "Epoch [100/100], Train Loss: 65503.1523, Val Loss: 65313.3984\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69418.6016, Val Loss: 69250.8672\n",
      "Epoch [20/100], Train Loss: 69201.0547, Val Loss: 69030.0938\n",
      "Epoch [30/100], Train Loss: 68924.2344, Val Loss: 68748.7188\n",
      "Epoch [40/100], Train Loss: 68576.0391, Val Loss: 68396.3984\n",
      "Epoch [50/100], Train Loss: 68157.4453, Val Loss: 67975.0625\n",
      "Epoch [60/100], Train Loss: 67678.0469, Val Loss: 67494.5703\n",
      "Epoch [70/100], Train Loss: 67152.1328, Val Loss: 66968.8672\n",
      "Epoch [80/100], Train Loss: 66594.1953, Val Loss: 66411.5859\n",
      "Epoch [90/100], Train Loss: 66016.0781, Val Loss: 65833.6719\n",
      "Epoch [100/100], Train Loss: 65426.3789, Val Loss: 65242.8633\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69410.6328, Val Loss: 69240.2734\n",
      "Epoch [20/100], Train Loss: 69196.1094, Val Loss: 69022.8359\n",
      "Epoch [30/100], Train Loss: 68925.4844, Val Loss: 68748.1016\n",
      "Epoch [40/100], Train Loss: 68586.4141, Val Loss: 68405.3984\n",
      "Epoch [50/100], Train Loss: 68179.7969, Val Loss: 67996.4844\n",
      "Epoch [60/100], Train Loss: 67714.5469, Val Loss: 67530.2500\n",
      "Epoch [70/100], Train Loss: 67202.1875, Val Loss: 67017.7734\n",
      "Epoch [80/100], Train Loss: 66654.5781, Val Loss: 66470.4531\n",
      "Epoch [90/100], Train Loss: 66082.6562, Val Loss: 65898.2969\n",
      "Epoch [100/100], Train Loss: 65495.2422, Val Loss: 65309.3359\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69544.4062, Val Loss: 69391.0938\n",
      "Epoch [20/100], Train Loss: 69525.4766, Val Loss: 69372.2969\n",
      "Epoch [30/100], Train Loss: 69506.5234, Val Loss: 69353.4609\n",
      "Epoch [40/100], Train Loss: 69487.4375, Val Loss: 69334.5078\n",
      "Epoch [50/100], Train Loss: 69468.1484, Val Loss: 69315.3359\n",
      "Epoch [60/100], Train Loss: 69448.5312, Val Loss: 69295.8516\n",
      "Epoch [70/100], Train Loss: 69428.4922, Val Loss: 69275.9141\n",
      "Epoch [80/100], Train Loss: 69407.9297, Val Loss: 69255.4766\n",
      "Epoch [90/100], Train Loss: 69386.7578, Val Loss: 69234.4141\n",
      "Epoch [100/100], Train Loss: 69364.8906, Val Loss: 69212.6719\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69544.7812, Val Loss: 69388.4531\n",
      "Epoch [20/100], Train Loss: 69525.7266, Val Loss: 69369.5156\n",
      "Epoch [30/100], Train Loss: 69506.5234, Val Loss: 69350.4062\n",
      "Epoch [40/100], Train Loss: 69487.0156, Val Loss: 69331.0000\n",
      "Epoch [50/100], Train Loss: 69467.0781, Val Loss: 69311.1562\n",
      "Epoch [60/100], Train Loss: 69446.6016, Val Loss: 69290.7734\n",
      "Epoch [70/100], Train Loss: 69425.4922, Val Loss: 69269.7656\n",
      "Epoch [80/100], Train Loss: 69403.6875, Val Loss: 69248.0391\n",
      "Epoch [90/100], Train Loss: 69381.0938, Val Loss: 69225.5469\n",
      "Epoch [100/100], Train Loss: 69357.6875, Val Loss: 69202.2344\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69494.0156, Val Loss: 69340.3594\n",
      "Epoch [20/100], Train Loss: 69475.1562, Val Loss: 69321.5703\n",
      "Epoch [30/100], Train Loss: 69456.2109, Val Loss: 69302.7109\n",
      "Epoch [40/100], Train Loss: 69437.1484, Val Loss: 69283.7031\n",
      "Epoch [50/100], Train Loss: 69417.8438, Val Loss: 69264.4688\n",
      "Epoch [60/100], Train Loss: 69398.2031, Val Loss: 69244.8984\n",
      "Epoch [70/100], Train Loss: 69378.1328, Val Loss: 69224.9062\n",
      "Epoch [80/100], Train Loss: 69357.5469, Val Loss: 69204.4219\n",
      "Epoch [90/100], Train Loss: 69336.3672, Val Loss: 69183.3203\n",
      "Epoch [100/100], Train Loss: 69314.5156, Val Loss: 69161.5625\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 64849.4453, Val Loss: 64109.2734\n",
      "Epoch [20/100], Train Loss: 59870.0820, Val Loss: 59165.5898\n",
      "Epoch [30/100], Train Loss: 55519.5469, Val Loss: 54841.3086\n",
      "Epoch [40/100], Train Loss: 51712.1250, Val Loss: 51017.8672\n",
      "Epoch [50/100], Train Loss: 48279.0625, Val Loss: 47579.0508\n",
      "Epoch [60/100], Train Loss: 45141.4336, Val Loss: 44419.2812\n",
      "Epoch [70/100], Train Loss: 42036.4102, Val Loss: 41391.5977\n",
      "Epoch [80/100], Train Loss: 39362.0195, Val Loss: 38722.4961\n",
      "Epoch [90/100], Train Loss: 36887.9141, Val Loss: 36277.2461\n",
      "Epoch [100/100], Train Loss: 34578.0078, Val Loss: 34011.7617\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 64688.7930, Val Loss: 63939.3125\n",
      "Epoch [20/100], Train Loss: 59756.5898, Val Loss: 59046.9453\n",
      "Epoch [30/100], Train Loss: 55501.0117, Val Loss: 54798.0859\n",
      "Epoch [40/100], Train Loss: 51685.3555, Val Loss: 50969.5625\n",
      "Epoch [50/100], Train Loss: 47977.9336, Val Loss: 47268.1758\n",
      "Epoch [60/100], Train Loss: 44729.3320, Val Loss: 44029.7617\n",
      "Epoch [70/100], Train Loss: 41746.3320, Val Loss: 41106.0781\n",
      "Epoch [80/100], Train Loss: 39115.3320, Val Loss: 38501.1367\n",
      "Epoch [90/100], Train Loss: 36706.3359, Val Loss: 36086.0117\n",
      "Epoch [100/100], Train Loss: 34473.1758, Val Loss: 33898.5195\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 64820.1016, Val Loss: 64072.9570\n",
      "Epoch [20/100], Train Loss: 59867.2891, Val Loss: 59162.8047\n",
      "Epoch [30/100], Train Loss: 55520.4609, Val Loss: 54840.4805\n",
      "Epoch [40/100], Train Loss: 51706.5117, Val Loss: 51003.9375\n",
      "Epoch [50/100], Train Loss: 48267.5117, Val Loss: 47568.9570\n",
      "Epoch [60/100], Train Loss: 45133.1914, Val Loss: 44412.8281\n",
      "Epoch [70/100], Train Loss: 42036.3398, Val Loss: 41379.7695\n",
      "Epoch [80/100], Train Loss: 39382.9648, Val Loss: 38741.2109\n",
      "Epoch [90/100], Train Loss: 36930.0312, Val Loss: 36299.1602\n",
      "Epoch [100/100], Train Loss: 34662.9297, Val Loss: 34078.0078\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69183.8281, Val Loss: 68968.0625\n",
      "Epoch [20/100], Train Loss: 68403.6484, Val Loss: 68149.3594\n",
      "Epoch [30/100], Train Loss: 67405.2422, Val Loss: 67156.4922\n",
      "Epoch [40/100], Train Loss: 66470.6172, Val Loss: 66214.7656\n",
      "Epoch [50/100], Train Loss: 65598.2031, Val Loss: 65329.1328\n",
      "Epoch [60/100], Train Loss: 64804.6367, Val Loss: 64529.0742\n",
      "Epoch [70/100], Train Loss: 64057.3867, Val Loss: 63776.8281\n",
      "Epoch [80/100], Train Loss: 63364.1602, Val Loss: 63083.2656\n",
      "Epoch [90/100], Train Loss: 62722.7109, Val Loss: 62445.6523\n",
      "Epoch [100/100], Train Loss: 62118.3750, Val Loss: 61843.4688\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69160.5781, Val Loss: 68951.8438\n",
      "Epoch [20/100], Train Loss: 68440.7656, Val Loss: 68195.6562\n",
      "Epoch [30/100], Train Loss: 67518.4688, Val Loss: 67280.5938\n",
      "Epoch [40/100], Train Loss: 66643.3672, Val Loss: 66392.3516\n",
      "Epoch [50/100], Train Loss: 65809.2422, Val Loss: 65544.3359\n",
      "Epoch [60/100], Train Loss: 65044.5078, Val Loss: 64773.1562\n",
      "Epoch [70/100], Train Loss: 64338.3906, Val Loss: 64063.3203\n",
      "Epoch [80/100], Train Loss: 63673.2109, Val Loss: 63400.1367\n",
      "Epoch [90/100], Train Loss: 63031.3555, Val Loss: 62757.5156\n",
      "Epoch [100/100], Train Loss: 62427.2773, Val Loss: 62155.6328\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69131.8203, Val Loss: 68910.6250\n",
      "Epoch [20/100], Train Loss: 68363.3359, Val Loss: 68097.0469\n",
      "Epoch [30/100], Train Loss: 67396.4297, Val Loss: 67132.4766\n",
      "Epoch [40/100], Train Loss: 66480.7891, Val Loss: 66213.8594\n",
      "Epoch [50/100], Train Loss: 65630.8047, Val Loss: 65348.8906\n",
      "Epoch [60/100], Train Loss: 64835.3477, Val Loss: 64549.5078\n",
      "Epoch [70/100], Train Loss: 64098.6602, Val Loss: 63814.2773\n",
      "Epoch [80/100], Train Loss: 63422.9805, Val Loss: 63144.6016\n",
      "Epoch [90/100], Train Loss: 62788.5703, Val Loss: 62509.8008\n",
      "Epoch [100/100], Train Loss: 62195.2031, Val Loss: 61920.1055\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69533.6016, Val Loss: 69380.9062\n",
      "Epoch [20/100], Train Loss: 69503.6797, Val Loss: 69350.5938\n",
      "Epoch [30/100], Train Loss: 69470.8281, Val Loss: 69317.8828\n",
      "Epoch [40/100], Train Loss: 69434.7031, Val Loss: 69281.2422\n",
      "Epoch [50/100], Train Loss: 69393.3516, Val Loss: 69239.1719\n",
      "Epoch [60/100], Train Loss: 69345.0391, Val Loss: 69190.3203\n",
      "Epoch [70/100], Train Loss: 69288.8906, Val Loss: 69133.4766\n",
      "Epoch [80/100], Train Loss: 69223.8203, Val Loss: 69067.6719\n",
      "Epoch [90/100], Train Loss: 69149.1172, Val Loss: 68992.2188\n",
      "Epoch [100/100], Train Loss: 69063.8438, Val Loss: 68906.8359\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69478.3750, Val Loss: 69323.3828\n",
      "Epoch [20/100], Train Loss: 69442.6328, Val Loss: 69286.3828\n",
      "Epoch [30/100], Train Loss: 69403.1641, Val Loss: 69246.5469\n",
      "Epoch [40/100], Train Loss: 69359.8906, Val Loss: 69201.9062\n",
      "Epoch [50/100], Train Loss: 69308.8828, Val Loss: 69150.5469\n",
      "Epoch [60/100], Train Loss: 69251.1719, Val Loss: 69090.7578\n",
      "Epoch [70/100], Train Loss: 69183.3359, Val Loss: 69021.1328\n",
      "Epoch [80/100], Train Loss: 69104.5234, Val Loss: 68940.7188\n",
      "Epoch [90/100], Train Loss: 69014.6172, Val Loss: 68849.0547\n",
      "Epoch [100/100], Train Loss: 68914.1172, Val Loss: 68746.3203\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69523.6797, Val Loss: 69369.3906\n",
      "Epoch [20/100], Train Loss: 69489.3359, Val Loss: 69334.0938\n",
      "Epoch [30/100], Train Loss: 69452.7422, Val Loss: 69296.5469\n",
      "Epoch [40/100], Train Loss: 69412.1328, Val Loss: 69255.0547\n",
      "Epoch [50/100], Train Loss: 69367.6328, Val Loss: 69207.9766\n",
      "Epoch [60/100], Train Loss: 69313.7109, Val Loss: 69153.8359\n",
      "Epoch [70/100], Train Loss: 69251.7656, Val Loss: 69091.2891\n",
      "Epoch [80/100], Train Loss: 69181.5234, Val Loss: 69019.2812\n",
      "Epoch [90/100], Train Loss: 69102.2500, Val Loss: 68937.0078\n",
      "Epoch [100/100], Train Loss: 69013.0469, Val Loss: 68844.0234\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 64392.9062, Val Loss: 63722.5742\n",
      "Epoch [20/100], Train Loss: 59892.5273, Val Loss: 59221.3438\n",
      "Epoch [30/100], Train Loss: 55777.7031, Val Loss: 55108.2930\n",
      "Epoch [40/100], Train Loss: 52137.1797, Val Loss: 51476.4023\n",
      "Epoch [50/100], Train Loss: 48968.1719, Val Loss: 48314.9102\n",
      "Epoch [60/100], Train Loss: 46237.4414, Val Loss: 45590.0469\n",
      "Epoch [70/100], Train Loss: 43903.7422, Val Loss: 43260.0078\n",
      "Epoch [80/100], Train Loss: 41924.8008, Val Loss: 41282.6367\n",
      "Epoch [90/100], Train Loss: 40260.0312, Val Loss: 39617.5664\n",
      "Epoch [100/100], Train Loss: 38871.2734, Val Loss: 38226.8828\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 64518.7734, Val Loss: 63846.1562\n",
      "Epoch [20/100], Train Loss: 59995.4844, Val Loss: 59320.5547\n",
      "Epoch [30/100], Train Loss: 55865.6641, Val Loss: 55195.7500\n",
      "Epoch [40/100], Train Loss: 52211.0938, Val Loss: 51549.6602\n",
      "Epoch [50/100], Train Loss: 49026.1641, Val Loss: 48368.9023\n",
      "Epoch [60/100], Train Loss: 46170.5039, Val Loss: 45486.4648\n",
      "Epoch [70/100], Train Loss: 43560.5273, Val Loss: 42781.4570\n",
      "Epoch [80/100], Train Loss: 40296.5156, Val Loss: 39652.9258\n",
      "Epoch [90/100], Train Loss: 37807.3555, Val Loss: 37179.7773\n",
      "Epoch [100/100], Train Loss: 35507.0703, Val Loss: 34924.5312\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 64456.9375, Val Loss: 63790.9531\n",
      "Epoch [20/100], Train Loss: 59954.5508, Val Loss: 59280.2930\n",
      "Epoch [30/100], Train Loss: 55828.3242, Val Loss: 55158.4141\n",
      "Epoch [40/100], Train Loss: 52177.5625, Val Loss: 51516.0859\n",
      "Epoch [50/100], Train Loss: 49000.0625, Val Loss: 48346.4844\n",
      "Epoch [60/100], Train Loss: 46263.0273, Val Loss: 45615.0312\n",
      "Epoch [70/100], Train Loss: 43919.5234, Val Loss: 43269.4141\n",
      "Epoch [80/100], Train Loss: 41758.4219, Val Loss: 40995.2266\n",
      "Epoch [90/100], Train Loss: 38268.8086, Val Loss: 37639.0898\n",
      "Epoch [100/100], Train Loss: 35958.3828, Val Loss: 35332.0547\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69023.6328, Val Loss: 68762.9609\n",
      "Epoch [20/100], Train Loss: 67758.5156, Val Loss: 67476.2734\n",
      "Epoch [30/100], Train Loss: 66715.0938, Val Loss: 66443.6875\n",
      "Epoch [40/100], Train Loss: 65925.4219, Val Loss: 65667.8828\n",
      "Epoch [50/100], Train Loss: 65273.2852, Val Loss: 65021.9688\n",
      "Epoch [60/100], Train Loss: 64695.2031, Val Loss: 64444.8008\n",
      "Epoch [70/100], Train Loss: 64145.3359, Val Loss: 63892.0898\n",
      "Epoch [80/100], Train Loss: 63611.9609, Val Loss: 63356.0078\n",
      "Epoch [90/100], Train Loss: 63103.2734, Val Loss: 62844.7266\n",
      "Epoch [100/100], Train Loss: 62607.8086, Val Loss: 62345.0625\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69003.2266, Val Loss: 68740.4609\n",
      "Epoch [20/100], Train Loss: 67714.5469, Val Loss: 67408.6328\n",
      "Epoch [30/100], Train Loss: 66601.0938, Val Loss: 66320.8203\n",
      "Epoch [40/100], Train Loss: 65808.0781, Val Loss: 65541.6172\n",
      "Epoch [50/100], Train Loss: 65147.5156, Val Loss: 64891.2578\n",
      "Epoch [60/100], Train Loss: 64563.8008, Val Loss: 64310.1445\n",
      "Epoch [70/100], Train Loss: 64015.8750, Val Loss: 63760.8906\n",
      "Epoch [80/100], Train Loss: 63489.8750, Val Loss: 63232.2344\n",
      "Epoch [90/100], Train Loss: 62981.6445, Val Loss: 62720.3789\n",
      "Epoch [100/100], Train Loss: 62487.4883, Val Loss: 62222.9531\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69072.0938, Val Loss: 68806.3047\n",
      "Epoch [20/100], Train Loss: 67827.2969, Val Loss: 67512.9609\n",
      "Epoch [30/100], Train Loss: 66754.5703, Val Loss: 66469.3828\n",
      "Epoch [40/100], Train Loss: 65957.7344, Val Loss: 65685.7656\n",
      "Epoch [50/100], Train Loss: 65280.3438, Val Loss: 65020.4492\n",
      "Epoch [60/100], Train Loss: 64683.2734, Val Loss: 64426.3555\n",
      "Epoch [70/100], Train Loss: 64128.8672, Val Loss: 63873.0625\n",
      "Epoch [80/100], Train Loss: 63599.2070, Val Loss: 63341.6523\n",
      "Epoch [90/100], Train Loss: 63088.2969, Val Loss: 62827.2539\n",
      "Epoch [100/100], Train Loss: 62591.4766, Val Loss: 62326.9805\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69480.9453, Val Loss: 69324.6094\n",
      "Epoch [20/100], Train Loss: 69437.4297, Val Loss: 69280.1328\n",
      "Epoch [30/100], Train Loss: 69387.4297, Val Loss: 69228.4766\n",
      "Epoch [40/100], Train Loss: 69326.2344, Val Loss: 69164.6797\n",
      "Epoch [50/100], Train Loss: 69246.9141, Val Loss: 69083.1328\n",
      "Epoch [60/100], Train Loss: 69145.6797, Val Loss: 68978.0938\n",
      "Epoch [70/100], Train Loss: 69017.0156, Val Loss: 68844.6953\n",
      "Epoch [80/100], Train Loss: 68856.2188, Val Loss: 68680.5703\n",
      "Epoch [90/100], Train Loss: 68667.5547, Val Loss: 68487.7969\n",
      "Epoch [100/100], Train Loss: 68454.1484, Val Loss: 68273.3281\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69511.0938, Val Loss: 69354.4375\n",
      "Epoch [20/100], Train Loss: 69469.0312, Val Loss: 69310.3281\n",
      "Epoch [30/100], Train Loss: 69419.7812, Val Loss: 69258.8594\n",
      "Epoch [40/100], Train Loss: 69358.2812, Val Loss: 69194.5703\n",
      "Epoch [50/100], Train Loss: 69278.2891, Val Loss: 69111.3438\n",
      "Epoch [60/100], Train Loss: 69175.8203, Val Loss: 69002.9062\n",
      "Epoch [70/100], Train Loss: 69041.3594, Val Loss: 68863.8438\n",
      "Epoch [80/100], Train Loss: 68874.2891, Val Loss: 68691.7109\n",
      "Epoch [90/100], Train Loss: 68679.8516, Val Loss: 68489.1641\n",
      "Epoch [100/100], Train Loss: 68458.5156, Val Loss: 68264.5234\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69535.7656, Val Loss: 69379.6094\n",
      "Epoch [20/100], Train Loss: 69495.2031, Val Loss: 69337.2891\n",
      "Epoch [30/100], Train Loss: 69449.1406, Val Loss: 69288.8906\n",
      "Epoch [40/100], Train Loss: 69391.5234, Val Loss: 69229.3750\n",
      "Epoch [50/100], Train Loss: 69321.3281, Val Loss: 69153.0547\n",
      "Epoch [60/100], Train Loss: 69227.5703, Val Loss: 69053.8516\n",
      "Epoch [70/100], Train Loss: 69107.5625, Val Loss: 68926.2266\n",
      "Epoch [80/100], Train Loss: 68955.3125, Val Loss: 68767.0781\n",
      "Epoch [90/100], Train Loss: 68774.6484, Val Loss: 68577.7578\n",
      "Epoch [100/100], Train Loss: 68570.0781, Val Loss: 68365.4844\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 64370.4844, Val Loss: 63720.6719\n",
      "Epoch [20/100], Train Loss: 59945.3203, Val Loss: 59277.0430\n",
      "Epoch [30/100], Train Loss: 55874.5703, Val Loss: 55209.8672\n",
      "Epoch [40/100], Train Loss: 52263.5625, Val Loss: 51606.2031\n",
      "Epoch [50/100], Train Loss: 49111.7773, Val Loss: 48460.3477\n",
      "Epoch [60/100], Train Loss: 46388.6367, Val Loss: 45740.4648\n",
      "Epoch [70/100], Train Loss: 44059.4258, Val Loss: 43416.3359\n",
      "Epoch [80/100], Train Loss: 42059.7812, Val Loss: 41422.6641\n",
      "Epoch [90/100], Train Loss: 40413.2500, Val Loss: 39749.0469\n",
      "Epoch [100/100], Train Loss: 38770.5273, Val Loss: 38066.7656\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 64433.7695, Val Loss: 63782.8594\n",
      "Epoch [20/100], Train Loss: 60004.8203, Val Loss: 59334.1211\n",
      "Epoch [30/100], Train Loss: 55933.9414, Val Loss: 55267.6484\n",
      "Epoch [40/100], Train Loss: 52320.2031, Val Loss: 51663.8438\n",
      "Epoch [50/100], Train Loss: 49180.1328, Val Loss: 48516.5234\n",
      "Epoch [60/100], Train Loss: 46437.3125, Val Loss: 45793.0977\n",
      "Epoch [70/100], Train Loss: 44097.8164, Val Loss: 43456.8906\n",
      "Epoch [80/100], Train Loss: 42118.6367, Val Loss: 41467.8516\n",
      "Epoch [90/100], Train Loss: 40431.4375, Val Loss: 39788.3047\n",
      "Epoch [100/100], Train Loss: 39022.3438, Val Loss: 38380.4375\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 64118.0352, Val Loss: 63463.0234\n",
      "Epoch [20/100], Train Loss: 59706.3945, Val Loss: 59038.5547\n",
      "Epoch [30/100], Train Loss: 55664.1641, Val Loss: 55000.1133\n",
      "Epoch [40/100], Train Loss: 52080.8203, Val Loss: 51423.7891\n",
      "Epoch [50/100], Train Loss: 48952.1953, Val Loss: 48302.0039\n",
      "Epoch [60/100], Train Loss: 46248.8203, Val Loss: 45603.7539\n",
      "Epoch [70/100], Train Loss: 43936.4102, Val Loss: 43290.6836\n",
      "Epoch [80/100], Train Loss: 41964.5430, Val Loss: 41323.3008\n",
      "Epoch [90/100], Train Loss: 40317.7578, Val Loss: 39670.3789\n",
      "Epoch [100/100], Train Loss: 38920.3828, Val Loss: 38274.1172\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68672.3906, Val Loss: 68320.0547\n",
      "Epoch [20/100], Train Loss: 66937.6094, Val Loss: 66641.3203\n",
      "Epoch [30/100], Train Loss: 66093.4297, Val Loss: 65842.4297\n",
      "Epoch [40/100], Train Loss: 65496.6328, Val Loss: 65252.4883\n",
      "Epoch [50/100], Train Loss: 64946.2930, Val Loss: 64700.1562\n",
      "Epoch [60/100], Train Loss: 64418.4922, Val Loss: 64169.6875\n",
      "Epoch [70/100], Train Loss: 63908.3945, Val Loss: 63656.8359\n",
      "Epoch [80/100], Train Loss: 63412.5469, Val Loss: 63158.1953\n",
      "Epoch [90/100], Train Loss: 62929.3086, Val Loss: 62670.7188\n",
      "Epoch [100/100], Train Loss: 62455.9219, Val Loss: 62192.9492\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68715.0156, Val Loss: 68372.4688\n",
      "Epoch [20/100], Train Loss: 67096.6641, Val Loss: 66784.6406\n",
      "Epoch [30/100], Train Loss: 66253.7969, Val Loss: 66002.4375\n",
      "Epoch [40/100], Train Loss: 65657.5703, Val Loss: 65413.0898\n",
      "Epoch [50/100], Train Loss: 65103.5234, Val Loss: 64857.9648\n",
      "Epoch [60/100], Train Loss: 64571.8789, Val Loss: 64323.9531\n",
      "Epoch [70/100], Train Loss: 64058.6094, Val Loss: 63808.0703\n",
      "Epoch [80/100], Train Loss: 63560.6367, Val Loss: 63306.9961\n",
      "Epoch [90/100], Train Loss: 63075.6328, Val Loss: 62818.4570\n",
      "Epoch [100/100], Train Loss: 62601.5078, Val Loss: 62338.6914\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68845.0703, Val Loss: 68498.3672\n",
      "Epoch [20/100], Train Loss: 67196.2969, Val Loss: 66867.7031\n",
      "Epoch [30/100], Train Loss: 66335.9062, Val Loss: 66076.9609\n",
      "Epoch [40/100], Train Loss: 65725.9297, Val Loss: 65480.0547\n",
      "Epoch [50/100], Train Loss: 65164.9688, Val Loss: 64918.3477\n",
      "Epoch [60/100], Train Loss: 64627.3516, Val Loss: 64378.8906\n",
      "Epoch [70/100], Train Loss: 64109.4297, Val Loss: 63858.5039\n",
      "Epoch [80/100], Train Loss: 63607.6562, Val Loss: 63353.5352\n",
      "Epoch [90/100], Train Loss: 63119.9961, Val Loss: 62861.3984\n",
      "Epoch [100/100], Train Loss: 62643.0586, Val Loss: 62381.3320\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69483.8359, Val Loss: 69325.2812\n",
      "Epoch [20/100], Train Loss: 69431.2031, Val Loss: 69271.1406\n",
      "Epoch [30/100], Train Loss: 69366.0156, Val Loss: 69203.4219\n",
      "Epoch [40/100], Train Loss: 69278.1328, Val Loss: 69110.2266\n",
      "Epoch [50/100], Train Loss: 69150.6328, Val Loss: 68976.4688\n",
      "Epoch [60/100], Train Loss: 68969.8438, Val Loss: 68786.0156\n",
      "Epoch [70/100], Train Loss: 68722.5234, Val Loss: 68529.9922\n",
      "Epoch [80/100], Train Loss: 68419.3750, Val Loss: 68218.6094\n",
      "Epoch [90/100], Train Loss: 68086.5625, Val Loss: 67882.0781\n",
      "Epoch [100/100], Train Loss: 67760.1641, Val Loss: 67553.7031\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69476.7578, Val Loss: 69318.4922\n",
      "Epoch [20/100], Train Loss: 69425.2422, Val Loss: 69265.0469\n",
      "Epoch [30/100], Train Loss: 69361.7578, Val Loss: 69197.9141\n",
      "Epoch [40/100], Train Loss: 69276.2109, Val Loss: 69106.8516\n",
      "Epoch [50/100], Train Loss: 69154.3281, Val Loss: 68979.0234\n",
      "Epoch [60/100], Train Loss: 68986.9453, Val Loss: 68801.1484\n",
      "Epoch [70/100], Train Loss: 68763.7578, Val Loss: 68566.9844\n",
      "Epoch [80/100], Train Loss: 68494.9062, Val Loss: 68285.8828\n",
      "Epoch [90/100], Train Loss: 68200.6875, Val Loss: 67983.2891\n",
      "Epoch [100/100], Train Loss: 67909.4297, Val Loss: 67687.6250\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69516.3750, Val Loss: 69357.6875\n",
      "Epoch [20/100], Train Loss: 69467.5625, Val Loss: 69306.1406\n",
      "Epoch [30/100], Train Loss: 69409.4766, Val Loss: 69245.2891\n",
      "Epoch [40/100], Train Loss: 69336.0859, Val Loss: 69165.5625\n",
      "Epoch [50/100], Train Loss: 69232.8906, Val Loss: 69054.3750\n",
      "Epoch [60/100], Train Loss: 69086.8984, Val Loss: 68897.1172\n",
      "Epoch [70/100], Train Loss: 68891.2031, Val Loss: 68681.8594\n",
      "Epoch [80/100], Train Loss: 68634.1719, Val Loss: 68409.1797\n",
      "Epoch [90/100], Train Loss: 68338.8438, Val Loss: 68098.5469\n",
      "Epoch [100/100], Train Loss: 68033.3281, Val Loss: 67780.5391\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 65144.1445, Val Loss: 64323.9336\n",
      "Epoch [20/100], Train Loss: 57319.0195, Val Loss: 56321.2148\n",
      "Epoch [30/100], Train Loss: 49095.9570, Val Loss: 47977.8750\n",
      "Epoch [40/100], Train Loss: 41594.0703, Val Loss: 40504.0273\n",
      "Epoch [50/100], Train Loss: 34791.5195, Val Loss: 33741.1133\n",
      "Epoch [60/100], Train Loss: 29225.7539, Val Loss: 28351.9863\n",
      "Epoch [70/100], Train Loss: 25237.9395, Val Loss: 24522.2129\n",
      "Epoch [80/100], Train Loss: 22071.5781, Val Loss: 21484.4785\n",
      "Epoch [90/100], Train Loss: 19519.1680, Val Loss: 19031.0723\n",
      "Epoch [100/100], Train Loss: 17428.9258, Val Loss: 17024.6172\n",
      "New best score: 17024.6171875 with params: {'hidden_dim': 256, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 65074.5938, Val Loss: 64252.5664\n",
      "Epoch [20/100], Train Loss: 57252.5742, Val Loss: 56251.2031\n",
      "Epoch [30/100], Train Loss: 49024.3516, Val Loss: 47903.0898\n",
      "Epoch [40/100], Train Loss: 41513.2930, Val Loss: 40422.9492\n",
      "Epoch [50/100], Train Loss: 34727.4570, Val Loss: 33675.1055\n",
      "Epoch [60/100], Train Loss: 29163.2266, Val Loss: 28294.0293\n",
      "Epoch [70/100], Train Loss: 25178.6836, Val Loss: 24464.1992\n",
      "Epoch [80/100], Train Loss: 22014.2285, Val Loss: 21425.3301\n",
      "Epoch [90/100], Train Loss: 19467.7871, Val Loss: 18980.2656\n",
      "Epoch [100/100], Train Loss: 17384.5801, Val Loss: 16981.5332\n",
      "New best score: 16981.533203125 with params: {'hidden_dim': 256, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 65307.2891, Val Loss: 64499.2891\n",
      "Epoch [20/100], Train Loss: 57518.5312, Val Loss: 56523.5234\n",
      "Epoch [30/100], Train Loss: 49261.8672, Val Loss: 48142.3945\n",
      "Epoch [40/100], Train Loss: 41706.2617, Val Loss: 40614.3125\n",
      "Epoch [50/100], Train Loss: 34879.2305, Val Loss: 33831.8047\n",
      "Epoch [60/100], Train Loss: 29285.6055, Val Loss: 28413.5957\n",
      "Epoch [70/100], Train Loss: 25273.7676, Val Loss: 24557.2793\n",
      "Epoch [80/100], Train Loss: 22089.5898, Val Loss: 21500.3086\n",
      "Epoch [90/100], Train Loss: 19529.7637, Val Loss: 19040.0625\n",
      "Epoch [100/100], Train Loss: 17435.6680, Val Loss: 17028.5781\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69256.2969, Val Loss: 69071.3984\n",
      "Epoch [20/100], Train Loss: 68876.7422, Val Loss: 68682.0078\n",
      "Epoch [30/100], Train Loss: 68357.5625, Val Loss: 68151.2500\n",
      "Epoch [40/100], Train Loss: 67686.6641, Val Loss: 67470.8594\n",
      "Epoch [50/100], Train Loss: 66879.0938, Val Loss: 66657.3594\n",
      "Epoch [60/100], Train Loss: 65963.6641, Val Loss: 65739.3750\n",
      "Epoch [70/100], Train Loss: 64972.5000, Val Loss: 64747.6289\n",
      "Epoch [80/100], Train Loss: 63933.6133, Val Loss: 63708.7891\n",
      "Epoch [90/100], Train Loss: 62869.1406, Val Loss: 62643.1289\n",
      "Epoch [100/100], Train Loss: 61795.1602, Val Loss: 61564.5703\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69292.8359, Val Loss: 69112.8828\n",
      "Epoch [20/100], Train Loss: 68941.0703, Val Loss: 68751.7500\n",
      "Epoch [30/100], Train Loss: 68451.6484, Val Loss: 68250.7578\n",
      "Epoch [40/100], Train Loss: 67807.5859, Val Loss: 67596.6094\n",
      "Epoch [50/100], Train Loss: 67018.9766, Val Loss: 66801.0625\n",
      "Epoch [60/100], Train Loss: 66112.7500, Val Loss: 65891.4766\n",
      "Epoch [70/100], Train Loss: 65122.7109, Val Loss: 64900.9180\n",
      "Epoch [80/100], Train Loss: 64080.0781, Val Loss: 63858.9414\n",
      "Epoch [90/100], Train Loss: 63009.8633, Val Loss: 62788.1211\n",
      "Epoch [100/100], Train Loss: 61929.9141, Val Loss: 61703.9414\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69272.8516, Val Loss: 69090.7422\n",
      "Epoch [20/100], Train Loss: 68913.7578, Val Loss: 68722.1953\n",
      "Epoch [30/100], Train Loss: 68414.6016, Val Loss: 68211.9844\n",
      "Epoch [40/100], Train Loss: 67760.6094, Val Loss: 67548.6562\n",
      "Epoch [50/100], Train Loss: 66963.8438, Val Loss: 66745.8438\n",
      "Epoch [60/100], Train Loss: 66053.1094, Val Loss: 65832.5391\n",
      "Epoch [70/100], Train Loss: 65062.9531, Val Loss: 64842.1719\n",
      "Epoch [80/100], Train Loss: 64023.7773, Val Loss: 63803.5195\n",
      "Epoch [90/100], Train Loss: 62958.7891, Val Loss: 62737.5938\n",
      "Epoch [100/100], Train Loss: 61884.3359, Val Loss: 61658.5938\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69527.3281, Val Loss: 69372.1719\n",
      "Epoch [20/100], Train Loss: 69499.6719, Val Loss: 69344.7344\n",
      "Epoch [30/100], Train Loss: 69471.7188, Val Loss: 69316.9688\n",
      "Epoch [40/100], Train Loss: 69443.1562, Val Loss: 69288.5703\n",
      "Epoch [50/100], Train Loss: 69413.6719, Val Loss: 69259.2188\n",
      "Epoch [60/100], Train Loss: 69382.9922, Val Loss: 69228.6641\n",
      "Epoch [70/100], Train Loss: 69350.8516, Val Loss: 69196.6484\n",
      "Epoch [80/100], Train Loss: 69317.0312, Val Loss: 69162.9453\n",
      "Epoch [90/100], Train Loss: 69281.3438, Val Loss: 69127.3750\n",
      "Epoch [100/100], Train Loss: 69243.6016, Val Loss: 69089.7578\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69521.0234, Val Loss: 69363.6562\n",
      "Epoch [20/100], Train Loss: 69492.7109, Val Loss: 69335.5000\n",
      "Epoch [30/100], Train Loss: 69463.9297, Val Loss: 69306.8594\n",
      "Epoch [40/100], Train Loss: 69434.3828, Val Loss: 69277.4141\n",
      "Epoch [50/100], Train Loss: 69403.7266, Val Loss: 69246.8281\n",
      "Epoch [60/100], Train Loss: 69371.6797, Val Loss: 69214.8125\n",
      "Epoch [70/100], Train Loss: 69337.9766, Val Loss: 69181.1328\n",
      "Epoch [80/100], Train Loss: 69302.4141, Val Loss: 69145.5859\n",
      "Epoch [90/100], Train Loss: 69264.8047, Val Loss: 69107.9844\n",
      "Epoch [100/100], Train Loss: 69225.0312, Val Loss: 69068.2109\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69550.1406, Val Loss: 69396.7031\n",
      "Epoch [20/100], Train Loss: 69522.1562, Val Loss: 69368.8281\n",
      "Epoch [30/100], Train Loss: 69493.7891, Val Loss: 69340.5391\n",
      "Epoch [40/100], Train Loss: 69464.7500, Val Loss: 69311.5547\n",
      "Epoch [50/100], Train Loss: 69434.6953, Val Loss: 69281.5469\n",
      "Epoch [60/100], Train Loss: 69403.3359, Val Loss: 69250.2344\n",
      "Epoch [70/100], Train Loss: 69370.4297, Val Loss: 69217.3672\n",
      "Epoch [80/100], Train Loss: 69335.7188, Val Loss: 69182.7031\n",
      "Epoch [90/100], Train Loss: 69299.0391, Val Loss: 69146.0703\n",
      "Epoch [100/100], Train Loss: 69260.2109, Val Loss: 69107.3203\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 60594.0508, Val Loss: 59399.5547\n",
      "Epoch [20/100], Train Loss: 52193.4102, Val Loss: 51131.6797\n",
      "Epoch [30/100], Train Loss: 45527.5000, Val Loss: 44518.4141\n",
      "Epoch [40/100], Train Loss: 39553.9414, Val Loss: 38539.7812\n",
      "Epoch [50/100], Train Loss: 34491.6328, Val Loss: 33670.0312\n",
      "Epoch [60/100], Train Loss: 30388.2832, Val Loss: 29678.1367\n",
      "Epoch [70/100], Train Loss: 26971.7852, Val Loss: 26370.8652\n",
      "Epoch [80/100], Train Loss: 24001.6289, Val Loss: 23483.0469\n",
      "Epoch [90/100], Train Loss: 21473.3184, Val Loss: 21031.9707\n",
      "Epoch [100/100], Train Loss: 19325.9570, Val Loss: 18968.0957\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 60808.7539, Val Loss: 59617.2930\n",
      "Epoch [20/100], Train Loss: 52481.6992, Val Loss: 51382.4844\n",
      "Epoch [30/100], Train Loss: 45660.7227, Val Loss: 44635.2305\n",
      "Epoch [40/100], Train Loss: 39674.5430, Val Loss: 38658.9805\n",
      "Epoch [50/100], Train Loss: 34584.6680, Val Loss: 33770.7773\n",
      "Epoch [60/100], Train Loss: 30480.5918, Val Loss: 29751.7520\n",
      "Epoch [70/100], Train Loss: 27067.7012, Val Loss: 26448.7988\n",
      "Epoch [80/100], Train Loss: 24123.0645, Val Loss: 23586.1289\n",
      "Epoch [90/100], Train Loss: 21562.0781, Val Loss: 21098.0977\n",
      "Epoch [100/100], Train Loss: 19419.3398, Val Loss: 19015.6816\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 60790.4883, Val Loss: 59613.6289\n",
      "Epoch [20/100], Train Loss: 52334.5938, Val Loss: 51284.1836\n",
      "Epoch [30/100], Train Loss: 45677.7773, Val Loss: 44666.9297\n",
      "Epoch [40/100], Train Loss: 39784.4766, Val Loss: 38752.3867\n",
      "Epoch [50/100], Train Loss: 34626.7578, Val Loss: 33801.7773\n",
      "Epoch [60/100], Train Loss: 30504.1367, Val Loss: 29780.4844\n",
      "Epoch [70/100], Train Loss: 27099.8848, Val Loss: 26472.9004\n",
      "Epoch [80/100], Train Loss: 24160.0293, Val Loss: 23596.7559\n",
      "Epoch [90/100], Train Loss: 21611.1719, Val Loss: 21118.8750\n",
      "Epoch [100/100], Train Loss: 19481.4434, Val Loss: 19036.9473\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68736.9922, Val Loss: 68438.2578\n",
      "Epoch [20/100], Train Loss: 67072.5078, Val Loss: 66757.5781\n",
      "Epoch [30/100], Train Loss: 65461.0195, Val Loss: 65129.7930\n",
      "Epoch [40/100], Train Loss: 63950.7812, Val Loss: 63597.9883\n",
      "Epoch [50/100], Train Loss: 62549.5625, Val Loss: 62182.5195\n",
      "Epoch [60/100], Train Loss: 61250.8789, Val Loss: 60890.7305\n",
      "Epoch [70/100], Train Loss: 60094.5039, Val Loss: 59738.0234\n",
      "Epoch [80/100], Train Loss: 58983.6367, Val Loss: 58625.3711\n",
      "Epoch [90/100], Train Loss: 57927.9023, Val Loss: 57570.4297\n",
      "Epoch [100/100], Train Loss: 56951.1602, Val Loss: 56593.2773\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68698.9297, Val Loss: 68392.5469\n",
      "Epoch [20/100], Train Loss: 66988.8125, Val Loss: 66662.2578\n",
      "Epoch [30/100], Train Loss: 65349.8008, Val Loss: 65017.4180\n",
      "Epoch [40/100], Train Loss: 63834.1484, Val Loss: 63480.5273\n",
      "Epoch [50/100], Train Loss: 62436.0352, Val Loss: 62072.2734\n",
      "Epoch [60/100], Train Loss: 61170.9492, Val Loss: 60821.6211\n",
      "Epoch [70/100], Train Loss: 59996.6953, Val Loss: 59643.3555\n",
      "Epoch [80/100], Train Loss: 58896.9102, Val Loss: 58540.0430\n",
      "Epoch [90/100], Train Loss: 57865.1719, Val Loss: 57508.6250\n",
      "Epoch [100/100], Train Loss: 56909.7578, Val Loss: 56549.2578\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68742.7266, Val Loss: 68443.4375\n",
      "Epoch [20/100], Train Loss: 67004.1562, Val Loss: 66672.0547\n",
      "Epoch [30/100], Train Loss: 65319.4102, Val Loss: 64989.8008\n",
      "Epoch [40/100], Train Loss: 63787.1406, Val Loss: 63430.9375\n",
      "Epoch [50/100], Train Loss: 62368.4219, Val Loss: 61999.5781\n",
      "Epoch [60/100], Train Loss: 61065.8633, Val Loss: 60709.7305\n",
      "Epoch [70/100], Train Loss: 59867.1133, Val Loss: 59512.6406\n",
      "Epoch [80/100], Train Loss: 58778.3281, Val Loss: 58424.2500\n",
      "Epoch [90/100], Train Loss: 57747.6562, Val Loss: 57386.3633\n",
      "Epoch [100/100], Train Loss: 56796.3672, Val Loss: 56433.4883\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69485.7109, Val Loss: 69327.5938\n",
      "Epoch [20/100], Train Loss: 69423.4688, Val Loss: 69264.3984\n",
      "Epoch [30/100], Train Loss: 69349.5078, Val Loss: 69189.0703\n",
      "Epoch [40/100], Train Loss: 69257.9766, Val Loss: 69095.5469\n",
      "Epoch [50/100], Train Loss: 69142.9922, Val Loss: 68978.2969\n",
      "Epoch [60/100], Train Loss: 68999.5234, Val Loss: 68832.7656\n",
      "Epoch [70/100], Train Loss: 68826.1328, Val Loss: 68656.5391\n",
      "Epoch [80/100], Train Loss: 68622.4062, Val Loss: 68450.1719\n",
      "Epoch [90/100], Train Loss: 68390.3281, Val Loss: 68217.5547\n",
      "Epoch [100/100], Train Loss: 68139.2734, Val Loss: 67965.3672\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69485.2734, Val Loss: 69325.2266\n",
      "Epoch [20/100], Train Loss: 69421.7109, Val Loss: 69260.0156\n",
      "Epoch [30/100], Train Loss: 69348.3125, Val Loss: 69183.1797\n",
      "Epoch [40/100], Train Loss: 69255.3203, Val Loss: 69088.0547\n",
      "Epoch [50/100], Train Loss: 69138.7344, Val Loss: 68968.3281\n",
      "Epoch [60/100], Train Loss: 68993.9609, Val Loss: 68819.0000\n",
      "Epoch [70/100], Train Loss: 68816.8984, Val Loss: 68637.2109\n",
      "Epoch [80/100], Train Loss: 68606.5547, Val Loss: 68423.2656\n",
      "Epoch [90/100], Train Loss: 68366.8047, Val Loss: 68181.3828\n",
      "Epoch [100/100], Train Loss: 68105.6797, Val Loss: 67918.6328\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69505.0703, Val Loss: 69346.2891\n",
      "Epoch [20/100], Train Loss: 69443.0938, Val Loss: 69283.7344\n",
      "Epoch [30/100], Train Loss: 69371.8125, Val Loss: 69210.1328\n",
      "Epoch [40/100], Train Loss: 69282.8047, Val Loss: 69118.4609\n",
      "Epoch [50/100], Train Loss: 69169.4062, Val Loss: 69002.1406\n",
      "Epoch [60/100], Train Loss: 69029.0781, Val Loss: 68855.8281\n",
      "Epoch [70/100], Train Loss: 68852.8281, Val Loss: 68676.3125\n",
      "Epoch [80/100], Train Loss: 68642.9297, Val Loss: 68463.5703\n",
      "Epoch [90/100], Train Loss: 68403.4375, Val Loss: 68221.5859\n",
      "Epoch [100/100], Train Loss: 68144.0234, Val Loss: 67957.8281\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 60370.4375, Val Loss: 59285.0859\n",
      "Epoch [20/100], Train Loss: 52534.4766, Val Loss: 51520.0586\n",
      "Epoch [30/100], Train Loss: 46135.4648, Val Loss: 45170.5430\n",
      "Epoch [40/100], Train Loss: 40932.8242, Val Loss: 39828.8008\n",
      "Epoch [50/100], Train Loss: 35390.5312, Val Loss: 34589.7227\n",
      "Epoch [60/100], Train Loss: 31302.6211, Val Loss: 30572.9707\n",
      "Epoch [70/100], Train Loss: 27854.6426, Val Loss: 27238.1094\n",
      "Epoch [80/100], Train Loss: 24820.0371, Val Loss: 24294.5938\n",
      "Epoch [90/100], Train Loss: 22226.7832, Val Loss: 21755.2871\n",
      "Epoch [100/100], Train Loss: 20024.1660, Val Loss: 19611.0645\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 60396.0312, Val Loss: 59318.7422\n",
      "Epoch [20/100], Train Loss: 52566.6836, Val Loss: 51536.3633\n",
      "Epoch [30/100], Train Loss: 46190.2305, Val Loss: 45223.3828\n",
      "Epoch [40/100], Train Loss: 40287.9609, Val Loss: 39233.8398\n",
      "Epoch [50/100], Train Loss: 35205.2266, Val Loss: 34401.8828\n",
      "Epoch [60/100], Train Loss: 31105.0703, Val Loss: 30386.1582\n",
      "Epoch [70/100], Train Loss: 27609.7500, Val Loss: 26998.3809\n",
      "Epoch [80/100], Train Loss: 24602.5469, Val Loss: 24048.8047\n",
      "Epoch [90/100], Train Loss: 22078.4180, Val Loss: 21600.3789\n",
      "Epoch [100/100], Train Loss: 19918.3145, Val Loss: 19516.3359\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 60288.3906, Val Loss: 59209.8945\n",
      "Epoch [20/100], Train Loss: 52476.3047, Val Loss: 51445.3672\n",
      "Epoch [30/100], Train Loss: 46110.0391, Val Loss: 45130.7422\n",
      "Epoch [40/100], Train Loss: 40463.5430, Val Loss: 39380.9414\n",
      "Epoch [50/100], Train Loss: 35207.5625, Val Loss: 34392.2070\n",
      "Epoch [60/100], Train Loss: 31125.9434, Val Loss: 30406.3711\n",
      "Epoch [70/100], Train Loss: 27679.7090, Val Loss: 27037.0742\n",
      "Epoch [80/100], Train Loss: 24668.0762, Val Loss: 24078.6816\n",
      "Epoch [90/100], Train Loss: 22137.8262, Val Loss: 21608.4941\n",
      "Epoch [100/100], Train Loss: 19972.9941, Val Loss: 19499.2148\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68046.5156, Val Loss: 67582.7109\n",
      "Epoch [20/100], Train Loss: 65712.8516, Val Loss: 65360.1094\n",
      "Epoch [30/100], Train Loss: 64302.0586, Val Loss: 63977.5898\n",
      "Epoch [40/100], Train Loss: 63180.6719, Val Loss: 62866.5156\n",
      "Epoch [50/100], Train Loss: 62153.1211, Val Loss: 61835.8750\n",
      "Epoch [60/100], Train Loss: 61169.7227, Val Loss: 60846.6094\n",
      "Epoch [70/100], Train Loss: 60227.4258, Val Loss: 59897.7188\n",
      "Epoch [80/100], Train Loss: 59322.8945, Val Loss: 58986.4414\n",
      "Epoch [90/100], Train Loss: 58451.3281, Val Loss: 58108.8945\n",
      "Epoch [100/100], Train Loss: 57608.0039, Val Loss: 57258.9297\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68083.6250, Val Loss: 67616.3203\n",
      "Epoch [20/100], Train Loss: 65735.0000, Val Loss: 65379.4531\n",
      "Epoch [30/100], Train Loss: 64312.0117, Val Loss: 63982.2148\n",
      "Epoch [40/100], Train Loss: 63152.4219, Val Loss: 62834.2383\n",
      "Epoch [50/100], Train Loss: 62115.8906, Val Loss: 61797.2188\n",
      "Epoch [60/100], Train Loss: 61128.8398, Val Loss: 60805.1367\n",
      "Epoch [70/100], Train Loss: 60183.4023, Val Loss: 59851.6211\n",
      "Epoch [80/100], Train Loss: 59276.2109, Val Loss: 58937.1484\n",
      "Epoch [90/100], Train Loss: 58402.5430, Val Loss: 58056.5742\n",
      "Epoch [100/100], Train Loss: 57557.7734, Val Loss: 57204.3672\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67942.7812, Val Loss: 67446.7578\n",
      "Epoch [20/100], Train Loss: 65601.6562, Val Loss: 65237.5508\n",
      "Epoch [30/100], Train Loss: 64222.3945, Val Loss: 63897.7578\n",
      "Epoch [40/100], Train Loss: 63099.1797, Val Loss: 62780.6016\n",
      "Epoch [50/100], Train Loss: 62068.4727, Val Loss: 61750.2773\n",
      "Epoch [60/100], Train Loss: 61091.7852, Val Loss: 60769.1328\n",
      "Epoch [70/100], Train Loss: 60154.4883, Val Loss: 59821.6875\n",
      "Epoch [80/100], Train Loss: 59252.3008, Val Loss: 58912.1250\n",
      "Epoch [90/100], Train Loss: 58381.6016, Val Loss: 58032.4844\n",
      "Epoch [100/100], Train Loss: 57533.7109, Val Loss: 57175.2422\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69476.7266, Val Loss: 69314.5859\n",
      "Epoch [20/100], Train Loss: 69382.1094, Val Loss: 69216.5625\n",
      "Epoch [30/100], Train Loss: 69251.3516, Val Loss: 69078.7578\n",
      "Epoch [40/100], Train Loss: 69052.4062, Val Loss: 68870.5000\n",
      "Epoch [50/100], Train Loss: 68756.7109, Val Loss: 68562.2812\n",
      "Epoch [60/100], Train Loss: 68352.8516, Val Loss: 68146.5703\n",
      "Epoch [70/100], Train Loss: 67872.7188, Val Loss: 67660.1875\n",
      "Epoch [80/100], Train Loss: 67379.0781, Val Loss: 67169.1562\n",
      "Epoch [90/100], Train Loss: 66928.6875, Val Loss: 66719.7500\n",
      "Epoch [100/100], Train Loss: 66531.4375, Val Loss: 66323.1094\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69520.7422, Val Loss: 69358.8516\n",
      "Epoch [20/100], Train Loss: 69427.6953, Val Loss: 69262.8047\n",
      "Epoch [30/100], Train Loss: 69300.0547, Val Loss: 69128.3281\n",
      "Epoch [40/100], Train Loss: 69106.9453, Val Loss: 68924.4297\n",
      "Epoch [50/100], Train Loss: 68815.9922, Val Loss: 68617.6641\n",
      "Epoch [60/100], Train Loss: 68408.5625, Val Loss: 68193.5391\n",
      "Epoch [70/100], Train Loss: 67911.0312, Val Loss: 67686.2969\n",
      "Epoch [80/100], Train Loss: 67391.0703, Val Loss: 67167.8906\n",
      "Epoch [90/100], Train Loss: 66914.0938, Val Loss: 66691.6797\n",
      "Epoch [100/100], Train Loss: 66495.1797, Val Loss: 66271.7344\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69478.8125, Val Loss: 69316.8594\n",
      "Epoch [20/100], Train Loss: 69388.6172, Val Loss: 69221.7734\n",
      "Epoch [30/100], Train Loss: 69260.7812, Val Loss: 69086.4688\n",
      "Epoch [40/100], Train Loss: 69069.4219, Val Loss: 68882.3203\n",
      "Epoch [50/100], Train Loss: 68787.8828, Val Loss: 68581.0234\n",
      "Epoch [60/100], Train Loss: 68396.6875, Val Loss: 68173.8125\n",
      "Epoch [70/100], Train Loss: 67928.3906, Val Loss: 67695.5469\n",
      "Epoch [80/100], Train Loss: 67448.5859, Val Loss: 67211.4766\n",
      "Epoch [90/100], Train Loss: 67001.5938, Val Loss: 66768.2188\n",
      "Epoch [100/100], Train Loss: 66611.2344, Val Loss: 66379.1250\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 60356.3945, Val Loss: 59295.2734\n",
      "Epoch [20/100], Train Loss: 52599.0859, Val Loss: 51578.9883\n",
      "Epoch [30/100], Train Loss: 46360.5664, Val Loss: 45430.5273\n",
      "Epoch [40/100], Train Loss: 41739.6602, Val Loss: 40893.7383\n",
      "Epoch [50/100], Train Loss: 38444.6562, Val Loss: 37654.8125\n",
      "Epoch [60/100], Train Loss: 35791.2773, Val Loss: 34610.5039\n",
      "Epoch [70/100], Train Loss: 29697.3984, Val Loss: 29048.2754\n",
      "Epoch [80/100], Train Loss: 26616.7207, Val Loss: 26030.3281\n",
      "Epoch [90/100], Train Loss: 23762.5586, Val Loss: 23233.9785\n",
      "Epoch [100/100], Train Loss: 21193.4883, Val Loss: 20680.1895\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 60524.8047, Val Loss: 59454.8047\n",
      "Epoch [20/100], Train Loss: 52753.6367, Val Loss: 51734.6562\n",
      "Epoch [30/100], Train Loss: 46499.4453, Val Loss: 45562.0234\n",
      "Epoch [40/100], Train Loss: 41817.1289, Val Loss: 40974.9688\n",
      "Epoch [50/100], Train Loss: 38370.3164, Val Loss: 37461.6602\n",
      "Epoch [60/100], Train Loss: 32542.8125, Val Loss: 31792.9531\n",
      "Epoch [70/100], Train Loss: 29026.4023, Val Loss: 28347.9199\n",
      "Epoch [80/100], Train Loss: 26033.9473, Val Loss: 25417.1387\n",
      "Epoch [90/100], Train Loss: 23295.6660, Val Loss: 22754.3223\n",
      "Epoch [100/100], Train Loss: 20811.2246, Val Loss: 20329.4824\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 60363.3086, Val Loss: 59291.1914\n",
      "Epoch [20/100], Train Loss: 52610.3867, Val Loss: 51592.1055\n",
      "Epoch [30/100], Train Loss: 46378.9609, Val Loss: 45446.9531\n",
      "Epoch [40/100], Train Loss: 41739.5391, Val Loss: 40862.5469\n",
      "Epoch [50/100], Train Loss: 38259.0781, Val Loss: 37444.6562\n",
      "Epoch [60/100], Train Loss: 35628.9453, Val Loss: 34871.9297\n",
      "Epoch [70/100], Train Loss: 29903.2090, Val Loss: 29249.6680\n",
      "Epoch [80/100], Train Loss: 26855.4258, Val Loss: 26265.8125\n",
      "Epoch [90/100], Train Loss: 24078.5449, Val Loss: 23463.9277\n",
      "Epoch [100/100], Train Loss: 21411.7871, Val Loss: 20840.5840\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 67165.3750, Val Loss: 66613.7344\n",
      "Epoch [20/100], Train Loss: 65168.3320, Val Loss: 64861.0039\n",
      "Epoch [30/100], Train Loss: 64111.0820, Val Loss: 63808.6211\n",
      "Epoch [40/100], Train Loss: 63103.4766, Val Loss: 62796.1445\n",
      "Epoch [50/100], Train Loss: 62134.3945, Val Loss: 61821.8008\n",
      "Epoch [60/100], Train Loss: 61203.7891, Val Loss: 60886.1445\n",
      "Epoch [70/100], Train Loss: 60307.5000, Val Loss: 59984.8516\n",
      "Epoch [80/100], Train Loss: 59441.8281, Val Loss: 59115.0000\n",
      "Epoch [90/100], Train Loss: 58602.1172, Val Loss: 58270.5781\n",
      "Epoch [100/100], Train Loss: 57780.9648, Val Loss: 57436.4727\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 67394.1328, Val Loss: 66839.0859\n",
      "Epoch [20/100], Train Loss: 65351.7266, Val Loss: 65041.7383\n",
      "Epoch [30/100], Train Loss: 64269.6914, Val Loss: 63966.9062\n",
      "Epoch [40/100], Train Loss: 63252.9648, Val Loss: 62945.3906\n",
      "Epoch [50/100], Train Loss: 62276.1992, Val Loss: 61964.0078\n",
      "Epoch [60/100], Train Loss: 61339.1211, Val Loss: 61022.0781\n",
      "Epoch [70/100], Train Loss: 60438.0039, Val Loss: 60115.8242\n",
      "Epoch [80/100], Train Loss: 59568.6484, Val Loss: 59241.2344\n",
      "Epoch [90/100], Train Loss: 58726.6914, Val Loss: 58394.5508\n",
      "Epoch [100/100], Train Loss: 57909.8828, Val Loss: 57572.0664\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67191.4453, Val Loss: 66593.3438\n",
      "Epoch [20/100], Train Loss: 65123.0664, Val Loss: 64798.7266\n",
      "Epoch [30/100], Train Loss: 64026.2266, Val Loss: 63721.9531\n",
      "Epoch [40/100], Train Loss: 63014.2266, Val Loss: 62704.9336\n",
      "Epoch [50/100], Train Loss: 62041.6406, Val Loss: 61727.3711\n",
      "Epoch [60/100], Train Loss: 61107.9844, Val Loss: 60789.2305\n",
      "Epoch [70/100], Train Loss: 60210.0625, Val Loss: 59886.1992\n",
      "Epoch [80/100], Train Loss: 59343.0117, Val Loss: 59014.2539\n",
      "Epoch [90/100], Train Loss: 58503.0781, Val Loss: 58168.1875\n",
      "Epoch [100/100], Train Loss: 57686.6797, Val Loss: 57341.6133\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69493.6250, Val Loss: 69331.1094\n",
      "Epoch [20/100], Train Loss: 69382.2578, Val Loss: 69212.9922\n",
      "Epoch [30/100], Train Loss: 69196.2891, Val Loss: 69013.7734\n",
      "Epoch [40/100], Train Loss: 68864.8594, Val Loss: 68657.7812\n",
      "Epoch [50/100], Train Loss: 68324.2734, Val Loss: 68091.7422\n",
      "Epoch [60/100], Train Loss: 67647.7031, Val Loss: 67406.5156\n",
      "Epoch [70/100], Train Loss: 67020.5703, Val Loss: 66783.3672\n",
      "Epoch [80/100], Train Loss: 66527.0234, Val Loss: 66299.6016\n",
      "Epoch [90/100], Train Loss: 66171.8594, Val Loss: 65950.2422\n",
      "Epoch [100/100], Train Loss: 65907.4922, Val Loss: 65692.1875\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69494.3828, Val Loss: 69331.8281\n",
      "Epoch [20/100], Train Loss: 69385.1484, Val Loss: 69216.3438\n",
      "Epoch [30/100], Train Loss: 69204.6641, Val Loss: 69021.4922\n",
      "Epoch [40/100], Train Loss: 68877.2812, Val Loss: 68668.6406\n",
      "Epoch [50/100], Train Loss: 68345.0938, Val Loss: 68104.9531\n",
      "Epoch [60/100], Train Loss: 67677.5781, Val Loss: 67424.8359\n",
      "Epoch [70/100], Train Loss: 67052.7812, Val Loss: 66800.9062\n",
      "Epoch [80/100], Train Loss: 66549.6797, Val Loss: 66304.8047\n",
      "Epoch [90/100], Train Loss: 66172.3203, Val Loss: 65933.9062\n",
      "Epoch [100/100], Train Loss: 65886.2109, Val Loss: 65655.7969\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69464.1719, Val Loss: 69299.4062\n",
      "Epoch [20/100], Train Loss: 69344.4688, Val Loss: 69170.5234\n",
      "Epoch [30/100], Train Loss: 69143.4453, Val Loss: 68952.1875\n",
      "Epoch [40/100], Train Loss: 68790.8047, Val Loss: 68565.3203\n",
      "Epoch [50/100], Train Loss: 68233.4688, Val Loss: 67969.6016\n",
      "Epoch [60/100], Train Loss: 67564.1797, Val Loss: 67282.3125\n",
      "Epoch [70/100], Train Loss: 66950.8906, Val Loss: 66673.2422\n",
      "Epoch [80/100], Train Loss: 66468.2656, Val Loss: 66199.5625\n",
      "Epoch [90/100], Train Loss: 66102.1328, Val Loss: 65850.4531\n",
      "Epoch [100/100], Train Loss: 65829.5156, Val Loss: 65586.0469\n",
      "Best Parameters for RNN: {'hidden_dim': 256, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Tuning LSTM Model\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69222.7188, Val Loss: 69009.9141\n",
      "Epoch [20/100], Train Loss: 68456.7109, Val Loss: 68212.7500\n",
      "Epoch [30/100], Train Loss: 67436.8047, Val Loss: 67181.8750\n",
      "Epoch [40/100], Train Loss: 66291.2188, Val Loss: 66028.7422\n",
      "Epoch [50/100], Train Loss: 65079.4492, Val Loss: 64807.9609\n",
      "Epoch [60/100], Train Loss: 63844.1719, Val Loss: 63558.4570\n",
      "Epoch [70/100], Train Loss: 62612.8711, Val Loss: 62311.3281\n",
      "Epoch [80/100], Train Loss: 61417.4375, Val Loss: 61101.2969\n",
      "Epoch [90/100], Train Loss: 60278.9375, Val Loss: 59951.7578\n",
      "Epoch [100/100], Train Loss: 59200.7188, Val Loss: 58866.2422\n",
      "New best score: 58866.2421875 with params: {'hidden_dim': 32, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69130.9766, Val Loss: 68913.6406\n",
      "Epoch [20/100], Train Loss: 68348.4766, Val Loss: 68100.3594\n",
      "Epoch [30/100], Train Loss: 67327.2109, Val Loss: 67067.5391\n",
      "Epoch [40/100], Train Loss: 66196.0234, Val Loss: 65929.8047\n",
      "Epoch [50/100], Train Loss: 65010.4023, Val Loss: 64735.6016\n",
      "Epoch [60/100], Train Loss: 63800.1211, Val Loss: 63512.1797\n",
      "Epoch [70/100], Train Loss: 62580.4336, Val Loss: 62277.2656\n",
      "Epoch [80/100], Train Loss: 61375.9766, Val Loss: 61057.3750\n",
      "Epoch [90/100], Train Loss: 60219.5938, Val Loss: 59889.0469\n",
      "Epoch [100/100], Train Loss: 59132.3594, Val Loss: 58795.2812\n",
      "New best score: 58795.28125 with params: {'hidden_dim': 32, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69087.6875, Val Loss: 68873.5312\n",
      "Epoch [20/100], Train Loss: 68298.7266, Val Loss: 68053.9062\n",
      "Epoch [30/100], Train Loss: 67268.6406, Val Loss: 67009.1328\n",
      "Epoch [40/100], Train Loss: 66127.3438, Val Loss: 65859.7891\n",
      "Epoch [50/100], Train Loss: 64936.0508, Val Loss: 64661.6094\n",
      "Epoch [60/100], Train Loss: 63727.4258, Val Loss: 63442.4727\n",
      "Epoch [70/100], Train Loss: 62510.6797, Val Loss: 62210.5195\n",
      "Epoch [80/100], Train Loss: 61310.6289, Val Loss: 60993.8711\n",
      "Epoch [90/100], Train Loss: 60161.8398, Val Loss: 59833.0312\n",
      "Epoch [100/100], Train Loss: 59080.6953, Val Loss: 58744.5781\n",
      "New best score: 58744.578125 with params: {'hidden_dim': 32, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.3}\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69575.5391, Val Loss: 69419.9141\n",
      "Epoch [20/100], Train Loss: 69538.8359, Val Loss: 69382.6406\n",
      "Epoch [30/100], Train Loss: 69495.0547, Val Loss: 69337.9844\n",
      "Epoch [40/100], Train Loss: 69441.0391, Val Loss: 69282.8984\n",
      "Epoch [50/100], Train Loss: 69374.8672, Val Loss: 69215.5938\n",
      "Epoch [60/100], Train Loss: 69295.8125, Val Loss: 69135.4141\n",
      "Epoch [70/100], Train Loss: 69204.1094, Val Loss: 69042.6719\n",
      "Epoch [80/100], Train Loss: 69100.8359, Val Loss: 68938.4609\n",
      "Epoch [90/100], Train Loss: 68987.6172, Val Loss: 68824.4531\n",
      "Epoch [100/100], Train Loss: 68866.3281, Val Loss: 68702.6016\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69490.4453, Val Loss: 69334.8438\n",
      "Epoch [20/100], Train Loss: 69456.1094, Val Loss: 69300.1094\n",
      "Epoch [30/100], Train Loss: 69415.9609, Val Loss: 69259.1641\n",
      "Epoch [40/100], Train Loss: 69366.4844, Val Loss: 69208.6719\n",
      "Epoch [50/100], Train Loss: 69305.5234, Val Loss: 69146.5312\n",
      "Epoch [60/100], Train Loss: 69232.0156, Val Loss: 69071.8359\n",
      "Epoch [70/100], Train Loss: 69146.0469, Val Loss: 68984.7422\n",
      "Epoch [80/100], Train Loss: 69048.3672, Val Loss: 68886.0625\n",
      "Epoch [90/100], Train Loss: 68940.1094, Val Loss: 68776.9609\n",
      "Epoch [100/100], Train Loss: 68822.6406, Val Loss: 68658.8359\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69535.3203, Val Loss: 69379.6094\n",
      "Epoch [20/100], Train Loss: 69493.8750, Val Loss: 69337.2422\n",
      "Epoch [30/100], Train Loss: 69444.0938, Val Loss: 69286.2109\n",
      "Epoch [40/100], Train Loss: 69383.5547, Val Loss: 69224.2266\n",
      "Epoch [50/100], Train Loss: 69310.8672, Val Loss: 69150.0781\n",
      "Epoch [60/100], Train Loss: 69225.8750, Val Loss: 69063.7578\n",
      "Epoch [70/100], Train Loss: 69129.3594, Val Loss: 68966.2031\n",
      "Epoch [80/100], Train Loss: 69022.7109, Val Loss: 68858.8359\n",
      "Epoch [90/100], Train Loss: 68907.5625, Val Loss: 68743.3672\n",
      "Epoch [100/100], Train Loss: 68785.6094, Val Loss: 68621.4688\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69539.5312, Val Loss: 69386.2969\n",
      "Epoch [20/100], Train Loss: 69538.4062, Val Loss: 69385.2891\n",
      "Epoch [30/100], Train Loss: 69538.0859, Val Loss: 69384.9844\n",
      "Epoch [40/100], Train Loss: 69537.9688, Val Loss: 69384.8828\n",
      "Epoch [50/100], Train Loss: 69537.9375, Val Loss: 69384.8516\n",
      "Epoch [60/100], Train Loss: 69537.9297, Val Loss: 69384.8516\n",
      "Epoch [70/100], Train Loss: 69537.9141, Val Loss: 69384.8438\n",
      "Epoch [80/100], Train Loss: 69537.9141, Val Loss: 69384.8438\n",
      "Epoch [90/100], Train Loss: 69537.9141, Val Loss: 69384.8438\n",
      "Epoch [100/100], Train Loss: 69537.9141, Val Loss: 69384.8438\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69513.1484, Val Loss: 69360.7812\n",
      "Epoch [20/100], Train Loss: 69511.8828, Val Loss: 69359.6484\n",
      "Epoch [30/100], Train Loss: 69511.5234, Val Loss: 69359.3125\n",
      "Epoch [40/100], Train Loss: 69511.3906, Val Loss: 69359.2031\n",
      "Epoch [50/100], Train Loss: 69511.3594, Val Loss: 69359.1562\n",
      "Epoch [60/100], Train Loss: 69511.3438, Val Loss: 69359.1562\n",
      "Epoch [70/100], Train Loss: 69511.3438, Val Loss: 69359.1562\n",
      "Epoch [80/100], Train Loss: 69511.3438, Val Loss: 69359.1562\n",
      "Epoch [90/100], Train Loss: 69511.3438, Val Loss: 69359.1562\n",
      "Epoch [100/100], Train Loss: 69511.3438, Val Loss: 69359.1562\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69528.8750, Val Loss: 69376.7812\n",
      "Epoch [20/100], Train Loss: 69527.8203, Val Loss: 69375.8359\n",
      "Epoch [30/100], Train Loss: 69527.5156, Val Loss: 69375.5703\n",
      "Epoch [40/100], Train Loss: 69527.4219, Val Loss: 69375.4766\n",
      "Epoch [50/100], Train Loss: 69527.3828, Val Loss: 69375.4453\n",
      "Epoch [60/100], Train Loss: 69527.3750, Val Loss: 69375.4375\n",
      "Epoch [70/100], Train Loss: 69527.3750, Val Loss: 69375.4375\n",
      "Epoch [80/100], Train Loss: 69527.3750, Val Loss: 69375.4375\n",
      "Epoch [90/100], Train Loss: 69527.3750, Val Loss: 69375.4375\n",
      "Epoch [100/100], Train Loss: 69527.3750, Val Loss: 69375.4375\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69164.8359, Val Loss: 68921.1484\n",
      "Epoch [20/100], Train Loss: 67901.4844, Val Loss: 67584.4688\n",
      "Epoch [30/100], Train Loss: 66455.0078, Val Loss: 66140.3672\n",
      "Epoch [40/100], Train Loss: 65204.0586, Val Loss: 64894.9648\n",
      "Epoch [50/100], Train Loss: 64081.4375, Val Loss: 63772.0352\n",
      "Epoch [60/100], Train Loss: 63015.8438, Val Loss: 62698.0469\n",
      "Epoch [70/100], Train Loss: 61965.4180, Val Loss: 61643.7031\n",
      "Epoch [80/100], Train Loss: 60970.6641, Val Loss: 60645.1797\n",
      "Epoch [90/100], Train Loss: 60019.0352, Val Loss: 59689.3711\n",
      "Epoch [100/100], Train Loss: 59102.5430, Val Loss: 58768.8906\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69121.8359, Val Loss: 68880.6016\n",
      "Epoch [20/100], Train Loss: 67953.7500, Val Loss: 67649.3828\n",
      "Epoch [30/100], Train Loss: 66556.4453, Val Loss: 66237.3047\n",
      "Epoch [40/100], Train Loss: 65272.7773, Val Loss: 64958.0273\n",
      "Epoch [50/100], Train Loss: 64115.3477, Val Loss: 63800.7383\n",
      "Epoch [60/100], Train Loss: 63035.6328, Val Loss: 62719.0273\n",
      "Epoch [70/100], Train Loss: 62014.4336, Val Loss: 61693.8828\n",
      "Epoch [80/100], Train Loss: 61039.1523, Val Loss: 60714.7773\n",
      "Epoch [90/100], Train Loss: 60103.1406, Val Loss: 59774.2617\n",
      "Epoch [100/100], Train Loss: 59199.0508, Val Loss: 58865.9961\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69157.2656, Val Loss: 68910.3047\n",
      "Epoch [20/100], Train Loss: 67948.7422, Val Loss: 67635.5547\n",
      "Epoch [30/100], Train Loss: 66558.7656, Val Loss: 66241.4219\n",
      "Epoch [40/100], Train Loss: 65317.2812, Val Loss: 65003.5742\n",
      "Epoch [50/100], Train Loss: 64163.5859, Val Loss: 63844.1250\n",
      "Epoch [60/100], Train Loss: 63058.7578, Val Loss: 62738.7109\n",
      "Epoch [70/100], Train Loss: 62022.9453, Val Loss: 61699.4883\n",
      "Epoch [80/100], Train Loss: 61036.1641, Val Loss: 60709.5586\n",
      "Epoch [90/100], Train Loss: 60091.4766, Val Loss: 59760.3125\n",
      "Epoch [100/100], Train Loss: 59180.4141, Val Loss: 58845.6289\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69596.6484, Val Loss: 69442.5078\n",
      "Epoch [20/100], Train Loss: 69572.1641, Val Loss: 69416.9922\n",
      "Epoch [30/100], Train Loss: 69536.6562, Val Loss: 69379.4922\n",
      "Epoch [40/100], Train Loss: 69480.7891, Val Loss: 69320.7109\n",
      "Epoch [50/100], Train Loss: 69393.9688, Val Loss: 69229.0234\n",
      "Epoch [60/100], Train Loss: 69264.7188, Val Loss: 69094.5781\n",
      "Epoch [70/100], Train Loss: 69091.9922, Val Loss: 68915.8281\n",
      "Epoch [80/100], Train Loss: 68880.1641, Val Loss: 68701.3672\n",
      "Epoch [90/100], Train Loss: 68646.5078, Val Loss: 68464.9922\n",
      "Epoch [100/100], Train Loss: 68402.3594, Val Loss: 68220.0234\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69548.6562, Val Loss: 69394.3438\n",
      "Epoch [20/100], Train Loss: 69525.0000, Val Loss: 69369.8359\n",
      "Epoch [30/100], Train Loss: 69491.7109, Val Loss: 69334.6172\n",
      "Epoch [40/100], Train Loss: 69439.8047, Val Loss: 69279.6328\n",
      "Epoch [50/100], Train Loss: 69358.8828, Val Loss: 69194.0547\n",
      "Epoch [60/100], Train Loss: 69239.2500, Val Loss: 69069.6484\n",
      "Epoch [70/100], Train Loss: 69081.7656, Val Loss: 68906.5391\n",
      "Epoch [80/100], Train Loss: 68890.3594, Val Loss: 68713.1328\n",
      "Epoch [90/100], Train Loss: 68682.1641, Val Loss: 68500.2656\n",
      "Epoch [100/100], Train Loss: 68460.8906, Val Loss: 68277.2031\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69458.9297, Val Loss: 69303.3438\n",
      "Epoch [20/100], Train Loss: 69433.2266, Val Loss: 69276.5938\n",
      "Epoch [30/100], Train Loss: 69396.5469, Val Loss: 69237.7500\n",
      "Epoch [40/100], Train Loss: 69339.9453, Val Loss: 69177.3516\n",
      "Epoch [50/100], Train Loss: 69250.7891, Val Loss: 69083.5781\n",
      "Epoch [60/100], Train Loss: 69119.9219, Val Loss: 68946.3203\n",
      "Epoch [70/100], Train Loss: 68944.1484, Val Loss: 68763.7266\n",
      "Epoch [80/100], Train Loss: 68731.3750, Val Loss: 68545.3438\n",
      "Epoch [90/100], Train Loss: 68497.4141, Val Loss: 68307.1875\n",
      "Epoch [100/100], Train Loss: 68256.6328, Val Loss: 68064.0703\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69606.8984, Val Loss: 69454.7812\n",
      "Epoch [20/100], Train Loss: 69606.2422, Val Loss: 69454.1797\n",
      "Epoch [30/100], Train Loss: 69606.1016, Val Loss: 69453.9922\n",
      "Epoch [40/100], Train Loss: 69606.0156, Val Loss: 69453.9453\n",
      "Epoch [50/100], Train Loss: 69605.9688, Val Loss: 69453.9219\n",
      "Epoch [60/100], Train Loss: 69605.9766, Val Loss: 69453.9141\n",
      "Epoch [70/100], Train Loss: 69605.9609, Val Loss: 69453.9141\n",
      "Epoch [80/100], Train Loss: 69605.9688, Val Loss: 69453.9141\n",
      "Epoch [90/100], Train Loss: 69605.9688, Val Loss: 69453.9141\n",
      "Epoch [100/100], Train Loss: 69605.9922, Val Loss: 69453.9141\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69572.0078, Val Loss: 69419.7344\n",
      "Epoch [20/100], Train Loss: 69571.3750, Val Loss: 69419.1953\n",
      "Epoch [30/100], Train Loss: 69571.1797, Val Loss: 69419.0391\n",
      "Epoch [40/100], Train Loss: 69571.1641, Val Loss: 69418.9922\n",
      "Epoch [50/100], Train Loss: 69571.1562, Val Loss: 69418.9766\n",
      "Epoch [60/100], Train Loss: 69571.0938, Val Loss: 69418.9688\n",
      "Epoch [70/100], Train Loss: 69571.1328, Val Loss: 69418.9688\n",
      "Epoch [80/100], Train Loss: 69571.1406, Val Loss: 69418.9688\n",
      "Epoch [90/100], Train Loss: 69571.1328, Val Loss: 69418.9688\n",
      "Epoch [100/100], Train Loss: 69571.1328, Val Loss: 69418.9688\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69513.1641, Val Loss: 69360.4531\n",
      "Epoch [20/100], Train Loss: 69512.3359, Val Loss: 69359.7734\n",
      "Epoch [30/100], Train Loss: 69512.1016, Val Loss: 69359.5625\n",
      "Epoch [40/100], Train Loss: 69511.9844, Val Loss: 69359.5000\n",
      "Epoch [50/100], Train Loss: 69512.0078, Val Loss: 69359.4766\n",
      "Epoch [60/100], Train Loss: 69511.9609, Val Loss: 69359.4688\n",
      "Epoch [70/100], Train Loss: 69512.0234, Val Loss: 69359.4688\n",
      "Epoch [80/100], Train Loss: 69511.9766, Val Loss: 69359.4688\n",
      "Epoch [90/100], Train Loss: 69512.0234, Val Loss: 69359.4688\n",
      "Epoch [100/100], Train Loss: 69512.0547, Val Loss: 69359.4688\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69135.0703, Val Loss: 68868.8672\n",
      "Epoch [20/100], Train Loss: 67752.3906, Val Loss: 67446.8984\n",
      "Epoch [30/100], Train Loss: 66510.7969, Val Loss: 66212.4453\n",
      "Epoch [40/100], Train Loss: 65361.4023, Val Loss: 65062.3047\n",
      "Epoch [50/100], Train Loss: 64294.5664, Val Loss: 63992.5156\n",
      "Epoch [60/100], Train Loss: 63277.6602, Val Loss: 62971.0938\n",
      "Epoch [70/100], Train Loss: 62307.9141, Val Loss: 61997.3750\n",
      "Epoch [80/100], Train Loss: 61377.0000, Val Loss: 61062.0742\n",
      "Epoch [90/100], Train Loss: 60480.4609, Val Loss: 60161.0586\n",
      "Epoch [100/100], Train Loss: 59612.8984, Val Loss: 59288.8438\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69241.6562, Val Loss: 68997.0625\n",
      "Epoch [20/100], Train Loss: 67985.1328, Val Loss: 67664.0391\n",
      "Epoch [30/100], Train Loss: 66560.2109, Val Loss: 66248.3516\n",
      "Epoch [40/100], Train Loss: 65347.2578, Val Loss: 65041.7188\n",
      "Epoch [50/100], Train Loss: 64228.5234, Val Loss: 63920.3516\n",
      "Epoch [60/100], Train Loss: 63165.7266, Val Loss: 62853.5117\n",
      "Epoch [70/100], Train Loss: 62154.4883, Val Loss: 61838.5508\n",
      "Epoch [80/100], Train Loss: 61188.0391, Val Loss: 60867.5547\n",
      "Epoch [90/100], Train Loss: 60258.6406, Val Loss: 59933.5508\n",
      "Epoch [100/100], Train Loss: 59361.6562, Val Loss: 59031.4805\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69246.2188, Val Loss: 68983.3594\n",
      "Epoch [20/100], Train Loss: 67832.9766, Val Loss: 67499.3828\n",
      "Epoch [30/100], Train Loss: 66462.0469, Val Loss: 66155.5156\n",
      "Epoch [40/100], Train Loss: 65304.2148, Val Loss: 65001.0625\n",
      "Epoch [50/100], Train Loss: 64215.5859, Val Loss: 63909.4492\n",
      "Epoch [60/100], Train Loss: 63178.2422, Val Loss: 62867.9414\n",
      "Epoch [70/100], Train Loss: 62187.8086, Val Loss: 61873.3281\n",
      "Epoch [80/100], Train Loss: 61236.7227, Val Loss: 60917.5117\n",
      "Epoch [90/100], Train Loss: 60320.0352, Val Loss: 59996.3750\n",
      "Epoch [100/100], Train Loss: 59433.3750, Val Loss: 59104.0586\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69559.7812, Val Loss: 69405.6328\n",
      "Epoch [20/100], Train Loss: 69539.4609, Val Loss: 69384.6797\n",
      "Epoch [30/100], Train Loss: 69511.9844, Val Loss: 69355.7891\n",
      "Epoch [40/100], Train Loss: 69467.2031, Val Loss: 69308.3203\n",
      "Epoch [50/100], Train Loss: 69386.4922, Val Loss: 69221.8672\n",
      "Epoch [60/100], Train Loss: 69244.5156, Val Loss: 69072.0078\n",
      "Epoch [70/100], Train Loss: 69033.9531, Val Loss: 68853.2734\n",
      "Epoch [80/100], Train Loss: 68772.7109, Val Loss: 68585.7188\n",
      "Epoch [90/100], Train Loss: 68488.2656, Val Loss: 68297.1875\n",
      "Epoch [100/100], Train Loss: 68205.3906, Val Loss: 68011.9766\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69597.2891, Val Loss: 69443.5938\n",
      "Epoch [20/100], Train Loss: 69579.1719, Val Loss: 69424.9844\n",
      "Epoch [30/100], Train Loss: 69554.8359, Val Loss: 69399.4453\n",
      "Epoch [40/100], Train Loss: 69514.8828, Val Loss: 69356.7188\n",
      "Epoch [50/100], Train Loss: 69440.6797, Val Loss: 69276.0078\n",
      "Epoch [60/100], Train Loss: 69304.2578, Val Loss: 69129.3516\n",
      "Epoch [70/100], Train Loss: 69091.7812, Val Loss: 68907.4688\n",
      "Epoch [80/100], Train Loss: 68825.1172, Val Loss: 68634.0156\n",
      "Epoch [90/100], Train Loss: 68537.0625, Val Loss: 68341.1328\n",
      "Epoch [100/100], Train Loss: 68249.4531, Val Loss: 68051.8750\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69467.0781, Val Loss: 69312.2109\n",
      "Epoch [20/100], Train Loss: 69447.1328, Val Loss: 69291.7188\n",
      "Epoch [30/100], Train Loss: 69420.3516, Val Loss: 69263.4141\n",
      "Epoch [40/100], Train Loss: 69376.4062, Val Loss: 69216.6016\n",
      "Epoch [50/100], Train Loss: 69296.4141, Val Loss: 69129.8203\n",
      "Epoch [60/100], Train Loss: 69151.6406, Val Loss: 68975.0156\n",
      "Epoch [70/100], Train Loss: 68933.7969, Val Loss: 68746.0078\n",
      "Epoch [80/100], Train Loss: 68665.7891, Val Loss: 68468.7891\n",
      "Epoch [90/100], Train Loss: 68378.5078, Val Loss: 68174.5078\n",
      "Epoch [100/100], Train Loss: 68093.5703, Val Loss: 67885.1797\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69527.0547, Val Loss: 69374.4375\n",
      "Epoch [20/100], Train Loss: 69526.4531, Val Loss: 69373.9062\n",
      "Epoch [30/100], Train Loss: 69526.2812, Val Loss: 69373.7422\n",
      "Epoch [40/100], Train Loss: 69526.2500, Val Loss: 69373.7031\n",
      "Epoch [50/100], Train Loss: 69526.2188, Val Loss: 69373.6797\n",
      "Epoch [60/100], Train Loss: 69526.1953, Val Loss: 69373.6719\n",
      "Epoch [70/100], Train Loss: 69526.2031, Val Loss: 69373.6719\n",
      "Epoch [80/100], Train Loss: 69526.2031, Val Loss: 69373.6719\n",
      "Epoch [90/100], Train Loss: 69526.2031, Val Loss: 69373.6719\n",
      "Epoch [100/100], Train Loss: 69526.1953, Val Loss: 69373.6719\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69613.1172, Val Loss: 69461.2188\n",
      "Epoch [20/100], Train Loss: 69612.5000, Val Loss: 69460.6484\n",
      "Epoch [30/100], Train Loss: 69612.3047, Val Loss: 69460.4844\n",
      "Epoch [40/100], Train Loss: 69612.2656, Val Loss: 69460.4219\n",
      "Epoch [50/100], Train Loss: 69612.2344, Val Loss: 69460.4141\n",
      "Epoch [60/100], Train Loss: 69612.2188, Val Loss: 69460.4062\n",
      "Epoch [70/100], Train Loss: 69612.2188, Val Loss: 69460.4062\n",
      "Epoch [80/100], Train Loss: 69612.2188, Val Loss: 69460.4062\n",
      "Epoch [90/100], Train Loss: 69612.2266, Val Loss: 69460.4062\n",
      "Epoch [100/100], Train Loss: 69612.2188, Val Loss: 69460.4062\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69580.8516, Val Loss: 69428.6641\n",
      "Epoch [20/100], Train Loss: 69580.2812, Val Loss: 69428.1328\n",
      "Epoch [30/100], Train Loss: 69580.0703, Val Loss: 69427.9844\n",
      "Epoch [40/100], Train Loss: 69580.0234, Val Loss: 69427.9297\n",
      "Epoch [50/100], Train Loss: 69580.0000, Val Loss: 69427.9141\n",
      "Epoch [60/100], Train Loss: 69580.0312, Val Loss: 69427.9062\n",
      "Epoch [70/100], Train Loss: 69580.0078, Val Loss: 69427.9062\n",
      "Epoch [80/100], Train Loss: 69579.9844, Val Loss: 69427.9062\n",
      "Epoch [90/100], Train Loss: 69579.9922, Val Loss: 69427.9062\n",
      "Epoch [100/100], Train Loss: 69580.0234, Val Loss: 69427.9062\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69098.7344, Val Loss: 68800.0781\n",
      "Epoch [20/100], Train Loss: 67478.2656, Val Loss: 67177.0234\n",
      "Epoch [30/100], Train Loss: 66323.9844, Val Loss: 66032.6016\n",
      "Epoch [40/100], Train Loss: 65263.6250, Val Loss: 64971.0234\n",
      "Epoch [50/100], Train Loss: 64262.1523, Val Loss: 63965.3438\n",
      "Epoch [60/100], Train Loss: 63270.2188, Val Loss: 62963.0273\n",
      "Epoch [70/100], Train Loss: 62302.2148, Val Loss: 61990.5195\n",
      "Epoch [80/100], Train Loss: 61371.1094, Val Loss: 61053.0156\n",
      "Epoch [90/100], Train Loss: 60469.4570, Val Loss: 60149.9141\n",
      "Epoch [100/100], Train Loss: 59600.7344, Val Loss: 59276.8203\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69209.6250, Val Loss: 68932.1953\n",
      "Epoch [20/100], Train Loss: 67694.5547, Val Loss: 67389.1641\n",
      "Epoch [30/100], Train Loss: 66521.2891, Val Loss: 66230.2031\n",
      "Epoch [40/100], Train Loss: 65455.1641, Val Loss: 65162.9258\n",
      "Epoch [50/100], Train Loss: 64447.3398, Val Loss: 64151.5195\n",
      "Epoch [60/100], Train Loss: 63487.2695, Val Loss: 63187.3242\n",
      "Epoch [70/100], Train Loss: 62566.9336, Val Loss: 62261.4375\n",
      "Epoch [80/100], Train Loss: 61677.8867, Val Loss: 61366.6523\n",
      "Epoch [90/100], Train Loss: 60809.6602, Val Loss: 60495.7656\n",
      "Epoch [100/100], Train Loss: 59973.6172, Val Loss: 59654.7266\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69084.6875, Val Loss: 68790.5547\n",
      "Epoch [20/100], Train Loss: 67617.9766, Val Loss: 67309.7109\n",
      "Epoch [30/100], Train Loss: 66400.5312, Val Loss: 66095.9688\n",
      "Epoch [40/100], Train Loss: 65238.9531, Val Loss: 64935.0977\n",
      "Epoch [50/100], Train Loss: 64155.1992, Val Loss: 63848.3672\n",
      "Epoch [60/100], Train Loss: 63124.3633, Val Loss: 62812.6406\n",
      "Epoch [70/100], Train Loss: 62137.2578, Val Loss: 61820.4219\n",
      "Epoch [80/100], Train Loss: 61185.4102, Val Loss: 60865.1562\n",
      "Epoch [90/100], Train Loss: 60270.1562, Val Loss: 59945.7812\n",
      "Epoch [100/100], Train Loss: 59384.7109, Val Loss: 59054.0547\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69565.9297, Val Loss: 69411.6719\n",
      "Epoch [20/100], Train Loss: 69544.4219, Val Loss: 69389.6484\n",
      "Epoch [30/100], Train Loss: 69517.4688, Val Loss: 69361.5234\n",
      "Epoch [40/100], Train Loss: 69475.9766, Val Loss: 69317.3125\n",
      "Epoch [50/100], Train Loss: 69396.2031, Val Loss: 69230.6250\n",
      "Epoch [60/100], Train Loss: 69229.6797, Val Loss: 69050.3047\n",
      "Epoch [70/100], Train Loss: 68947.5859, Val Loss: 68754.5703\n",
      "Epoch [80/100], Train Loss: 68604.4531, Val Loss: 68406.4062\n",
      "Epoch [90/100], Train Loss: 68268.0234, Val Loss: 68069.7891\n",
      "Epoch [100/100], Train Loss: 67968.6641, Val Loss: 67770.0703\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69551.8672, Val Loss: 69397.6094\n",
      "Epoch [20/100], Train Loss: 69530.6641, Val Loss: 69375.7344\n",
      "Epoch [30/100], Train Loss: 69501.8047, Val Loss: 69345.2969\n",
      "Epoch [40/100], Train Loss: 69453.4844, Val Loss: 69293.5312\n",
      "Epoch [50/100], Train Loss: 69357.2812, Val Loss: 69188.5938\n",
      "Epoch [60/100], Train Loss: 69164.8594, Val Loss: 68980.3516\n",
      "Epoch [70/100], Train Loss: 68863.2422, Val Loss: 68664.9062\n",
      "Epoch [80/100], Train Loss: 68523.1641, Val Loss: 68317.8125\n",
      "Epoch [90/100], Train Loss: 68206.9609, Val Loss: 68003.2656\n",
      "Epoch [100/100], Train Loss: 67939.5078, Val Loss: 67737.4766\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69589.1875, Val Loss: 69435.3594\n",
      "Epoch [20/100], Train Loss: 69570.6094, Val Loss: 69416.3984\n",
      "Epoch [30/100], Train Loss: 69547.6719, Val Loss: 69392.4453\n",
      "Epoch [40/100], Train Loss: 69513.0000, Val Loss: 69355.2969\n",
      "Epoch [50/100], Train Loss: 69448.2656, Val Loss: 69283.8125\n",
      "Epoch [60/100], Train Loss: 69314.6094, Val Loss: 69134.8359\n",
      "Epoch [70/100], Train Loss: 69077.1953, Val Loss: 68879.5078\n",
      "Epoch [80/100], Train Loss: 68772.8594, Val Loss: 68563.8672\n",
      "Epoch [90/100], Train Loss: 68460.2734, Val Loss: 68250.5078\n",
      "Epoch [100/100], Train Loss: 68174.6484, Val Loss: 67964.1328\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69511.8359, Val Loss: 69359.0859\n",
      "Epoch [20/100], Train Loss: 69511.1797, Val Loss: 69358.5000\n",
      "Epoch [30/100], Train Loss: 69510.9922, Val Loss: 69358.3125\n",
      "Epoch [40/100], Train Loss: 69510.9375, Val Loss: 69358.2656\n",
      "Epoch [50/100], Train Loss: 69510.9141, Val Loss: 69358.2344\n",
      "Epoch [60/100], Train Loss: 69510.8984, Val Loss: 69358.2344\n",
      "Epoch [70/100], Train Loss: 69510.9141, Val Loss: 69358.2344\n",
      "Epoch [80/100], Train Loss: 69510.9062, Val Loss: 69358.2344\n",
      "Epoch [90/100], Train Loss: 69510.9141, Val Loss: 69358.2344\n",
      "Epoch [100/100], Train Loss: 69510.8906, Val Loss: 69358.2344\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69509.1016, Val Loss: 69356.3203\n",
      "Epoch [20/100], Train Loss: 69508.4922, Val Loss: 69355.7812\n",
      "Epoch [30/100], Train Loss: 69508.3281, Val Loss: 69355.6250\n",
      "Epoch [40/100], Train Loss: 69508.2578, Val Loss: 69355.5781\n",
      "Epoch [50/100], Train Loss: 69508.2500, Val Loss: 69355.5547\n",
      "Epoch [60/100], Train Loss: 69508.2422, Val Loss: 69355.5547\n",
      "Epoch [70/100], Train Loss: 69508.2422, Val Loss: 69355.5547\n",
      "Epoch [80/100], Train Loss: 69508.2266, Val Loss: 69355.5547\n",
      "Epoch [90/100], Train Loss: 69508.2266, Val Loss: 69355.5547\n",
      "Epoch [100/100], Train Loss: 69508.2344, Val Loss: 69355.5547\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69616.7422, Val Loss: 69464.8750\n",
      "Epoch [20/100], Train Loss: 69616.1875, Val Loss: 69464.3672\n",
      "Epoch [30/100], Train Loss: 69616.0469, Val Loss: 69464.2188\n",
      "Epoch [40/100], Train Loss: 69615.9766, Val Loss: 69464.1719\n",
      "Epoch [50/100], Train Loss: 69615.9531, Val Loss: 69464.1562\n",
      "Epoch [60/100], Train Loss: 69615.9453, Val Loss: 69464.1562\n",
      "Epoch [70/100], Train Loss: 69615.9453, Val Loss: 69464.1484\n",
      "Epoch [80/100], Train Loss: 69615.9531, Val Loss: 69464.1484\n",
      "Epoch [90/100], Train Loss: 69615.9609, Val Loss: 69464.1484\n",
      "Epoch [100/100], Train Loss: 69615.9453, Val Loss: 69464.1484\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68876.4375, Val Loss: 68603.9609\n",
      "Epoch [20/100], Train Loss: 67359.2188, Val Loss: 67026.7188\n",
      "Epoch [30/100], Train Loss: 65403.2656, Val Loss: 65051.7305\n",
      "Epoch [40/100], Train Loss: 63264.9648, Val Loss: 62901.3672\n",
      "Epoch [50/100], Train Loss: 61035.3711, Val Loss: 60656.3750\n",
      "Epoch [60/100], Train Loss: 58785.6406, Val Loss: 58382.9062\n",
      "Epoch [70/100], Train Loss: 56577.5938, Val Loss: 56149.0234\n",
      "Epoch [80/100], Train Loss: 54487.5703, Val Loss: 54040.6875\n",
      "Epoch [90/100], Train Loss: 52566.6484, Val Loss: 52111.6836\n",
      "Epoch [100/100], Train Loss: 50800.6758, Val Loss: 50342.6641\n",
      "New best score: 50342.6640625 with params: {'hidden_dim': 64, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68917.2031, Val Loss: 68642.3906\n",
      "Epoch [20/100], Train Loss: 67412.9062, Val Loss: 67075.6406\n",
      "Epoch [30/100], Train Loss: 65471.0898, Val Loss: 65111.2734\n",
      "Epoch [40/100], Train Loss: 63328.2656, Val Loss: 62958.7695\n",
      "Epoch [50/100], Train Loss: 61099.5508, Val Loss: 60717.3750\n",
      "Epoch [60/100], Train Loss: 58863.8711, Val Loss: 58459.1562\n",
      "Epoch [70/100], Train Loss: 56660.3594, Val Loss: 56230.6289\n",
      "Epoch [80/100], Train Loss: 54557.7734, Val Loss: 54110.7812\n",
      "Epoch [90/100], Train Loss: 52619.6680, Val Loss: 52164.8164\n",
      "Epoch [100/100], Train Loss: 50844.0000, Val Loss: 50386.5273\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68881.2656, Val Loss: 68606.2031\n",
      "Epoch [20/100], Train Loss: 67348.4531, Val Loss: 67015.3672\n",
      "Epoch [30/100], Train Loss: 65377.5703, Val Loss: 65026.6758\n",
      "Epoch [40/100], Train Loss: 63219.9453, Val Loss: 62860.6016\n",
      "Epoch [50/100], Train Loss: 60979.8867, Val Loss: 60606.2227\n",
      "Epoch [60/100], Train Loss: 58726.2891, Val Loss: 58328.1133\n",
      "Epoch [70/100], Train Loss: 56511.6367, Val Loss: 56085.9414\n",
      "Epoch [80/100], Train Loss: 54407.3555, Val Loss: 53964.3750\n",
      "Epoch [90/100], Train Loss: 52465.9922, Val Loss: 52014.6211\n",
      "Epoch [100/100], Train Loss: 50675.3359, Val Loss: 50220.2891\n",
      "New best score: 50220.2890625 with params: {'hidden_dim': 64, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.3}\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69550.6484, Val Loss: 69394.2578\n",
      "Epoch [20/100], Train Loss: 69498.7891, Val Loss: 69341.0859\n",
      "Epoch [30/100], Train Loss: 69429.2812, Val Loss: 69269.4766\n",
      "Epoch [40/100], Train Loss: 69335.9219, Val Loss: 69173.5078\n",
      "Epoch [50/100], Train Loss: 69215.1250, Val Loss: 69049.9297\n",
      "Epoch [60/100], Train Loss: 69066.0391, Val Loss: 68898.1875\n",
      "Epoch [70/100], Train Loss: 68890.2656, Val Loss: 68720.2188\n",
      "Epoch [80/100], Train Loss: 68691.2969, Val Loss: 68519.8281\n",
      "Epoch [90/100], Train Loss: 68473.6719, Val Loss: 68301.6094\n",
      "Epoch [100/100], Train Loss: 68242.0547, Val Loss: 68070.1953\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69511.7266, Val Loss: 69352.2344\n",
      "Epoch [20/100], Train Loss: 69457.3672, Val Loss: 69296.1875\n",
      "Epoch [30/100], Train Loss: 69386.4062, Val Loss: 69222.6719\n",
      "Epoch [40/100], Train Loss: 69292.2500, Val Loss: 69125.3828\n",
      "Epoch [50/100], Train Loss: 69170.9844, Val Loss: 69000.7891\n",
      "Epoch [60/100], Train Loss: 69021.6406, Val Loss: 68848.3594\n",
      "Epoch [70/100], Train Loss: 68845.9453, Val Loss: 68670.0938\n",
      "Epoch [80/100], Train Loss: 68647.3516, Val Loss: 68469.6719\n",
      "Epoch [90/100], Train Loss: 68430.2578, Val Loss: 68251.6016\n",
      "Epoch [100/100], Train Loss: 68199.0469, Val Loss: 68020.2969\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69479.1797, Val Loss: 69322.3672\n",
      "Epoch [20/100], Train Loss: 69425.5859, Val Loss: 69267.5234\n",
      "Epoch [30/100], Train Loss: 69356.9531, Val Loss: 69196.8516\n",
      "Epoch [40/100], Train Loss: 69266.1172, Val Loss: 69103.4453\n",
      "Epoch [50/100], Train Loss: 69148.7422, Val Loss: 68983.2812\n",
      "Epoch [60/100], Train Loss: 69003.1953, Val Loss: 68835.0625\n",
      "Epoch [70/100], Train Loss: 68830.4062, Val Loss: 68660.0703\n",
      "Epoch [80/100], Train Loss: 68633.3281, Val Loss: 68461.4531\n",
      "Epoch [90/100], Train Loss: 68416.0312, Val Loss: 68243.4766\n",
      "Epoch [100/100], Train Loss: 68182.9219, Val Loss: 68010.5391\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69589.2734, Val Loss: 69436.8203\n",
      "Epoch [20/100], Train Loss: 69587.7031, Val Loss: 69435.4219\n",
      "Epoch [30/100], Train Loss: 69587.2578, Val Loss: 69435.0078\n",
      "Epoch [40/100], Train Loss: 69587.1016, Val Loss: 69434.8672\n",
      "Epoch [50/100], Train Loss: 69587.0547, Val Loss: 69434.8203\n",
      "Epoch [60/100], Train Loss: 69587.0391, Val Loss: 69434.8125\n",
      "Epoch [70/100], Train Loss: 69587.0391, Val Loss: 69434.8125\n",
      "Epoch [80/100], Train Loss: 69587.0391, Val Loss: 69434.8125\n",
      "Epoch [90/100], Train Loss: 69587.0391, Val Loss: 69434.8125\n",
      "Epoch [100/100], Train Loss: 69587.0391, Val Loss: 69434.8125\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69573.0469, Val Loss: 69420.5078\n",
      "Epoch [20/100], Train Loss: 69571.3828, Val Loss: 69419.0234\n",
      "Epoch [30/100], Train Loss: 69570.9141, Val Loss: 69418.5781\n",
      "Epoch [40/100], Train Loss: 69570.7500, Val Loss: 69418.4297\n",
      "Epoch [50/100], Train Loss: 69570.6953, Val Loss: 69418.3828\n",
      "Epoch [60/100], Train Loss: 69570.6875, Val Loss: 69418.3750\n",
      "Epoch [70/100], Train Loss: 69570.6719, Val Loss: 69418.3672\n",
      "Epoch [80/100], Train Loss: 69570.6719, Val Loss: 69418.3672\n",
      "Epoch [90/100], Train Loss: 69570.6719, Val Loss: 69418.3672\n",
      "Epoch [100/100], Train Loss: 69570.6719, Val Loss: 69418.3672\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69595.0547, Val Loss: 69443.6797\n",
      "Epoch [20/100], Train Loss: 69593.2031, Val Loss: 69442.0234\n",
      "Epoch [30/100], Train Loss: 69592.6875, Val Loss: 69441.5312\n",
      "Epoch [40/100], Train Loss: 69592.5000, Val Loss: 69441.3672\n",
      "Epoch [50/100], Train Loss: 69592.4375, Val Loss: 69441.3203\n",
      "Epoch [60/100], Train Loss: 69592.4297, Val Loss: 69441.3125\n",
      "Epoch [70/100], Train Loss: 69592.4219, Val Loss: 69441.3047\n",
      "Epoch [80/100], Train Loss: 69592.4141, Val Loss: 69441.2969\n",
      "Epoch [90/100], Train Loss: 69592.4141, Val Loss: 69441.2969\n",
      "Epoch [100/100], Train Loss: 69592.4141, Val Loss: 69441.2969\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68742.1250, Val Loss: 68371.2891\n",
      "Epoch [20/100], Train Loss: 66180.2422, Val Loss: 65739.5078\n",
      "Epoch [30/100], Train Loss: 63756.0391, Val Loss: 63331.1797\n",
      "Epoch [40/100], Train Loss: 61649.8164, Val Loss: 61226.1875\n",
      "Epoch [50/100], Train Loss: 59699.1680, Val Loss: 59269.7539\n",
      "Epoch [60/100], Train Loss: 57874.3516, Val Loss: 57441.6641\n",
      "Epoch [70/100], Train Loss: 56158.8477, Val Loss: 55721.7109\n",
      "Epoch [80/100], Train Loss: 54534.6172, Val Loss: 54095.3047\n",
      "Epoch [90/100], Train Loss: 52996.1992, Val Loss: 52555.1055\n",
      "Epoch [100/100], Train Loss: 51529.0977, Val Loss: 51082.8203\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68680.6094, Val Loss: 68302.1875\n",
      "Epoch [20/100], Train Loss: 66068.4375, Val Loss: 65623.8047\n",
      "Epoch [30/100], Train Loss: 63668.5430, Val Loss: 63240.0273\n",
      "Epoch [40/100], Train Loss: 61561.3984, Val Loss: 61135.6875\n",
      "Epoch [50/100], Train Loss: 59614.6016, Val Loss: 59183.2461\n",
      "Epoch [60/100], Train Loss: 57792.7070, Val Loss: 57359.4648\n",
      "Epoch [70/100], Train Loss: 56079.0312, Val Loss: 55642.7578\n",
      "Epoch [80/100], Train Loss: 54459.5508, Val Loss: 54019.5156\n",
      "Epoch [90/100], Train Loss: 52925.0352, Val Loss: 52482.9297\n",
      "Epoch [100/100], Train Loss: 51460.8008, Val Loss: 51013.0234\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68601.3281, Val Loss: 68209.3438\n",
      "Epoch [20/100], Train Loss: 65989.0703, Val Loss: 65540.0312\n",
      "Epoch [30/100], Train Loss: 63597.3203, Val Loss: 63168.3203\n",
      "Epoch [40/100], Train Loss: 61512.3906, Val Loss: 61087.1211\n",
      "Epoch [50/100], Train Loss: 59580.7031, Val Loss: 59149.8320\n",
      "Epoch [60/100], Train Loss: 57770.9375, Val Loss: 57336.2344\n",
      "Epoch [70/100], Train Loss: 56067.3477, Val Loss: 55629.8203\n",
      "Epoch [80/100], Train Loss: 54455.2891, Val Loss: 54014.5273\n",
      "Epoch [90/100], Train Loss: 52927.4609, Val Loss: 52483.9531\n",
      "Epoch [100/100], Train Loss: 51470.9336, Val Loss: 51021.5039\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69492.2188, Val Loss: 69336.2891\n",
      "Epoch [20/100], Train Loss: 69453.9062, Val Loss: 69295.1797\n",
      "Epoch [30/100], Train Loss: 69380.4922, Val Loss: 69215.6641\n",
      "Epoch [40/100], Train Loss: 69234.1875, Val Loss: 69058.3594\n",
      "Epoch [50/100], Train Loss: 68976.3047, Val Loss: 68788.2500\n",
      "Epoch [60/100], Train Loss: 68610.6719, Val Loss: 68412.4531\n",
      "Epoch [70/100], Train Loss: 68176.8281, Val Loss: 67974.9531\n",
      "Epoch [80/100], Train Loss: 67724.7734, Val Loss: 67521.2188\n",
      "Epoch [90/100], Train Loss: 67281.9609, Val Loss: 67077.2734\n",
      "Epoch [100/100], Train Loss: 66858.0781, Val Loss: 66651.5547\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69534.4609, Val Loss: 69378.4375\n",
      "Epoch [20/100], Train Loss: 69486.9453, Val Loss: 69327.6562\n",
      "Epoch [30/100], Train Loss: 69394.4609, Val Loss: 69228.3672\n",
      "Epoch [40/100], Train Loss: 69214.0703, Val Loss: 69035.7578\n",
      "Epoch [50/100], Train Loss: 68908.3516, Val Loss: 68715.8906\n",
      "Epoch [60/100], Train Loss: 68489.4688, Val Loss: 68287.2266\n",
      "Epoch [70/100], Train Loss: 68011.0391, Val Loss: 67806.6094\n",
      "Epoch [80/100], Train Loss: 67530.2266, Val Loss: 67324.4922\n",
      "Epoch [90/100], Train Loss: 67072.2266, Val Loss: 66865.0781\n",
      "Epoch [100/100], Train Loss: 66645.0312, Val Loss: 66433.3281\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69504.9141, Val Loss: 69348.2422\n",
      "Epoch [20/100], Train Loss: 69454.8438, Val Loss: 69294.9531\n",
      "Epoch [30/100], Train Loss: 69360.7109, Val Loss: 69193.6484\n",
      "Epoch [40/100], Train Loss: 69183.5469, Val Loss: 69003.1719\n",
      "Epoch [50/100], Train Loss: 68888.7109, Val Loss: 68696.3125\n",
      "Epoch [60/100], Train Loss: 68492.8281, Val Loss: 68290.9766\n",
      "Epoch [70/100], Train Loss: 68039.7031, Val Loss: 67835.5156\n",
      "Epoch [80/100], Train Loss: 67581.5391, Val Loss: 67374.0703\n",
      "Epoch [90/100], Train Loss: 67138.4609, Val Loss: 66929.5469\n",
      "Epoch [100/100], Train Loss: 66720.6328, Val Loss: 66508.5391\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69575.0625, Val Loss: 69422.9219\n",
      "Epoch [20/100], Train Loss: 69574.0391, Val Loss: 69422.0391\n",
      "Epoch [30/100], Train Loss: 69573.7578, Val Loss: 69421.7812\n",
      "Epoch [40/100], Train Loss: 69573.6953, Val Loss: 69421.6953\n",
      "Epoch [50/100], Train Loss: 69573.6719, Val Loss: 69421.6641\n",
      "Epoch [60/100], Train Loss: 69573.6406, Val Loss: 69421.6562\n",
      "Epoch [70/100], Train Loss: 69573.6641, Val Loss: 69421.6562\n",
      "Epoch [80/100], Train Loss: 69573.6562, Val Loss: 69421.6562\n",
      "Epoch [90/100], Train Loss: 69573.6484, Val Loss: 69421.6562\n",
      "Epoch [100/100], Train Loss: 69573.6250, Val Loss: 69421.6562\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69557.0000, Val Loss: 69404.5156\n",
      "Epoch [20/100], Train Loss: 69556.0547, Val Loss: 69403.6250\n",
      "Epoch [30/100], Train Loss: 69555.7109, Val Loss: 69403.3516\n",
      "Epoch [40/100], Train Loss: 69555.6484, Val Loss: 69403.2734\n",
      "Epoch [50/100], Train Loss: 69555.6250, Val Loss: 69403.2422\n",
      "Epoch [60/100], Train Loss: 69555.6719, Val Loss: 69403.2344\n",
      "Epoch [70/100], Train Loss: 69555.6016, Val Loss: 69403.2344\n",
      "Epoch [80/100], Train Loss: 69555.5703, Val Loss: 69403.2344\n",
      "Epoch [90/100], Train Loss: 69555.5625, Val Loss: 69403.2344\n",
      "Epoch [100/100], Train Loss: 69555.5781, Val Loss: 69403.2344\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69534.5312, Val Loss: 69382.0156\n",
      "Epoch [20/100], Train Loss: 69533.5000, Val Loss: 69381.0938\n",
      "Epoch [30/100], Train Loss: 69533.2578, Val Loss: 69380.8203\n",
      "Epoch [40/100], Train Loss: 69533.1250, Val Loss: 69380.7344\n",
      "Epoch [50/100], Train Loss: 69533.1016, Val Loss: 69380.7031\n",
      "Epoch [60/100], Train Loss: 69533.0625, Val Loss: 69380.6953\n",
      "Epoch [70/100], Train Loss: 69533.0703, Val Loss: 69380.6953\n",
      "Epoch [80/100], Train Loss: 69533.0625, Val Loss: 69380.6875\n",
      "Epoch [90/100], Train Loss: 69533.0703, Val Loss: 69380.6875\n",
      "Epoch [100/100], Train Loss: 69533.0703, Val Loss: 69380.6875\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68242.1641, Val Loss: 67781.4531\n",
      "Epoch [20/100], Train Loss: 65629.4531, Val Loss: 65224.8672\n",
      "Epoch [30/100], Train Loss: 63561.7695, Val Loss: 63156.9141\n",
      "Epoch [40/100], Train Loss: 61609.2461, Val Loss: 61196.2891\n",
      "Epoch [50/100], Train Loss: 59757.7656, Val Loss: 59335.6758\n",
      "Epoch [60/100], Train Loss: 58004.9766, Val Loss: 57578.9531\n",
      "Epoch [70/100], Train Loss: 56348.4766, Val Loss: 55918.3477\n",
      "Epoch [80/100], Train Loss: 54772.6797, Val Loss: 54337.2539\n",
      "Epoch [90/100], Train Loss: 53260.3438, Val Loss: 52825.3242\n",
      "Epoch [100/100], Train Loss: 51811.8516, Val Loss: 51373.1680\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68490.1797, Val Loss: 68041.4922\n",
      "Epoch [20/100], Train Loss: 65906.1562, Val Loss: 65510.2422\n",
      "Epoch [30/100], Train Loss: 63893.3906, Val Loss: 63489.0234\n",
      "Epoch [40/100], Train Loss: 61935.9688, Val Loss: 61525.7344\n",
      "Epoch [50/100], Train Loss: 60084.6289, Val Loss: 59665.4883\n",
      "Epoch [60/100], Train Loss: 58331.3984, Val Loss: 57905.5117\n",
      "Epoch [70/100], Train Loss: 56672.4219, Val Loss: 56246.3750\n",
      "Epoch [80/100], Train Loss: 55100.7930, Val Loss: 54669.4727\n",
      "Epoch [90/100], Train Loss: 53598.3594, Val Loss: 53162.2070\n",
      "Epoch [100/100], Train Loss: 52153.1016, Val Loss: 51714.1914\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68460.8906, Val Loss: 68009.3359\n",
      "Epoch [20/100], Train Loss: 65816.6406, Val Loss: 65414.4453\n",
      "Epoch [30/100], Train Loss: 63779.4336, Val Loss: 63378.1758\n",
      "Epoch [40/100], Train Loss: 61843.2148, Val Loss: 61433.8203\n",
      "Epoch [50/100], Train Loss: 60004.2188, Val Loss: 59584.0000\n",
      "Epoch [60/100], Train Loss: 58222.4219, Val Loss: 57793.8828\n",
      "Epoch [70/100], Train Loss: 56540.2891, Val Loss: 56109.7031\n",
      "Epoch [80/100], Train Loss: 54943.2852, Val Loss: 54506.6445\n",
      "Epoch [90/100], Train Loss: 53416.6953, Val Loss: 52977.9453\n",
      "Epoch [100/100], Train Loss: 51952.3711, Val Loss: 51507.2266\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69501.0312, Val Loss: 69345.1484\n",
      "Epoch [20/100], Train Loss: 69461.0078, Val Loss: 69302.2734\n",
      "Epoch [30/100], Train Loss: 69373.9375, Val Loss: 69206.1875\n",
      "Epoch [40/100], Train Loss: 69146.3359, Val Loss: 68956.3594\n",
      "Epoch [50/100], Train Loss: 68677.6875, Val Loss: 68463.5781\n",
      "Epoch [60/100], Train Loss: 68072.9453, Val Loss: 67850.4062\n",
      "Epoch [70/100], Train Loss: 67484.4297, Val Loss: 67261.9844\n",
      "Epoch [80/100], Train Loss: 66968.2969, Val Loss: 66746.4062\n",
      "Epoch [90/100], Train Loss: 66530.9766, Val Loss: 66311.3438\n",
      "Epoch [100/100], Train Loss: 66160.8516, Val Loss: 65943.8438\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69513.9297, Val Loss: 69358.4219\n",
      "Epoch [20/100], Train Loss: 69479.0391, Val Loss: 69321.4766\n",
      "Epoch [30/100], Train Loss: 69408.8750, Val Loss: 69244.1719\n",
      "Epoch [40/100], Train Loss: 69228.7266, Val Loss: 69044.8203\n",
      "Epoch [50/100], Train Loss: 68834.8984, Val Loss: 68625.2734\n",
      "Epoch [60/100], Train Loss: 68279.6094, Val Loss: 68058.0703\n",
      "Epoch [70/100], Train Loss: 67702.9453, Val Loss: 67476.5938\n",
      "Epoch [80/100], Train Loss: 67176.4219, Val Loss: 66950.6562\n",
      "Epoch [90/100], Train Loss: 66731.6484, Val Loss: 66508.9297\n",
      "Epoch [100/100], Train Loss: 66360.0078, Val Loss: 66140.2500\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69572.1406, Val Loss: 69417.1016\n",
      "Epoch [20/100], Train Loss: 69537.4062, Val Loss: 69380.2969\n",
      "Epoch [30/100], Train Loss: 69469.1250, Val Loss: 69305.0312\n",
      "Epoch [40/100], Train Loss: 69299.7969, Val Loss: 69117.1719\n",
      "Epoch [50/100], Train Loss: 68941.2734, Val Loss: 68734.5391\n",
      "Epoch [60/100], Train Loss: 68426.3672, Val Loss: 68202.8594\n",
      "Epoch [70/100], Train Loss: 67852.6562, Val Loss: 67623.7656\n",
      "Epoch [80/100], Train Loss: 67309.4141, Val Loss: 67076.3750\n",
      "Epoch [90/100], Train Loss: 66839.9766, Val Loss: 66609.6719\n",
      "Epoch [100/100], Train Loss: 66447.4141, Val Loss: 66221.6719\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69599.5312, Val Loss: 69447.4453\n",
      "Epoch [20/100], Train Loss: 69598.6797, Val Loss: 69446.6719\n",
      "Epoch [30/100], Train Loss: 69598.4219, Val Loss: 69446.4453\n",
      "Epoch [40/100], Train Loss: 69598.3281, Val Loss: 69446.3672\n",
      "Epoch [50/100], Train Loss: 69598.3125, Val Loss: 69446.3438\n",
      "Epoch [60/100], Train Loss: 69598.2969, Val Loss: 69446.3438\n",
      "Epoch [70/100], Train Loss: 69598.3047, Val Loss: 69446.3281\n",
      "Epoch [80/100], Train Loss: 69598.2969, Val Loss: 69446.3281\n",
      "Epoch [90/100], Train Loss: 69598.3047, Val Loss: 69446.3281\n",
      "Epoch [100/100], Train Loss: 69598.3047, Val Loss: 69446.3281\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69503.2422, Val Loss: 69350.3828\n",
      "Epoch [20/100], Train Loss: 69502.3984, Val Loss: 69349.6562\n",
      "Epoch [30/100], Train Loss: 69502.1719, Val Loss: 69349.4453\n",
      "Epoch [40/100], Train Loss: 69502.1016, Val Loss: 69349.3750\n",
      "Epoch [50/100], Train Loss: 69502.0703, Val Loss: 69349.3359\n",
      "Epoch [60/100], Train Loss: 69502.0781, Val Loss: 69349.3359\n",
      "Epoch [70/100], Train Loss: 69502.0625, Val Loss: 69349.3359\n",
      "Epoch [80/100], Train Loss: 69502.0703, Val Loss: 69349.3359\n",
      "Epoch [90/100], Train Loss: 69502.0625, Val Loss: 69349.3359\n",
      "Epoch [100/100], Train Loss: 69502.0625, Val Loss: 69349.3359\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69602.2422, Val Loss: 69450.1953\n",
      "Epoch [20/100], Train Loss: 69601.3984, Val Loss: 69449.4375\n",
      "Epoch [30/100], Train Loss: 69601.1641, Val Loss: 69449.2188\n",
      "Epoch [40/100], Train Loss: 69601.0781, Val Loss: 69449.1406\n",
      "Epoch [50/100], Train Loss: 69601.0625, Val Loss: 69449.1250\n",
      "Epoch [60/100], Train Loss: 69601.0391, Val Loss: 69449.1094\n",
      "Epoch [70/100], Train Loss: 69601.0391, Val Loss: 69449.1094\n",
      "Epoch [80/100], Train Loss: 69601.0391, Val Loss: 69449.1094\n",
      "Epoch [90/100], Train Loss: 69601.0391, Val Loss: 69449.1094\n",
      "Epoch [100/100], Train Loss: 69601.0312, Val Loss: 69449.1094\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68006.6484, Val Loss: 67511.0703\n",
      "Epoch [20/100], Train Loss: 65494.3438, Val Loss: 65103.7422\n",
      "Epoch [30/100], Train Loss: 63521.4023, Val Loss: 63122.2969\n",
      "Epoch [40/100], Train Loss: 61626.3555, Val Loss: 61218.2734\n",
      "Epoch [50/100], Train Loss: 59826.4023, Val Loss: 59409.0820\n",
      "Epoch [60/100], Train Loss: 58114.5156, Val Loss: 57694.8945\n",
      "Epoch [70/100], Train Loss: 56491.5547, Val Loss: 56066.4062\n",
      "Epoch [80/100], Train Loss: 54944.6406, Val Loss: 54513.0078\n",
      "Epoch [90/100], Train Loss: 53457.5547, Val Loss: 53030.6289\n",
      "Epoch [100/100], Train Loss: 52035.6016, Val Loss: 51598.9922\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68307.1875, Val Loss: 67846.1484\n",
      "Epoch [20/100], Train Loss: 65860.6172, Val Loss: 65464.4180\n",
      "Epoch [30/100], Train Loss: 63861.4219, Val Loss: 63462.6523\n",
      "Epoch [40/100], Train Loss: 61902.3281, Val Loss: 61486.9023\n",
      "Epoch [50/100], Train Loss: 60023.4492, Val Loss: 59600.7656\n",
      "Epoch [60/100], Train Loss: 58242.1758, Val Loss: 57814.0117\n",
      "Epoch [70/100], Train Loss: 56559.0469, Val Loss: 56127.0664\n",
      "Epoch [80/100], Train Loss: 54962.7188, Val Loss: 54529.5469\n",
      "Epoch [90/100], Train Loss: 53440.3438, Val Loss: 53004.4023\n",
      "Epoch [100/100], Train Loss: 51976.8945, Val Loss: 51537.7422\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68181.0781, Val Loss: 67689.3281\n",
      "Epoch [20/100], Train Loss: 65748.1094, Val Loss: 65364.0742\n",
      "Epoch [30/100], Train Loss: 63825.2070, Val Loss: 63432.9219\n",
      "Epoch [40/100], Train Loss: 61976.3125, Val Loss: 61575.2188\n",
      "Epoch [50/100], Train Loss: 60220.2422, Val Loss: 59811.9688\n",
      "Epoch [60/100], Train Loss: 58538.2891, Val Loss: 58117.3203\n",
      "Epoch [70/100], Train Loss: 56902.5820, Val Loss: 56479.4258\n",
      "Epoch [80/100], Train Loss: 55350.5000, Val Loss: 54920.1602\n",
      "Epoch [90/100], Train Loss: 53859.3789, Val Loss: 53426.9727\n",
      "Epoch [100/100], Train Loss: 52441.4922, Val Loss: 52014.6719\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69551.5078, Val Loss: 69396.2891\n",
      "Epoch [20/100], Train Loss: 69515.0625, Val Loss: 69357.3906\n",
      "Epoch [30/100], Train Loss: 69432.1406, Val Loss: 69264.6406\n",
      "Epoch [40/100], Train Loss: 69166.1484, Val Loss: 68965.9922\n",
      "Epoch [50/100], Train Loss: 68556.8047, Val Loss: 68328.5703\n",
      "Epoch [60/100], Train Loss: 67861.5078, Val Loss: 67632.5781\n",
      "Epoch [70/100], Train Loss: 67287.7812, Val Loss: 67065.2656\n",
      "Epoch [80/100], Train Loss: 66852.0312, Val Loss: 66636.0781\n",
      "Epoch [90/100], Train Loss: 66502.4688, Val Loss: 66289.5156\n",
      "Epoch [100/100], Train Loss: 66197.4922, Val Loss: 65986.1016\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69514.1875, Val Loss: 69358.7422\n",
      "Epoch [20/100], Train Loss: 69480.3828, Val Loss: 69322.7500\n",
      "Epoch [30/100], Train Loss: 69406.9062, Val Loss: 69240.6562\n",
      "Epoch [40/100], Train Loss: 69175.0469, Val Loss: 68977.7500\n",
      "Epoch [50/100], Train Loss: 68627.7344, Val Loss: 68395.0781\n",
      "Epoch [60/100], Train Loss: 67949.1562, Val Loss: 67714.1953\n",
      "Epoch [70/100], Train Loss: 67351.6719, Val Loss: 67121.5234\n",
      "Epoch [80/100], Train Loss: 66882.1484, Val Loss: 66659.3906\n",
      "Epoch [90/100], Train Loss: 66511.8750, Val Loss: 66294.4766\n",
      "Epoch [100/100], Train Loss: 66198.0781, Val Loss: 65983.4453\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69512.6797, Val Loss: 69356.9844\n",
      "Epoch [20/100], Train Loss: 69475.0781, Val Loss: 69316.7344\n",
      "Epoch [30/100], Train Loss: 69392.1328, Val Loss: 69223.5938\n",
      "Epoch [40/100], Train Loss: 69133.4766, Val Loss: 68926.5078\n",
      "Epoch [50/100], Train Loss: 68518.3594, Val Loss: 68270.0703\n",
      "Epoch [60/100], Train Loss: 67804.8750, Val Loss: 67556.6094\n",
      "Epoch [70/100], Train Loss: 67228.3125, Val Loss: 66992.4453\n",
      "Epoch [80/100], Train Loss: 66792.9844, Val Loss: 66567.2344\n",
      "Epoch [90/100], Train Loss: 66444.1328, Val Loss: 66224.6406\n",
      "Epoch [100/100], Train Loss: 66143.1875, Val Loss: 65926.3125\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69596.2344, Val Loss: 69444.1328\n",
      "Epoch [20/100], Train Loss: 69595.3516, Val Loss: 69443.3281\n",
      "Epoch [30/100], Train Loss: 69595.0859, Val Loss: 69443.0938\n",
      "Epoch [40/100], Train Loss: 69595.0078, Val Loss: 69443.0234\n",
      "Epoch [50/100], Train Loss: 69594.9766, Val Loss: 69442.9922\n",
      "Epoch [60/100], Train Loss: 69594.9688, Val Loss: 69442.9844\n",
      "Epoch [70/100], Train Loss: 69594.9688, Val Loss: 69442.9844\n",
      "Epoch [80/100], Train Loss: 69594.9688, Val Loss: 69442.9844\n",
      "Epoch [90/100], Train Loss: 69594.9688, Val Loss: 69442.9766\n",
      "Epoch [100/100], Train Loss: 69594.9609, Val Loss: 69442.9766\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69580.5391, Val Loss: 69428.3047\n",
      "Epoch [20/100], Train Loss: 69579.6406, Val Loss: 69427.5000\n",
      "Epoch [30/100], Train Loss: 69579.3906, Val Loss: 69427.2656\n",
      "Epoch [40/100], Train Loss: 69579.3047, Val Loss: 69427.1797\n",
      "Epoch [50/100], Train Loss: 69579.2734, Val Loss: 69427.1562\n",
      "Epoch [60/100], Train Loss: 69579.2656, Val Loss: 69427.1562\n",
      "Epoch [70/100], Train Loss: 69579.2578, Val Loss: 69427.1562\n",
      "Epoch [80/100], Train Loss: 69579.2578, Val Loss: 69427.1562\n",
      "Epoch [90/100], Train Loss: 69579.2578, Val Loss: 69427.1562\n",
      "Epoch [100/100], Train Loss: 69579.2578, Val Loss: 69427.1562\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69516.2656, Val Loss: 69363.4922\n",
      "Epoch [20/100], Train Loss: 69515.3438, Val Loss: 69362.6562\n",
      "Epoch [30/100], Train Loss: 69515.0625, Val Loss: 69362.4141\n",
      "Epoch [40/100], Train Loss: 69514.9688, Val Loss: 69362.3281\n",
      "Epoch [50/100], Train Loss: 69514.9453, Val Loss: 69362.3047\n",
      "Epoch [60/100], Train Loss: 69514.9297, Val Loss: 69362.2969\n",
      "Epoch [70/100], Train Loss: 69514.9297, Val Loss: 69362.2969\n",
      "Epoch [80/100], Train Loss: 69514.9297, Val Loss: 69362.2969\n",
      "Epoch [90/100], Train Loss: 69514.9219, Val Loss: 69362.2969\n",
      "Epoch [100/100], Train Loss: 69514.9453, Val Loss: 69362.2969\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68332.6484, Val Loss: 67953.2344\n",
      "Epoch [20/100], Train Loss: 65387.4219, Val Loss: 64899.1289\n",
      "Epoch [30/100], Train Loss: 61655.6680, Val Loss: 61141.3320\n",
      "Epoch [40/100], Train Loss: 57678.0508, Val Loss: 57152.5117\n",
      "Epoch [50/100], Train Loss: 53674.9727, Val Loss: 53128.6641\n",
      "Epoch [60/100], Train Loss: 49790.7812, Val Loss: 49214.6641\n",
      "Epoch [70/100], Train Loss: 46144.8398, Val Loss: 45549.0664\n",
      "Epoch [80/100], Train Loss: 42896.4922, Val Loss: 42299.0391\n",
      "Epoch [90/100], Train Loss: 40078.5742, Val Loss: 39494.9023\n",
      "Epoch [100/100], Train Loss: 37626.9961, Val Loss: 37062.4844\n",
      "New best score: 37062.484375 with params: {'hidden_dim': 128, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68414.6562, Val Loss: 68042.3203\n",
      "Epoch [20/100], Train Loss: 65540.3750, Val Loss: 65058.9961\n",
      "Epoch [30/100], Train Loss: 61855.0000, Val Loss: 61341.3516\n",
      "Epoch [40/100], Train Loss: 57880.5703, Val Loss: 57347.9805\n",
      "Epoch [50/100], Train Loss: 53842.5742, Val Loss: 53286.3477\n",
      "Epoch [60/100], Train Loss: 49908.7969, Val Loss: 49322.2656\n",
      "Epoch [70/100], Train Loss: 46234.8711, Val Loss: 45632.5625\n",
      "Epoch [80/100], Train Loss: 42980.9688, Val Loss: 42379.6328\n",
      "Epoch [90/100], Train Loss: 40164.5938, Val Loss: 39577.9414\n",
      "Epoch [100/100], Train Loss: 37712.1289, Val Loss: 37145.2578\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68388.7734, Val Loss: 68013.5703\n",
      "Epoch [20/100], Train Loss: 65489.6328, Val Loss: 64998.1055\n",
      "Epoch [30/100], Train Loss: 61770.4805, Val Loss: 61250.8867\n",
      "Epoch [40/100], Train Loss: 57785.0391, Val Loss: 57252.1094\n",
      "Epoch [50/100], Train Loss: 53756.2305, Val Loss: 53202.4102\n",
      "Epoch [60/100], Train Loss: 49845.8438, Val Loss: 49263.1328\n",
      "Epoch [70/100], Train Loss: 46186.1328, Val Loss: 45584.1602\n",
      "Epoch [80/100], Train Loss: 42933.7188, Val Loss: 42331.3008\n",
      "Epoch [90/100], Train Loss: 40119.8359, Val Loss: 39532.0586\n",
      "Epoch [100/100], Train Loss: 37677.1836, Val Loss: 37108.8828\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69491.6953, Val Loss: 69331.6797\n",
      "Epoch [20/100], Train Loss: 69400.6172, Val Loss: 69237.8203\n",
      "Epoch [30/100], Train Loss: 69271.1406, Val Loss: 69103.9688\n",
      "Epoch [40/100], Train Loss: 69089.5859, Val Loss: 68917.1094\n",
      "Epoch [50/100], Train Loss: 68848.8672, Val Loss: 68670.8516\n",
      "Epoch [60/100], Train Loss: 68548.8828, Val Loss: 68365.9922\n",
      "Epoch [70/100], Train Loss: 68195.7969, Val Loss: 68009.3438\n",
      "Epoch [80/100], Train Loss: 67799.5781, Val Loss: 67611.1328\n",
      "Epoch [90/100], Train Loss: 67371.0391, Val Loss: 67182.2188\n",
      "Epoch [100/100], Train Loss: 66919.8906, Val Loss: 66732.0625\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69465.4141, Val Loss: 69306.5547\n",
      "Epoch [20/100], Train Loss: 69377.6406, Val Loss: 69215.8281\n",
      "Epoch [30/100], Train Loss: 69250.9141, Val Loss: 69084.7891\n",
      "Epoch [40/100], Train Loss: 69072.5859, Val Loss: 68901.3594\n",
      "Epoch [50/100], Train Loss: 68835.7031, Val Loss: 68659.3438\n",
      "Epoch [60/100], Train Loss: 68540.2188, Val Loss: 68359.5312\n",
      "Epoch [70/100], Train Loss: 68192.1016, Val Loss: 68008.5234\n",
      "Epoch [80/100], Train Loss: 67800.9609, Val Loss: 67616.1406\n",
      "Epoch [90/100], Train Loss: 67377.3125, Val Loss: 67192.8047\n",
      "Epoch [100/100], Train Loss: 66930.7188, Val Loss: 66747.7969\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69493.2500, Val Loss: 69333.0703\n",
      "Epoch [20/100], Train Loss: 69407.8594, Val Loss: 69244.7188\n",
      "Epoch [30/100], Train Loss: 69286.1719, Val Loss: 69118.3438\n",
      "Epoch [40/100], Train Loss: 69114.1875, Val Loss: 68940.6094\n",
      "Epoch [50/100], Train Loss: 68883.8828, Val Loss: 68704.2031\n",
      "Epoch [60/100], Train Loss: 68594.0078, Val Loss: 68408.7500\n",
      "Epoch [70/100], Train Loss: 68249.8047, Val Loss: 68060.2969\n",
      "Epoch [80/100], Train Loss: 67860.8672, Val Loss: 67668.8828\n",
      "Epoch [90/100], Train Loss: 67438.0781, Val Loss: 67245.4062\n",
      "Epoch [100/100], Train Loss: 66991.1641, Val Loss: 66799.3359\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69564.4844, Val Loss: 69412.2031\n",
      "Epoch [20/100], Train Loss: 69562.2422, Val Loss: 69410.2031\n",
      "Epoch [30/100], Train Loss: 69561.4531, Val Loss: 69409.4531\n",
      "Epoch [40/100], Train Loss: 69561.1328, Val Loss: 69409.1406\n",
      "Epoch [50/100], Train Loss: 69561.0234, Val Loss: 69409.0547\n",
      "Epoch [60/100], Train Loss: 69560.9766, Val Loss: 69409.0156\n",
      "Epoch [70/100], Train Loss: 69560.9688, Val Loss: 69409.0156\n",
      "Epoch [80/100], Train Loss: 69560.9688, Val Loss: 69409.0156\n",
      "Epoch [90/100], Train Loss: 69560.9688, Val Loss: 69409.0156\n",
      "Epoch [100/100], Train Loss: 69560.9688, Val Loss: 69409.0078\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69573.8672, Val Loss: 69420.6250\n",
      "Epoch [20/100], Train Loss: 69571.7266, Val Loss: 69418.7031\n",
      "Epoch [30/100], Train Loss: 69571.1094, Val Loss: 69418.1328\n",
      "Epoch [40/100], Train Loss: 69570.8203, Val Loss: 69417.8672\n",
      "Epoch [50/100], Train Loss: 69570.7266, Val Loss: 69417.7891\n",
      "Epoch [60/100], Train Loss: 69570.6953, Val Loss: 69417.7656\n",
      "Epoch [70/100], Train Loss: 69570.6953, Val Loss: 69417.7500\n",
      "Epoch [80/100], Train Loss: 69570.6797, Val Loss: 69417.7500\n",
      "Epoch [90/100], Train Loss: 69570.6797, Val Loss: 69417.7500\n",
      "Epoch [100/100], Train Loss: 69570.6797, Val Loss: 69417.7500\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69574.8516, Val Loss: 69421.8516\n",
      "Epoch [20/100], Train Loss: 69572.5000, Val Loss: 69419.6484\n",
      "Epoch [30/100], Train Loss: 69571.1484, Val Loss: 69418.3984\n",
      "Epoch [40/100], Train Loss: 69570.6875, Val Loss: 69417.9922\n",
      "Epoch [50/100], Train Loss: 69570.5391, Val Loss: 69417.8516\n",
      "Epoch [60/100], Train Loss: 69570.4922, Val Loss: 69417.8125\n",
      "Epoch [70/100], Train Loss: 69570.4766, Val Loss: 69417.8125\n",
      "Epoch [80/100], Train Loss: 69570.4688, Val Loss: 69417.8047\n",
      "Epoch [90/100], Train Loss: 69570.4688, Val Loss: 69417.8047\n",
      "Epoch [100/100], Train Loss: 69570.4688, Val Loss: 69417.8047\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 67366.1406, Val Loss: 66711.2344\n",
      "Epoch [20/100], Train Loss: 62520.5352, Val Loss: 61874.6172\n",
      "Epoch [30/100], Train Loss: 58598.3359, Val Loss: 57981.1914\n",
      "Epoch [40/100], Train Loss: 55089.8672, Val Loss: 54465.9961\n",
      "Epoch [50/100], Train Loss: 51882.1680, Val Loss: 51268.7930\n",
      "Epoch [60/100], Train Loss: 48963.6953, Val Loss: 48364.3984\n",
      "Epoch [70/100], Train Loss: 46307.6484, Val Loss: 45718.0742\n",
      "Epoch [80/100], Train Loss: 43859.6211, Val Loss: 43275.5586\n",
      "Epoch [90/100], Train Loss: 41597.9180, Val Loss: 41040.3047\n",
      "Epoch [100/100], Train Loss: 39488.7734, Val Loss: 38953.6758\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 67181.3984, Val Loss: 66531.8203\n",
      "Epoch [20/100], Train Loss: 62491.7422, Val Loss: 61849.0430\n",
      "Epoch [30/100], Train Loss: 58576.1797, Val Loss: 57960.3750\n",
      "Epoch [40/100], Train Loss: 55081.3320, Val Loss: 54459.2148\n",
      "Epoch [50/100], Train Loss: 51884.3555, Val Loss: 51273.2930\n",
      "Epoch [60/100], Train Loss: 48975.4805, Val Loss: 48374.2695\n",
      "Epoch [70/100], Train Loss: 46329.5117, Val Loss: 45742.2188\n",
      "Epoch [80/100], Train Loss: 43896.0352, Val Loss: 43307.8398\n",
      "Epoch [90/100], Train Loss: 41628.6445, Val Loss: 41063.9922\n",
      "Epoch [100/100], Train Loss: 39519.9492, Val Loss: 38980.0430\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67107.8984, Val Loss: 66435.2031\n",
      "Epoch [20/100], Train Loss: 62286.2656, Val Loss: 61637.0586\n",
      "Epoch [30/100], Train Loss: 58401.6211, Val Loss: 57783.9102\n",
      "Epoch [40/100], Train Loss: 54918.5664, Val Loss: 54295.1055\n",
      "Epoch [50/100], Train Loss: 51732.4336, Val Loss: 51120.9844\n",
      "Epoch [60/100], Train Loss: 48835.7617, Val Loss: 48235.0820\n",
      "Epoch [70/100], Train Loss: 46197.7734, Val Loss: 45603.9492\n",
      "Epoch [80/100], Train Loss: 43765.3555, Val Loss: 43175.6250\n",
      "Epoch [90/100], Train Loss: 41502.5508, Val Loss: 40938.6094\n",
      "Epoch [100/100], Train Loss: 39415.3047, Val Loss: 38868.2344\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69519.4531, Val Loss: 69360.8047\n",
      "Epoch [20/100], Train Loss: 69421.3750, Val Loss: 69252.8906\n",
      "Epoch [30/100], Train Loss: 69170.9219, Val Loss: 68979.5938\n",
      "Epoch [40/100], Train Loss: 68645.8594, Val Loss: 68427.3359\n",
      "Epoch [50/100], Train Loss: 67873.5781, Val Loss: 67640.2812\n",
      "Epoch [60/100], Train Loss: 67005.9531, Val Loss: 66769.7422\n",
      "Epoch [70/100], Train Loss: 66155.4141, Val Loss: 65918.3047\n",
      "Epoch [80/100], Train Loss: 65355.1289, Val Loss: 65109.6367\n",
      "Epoch [90/100], Train Loss: 64604.7773, Val Loss: 64351.2578\n",
      "Epoch [100/100], Train Loss: 63914.6328, Val Loss: 63654.5430\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69522.1172, Val Loss: 69363.1562\n",
      "Epoch [20/100], Train Loss: 69419.6875, Val Loss: 69250.3203\n",
      "Epoch [30/100], Train Loss: 69160.1797, Val Loss: 68967.9453\n",
      "Epoch [40/100], Train Loss: 68635.1641, Val Loss: 68416.6250\n",
      "Epoch [50/100], Train Loss: 67878.8750, Val Loss: 67646.0000\n",
      "Epoch [60/100], Train Loss: 67033.5078, Val Loss: 66798.4297\n",
      "Epoch [70/100], Train Loss: 66208.4688, Val Loss: 65970.5938\n",
      "Epoch [80/100], Train Loss: 65428.2031, Val Loss: 65181.9336\n",
      "Epoch [90/100], Train Loss: 64691.8047, Val Loss: 64436.8398\n",
      "Epoch [100/100], Train Loss: 64008.8750, Val Loss: 63747.0039\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69496.5625, Val Loss: 69337.3984\n",
      "Epoch [20/100], Train Loss: 69398.3359, Val Loss: 69229.0469\n",
      "Epoch [30/100], Train Loss: 69149.3203, Val Loss: 68956.4766\n",
      "Epoch [40/100], Train Loss: 68634.0000, Val Loss: 68412.3750\n",
      "Epoch [50/100], Train Loss: 67877.9375, Val Loss: 67641.6797\n",
      "Epoch [60/100], Train Loss: 67033.1641, Val Loss: 66791.9141\n",
      "Epoch [70/100], Train Loss: 66205.1953, Val Loss: 65962.4062\n",
      "Epoch [80/100], Train Loss: 65423.8125, Val Loss: 65172.4883\n",
      "Epoch [90/100], Train Loss: 64684.0156, Val Loss: 64423.9492\n",
      "Epoch [100/100], Train Loss: 63995.7070, Val Loss: 63727.7344\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69520.2734, Val Loss: 69367.4141\n",
      "Epoch [20/100], Train Loss: 69518.8438, Val Loss: 69366.1250\n",
      "Epoch [30/100], Train Loss: 69518.4297, Val Loss: 69365.7266\n",
      "Epoch [40/100], Train Loss: 69518.2734, Val Loss: 69365.6016\n",
      "Epoch [50/100], Train Loss: 69518.2422, Val Loss: 69365.5547\n",
      "Epoch [60/100], Train Loss: 69518.2188, Val Loss: 69365.5547\n",
      "Epoch [70/100], Train Loss: 69518.2188, Val Loss: 69365.5391\n",
      "Epoch [80/100], Train Loss: 69518.2422, Val Loss: 69365.5391\n",
      "Epoch [90/100], Train Loss: 69518.2031, Val Loss: 69365.5391\n",
      "Epoch [100/100], Train Loss: 69518.2344, Val Loss: 69365.5391\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69543.4766, Val Loss: 69390.9609\n",
      "Epoch [20/100], Train Loss: 69542.0625, Val Loss: 69389.6719\n",
      "Epoch [30/100], Train Loss: 69541.6250, Val Loss: 69389.2812\n",
      "Epoch [40/100], Train Loss: 69541.5000, Val Loss: 69389.1562\n",
      "Epoch [50/100], Train Loss: 69541.4531, Val Loss: 69389.1172\n",
      "Epoch [60/100], Train Loss: 69541.4531, Val Loss: 69389.1094\n",
      "Epoch [70/100], Train Loss: 69541.4531, Val Loss: 69389.1016\n",
      "Epoch [80/100], Train Loss: 69541.4531, Val Loss: 69389.1016\n",
      "Epoch [90/100], Train Loss: 69541.4375, Val Loss: 69389.1016\n",
      "Epoch [100/100], Train Loss: 69541.4219, Val Loss: 69389.1016\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69550.1016, Val Loss: 69397.4609\n",
      "Epoch [20/100], Train Loss: 69548.7734, Val Loss: 69396.2422\n",
      "Epoch [30/100], Train Loss: 69548.4141, Val Loss: 69395.8750\n",
      "Epoch [40/100], Train Loss: 69548.3203, Val Loss: 69395.7578\n",
      "Epoch [50/100], Train Loss: 69548.2031, Val Loss: 69395.7188\n",
      "Epoch [60/100], Train Loss: 69548.2188, Val Loss: 69395.7109\n",
      "Epoch [70/100], Train Loss: 69548.1719, Val Loss: 69395.7031\n",
      "Epoch [80/100], Train Loss: 69548.2109, Val Loss: 69395.7031\n",
      "Epoch [90/100], Train Loss: 69548.2031, Val Loss: 69395.7031\n",
      "Epoch [100/100], Train Loss: 69548.1719, Val Loss: 69395.7031\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 66428.4297, Val Loss: 65718.7891\n",
      "Epoch [20/100], Train Loss: 62351.9961, Val Loss: 61770.6016\n",
      "Epoch [30/100], Train Loss: 58788.7578, Val Loss: 58188.3477\n",
      "Epoch [40/100], Train Loss: 55450.5742, Val Loss: 54841.3828\n",
      "Epoch [50/100], Train Loss: 52374.5508, Val Loss: 51770.0273\n",
      "Epoch [60/100], Train Loss: 49562.7070, Val Loss: 48967.4102\n",
      "Epoch [70/100], Train Loss: 46963.2891, Val Loss: 46379.5078\n",
      "Epoch [80/100], Train Loss: 44541.8711, Val Loss: 43958.4883\n",
      "Epoch [90/100], Train Loss: 42266.0703, Val Loss: 41709.7852\n",
      "Epoch [100/100], Train Loss: 40146.9453, Val Loss: 39632.2188\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 66300.7422, Val Loss: 65602.3438\n",
      "Epoch [20/100], Train Loss: 62226.0820, Val Loss: 61648.5078\n",
      "Epoch [30/100], Train Loss: 58708.7461, Val Loss: 58114.1406\n",
      "Epoch [40/100], Train Loss: 55421.1094, Val Loss: 54815.9297\n",
      "Epoch [50/100], Train Loss: 52396.5156, Val Loss: 51796.6875\n",
      "Epoch [60/100], Train Loss: 49629.7734, Val Loss: 49042.6406\n",
      "Epoch [70/100], Train Loss: 47068.6797, Val Loss: 46492.0430\n",
      "Epoch [80/100], Train Loss: 44674.3711, Val Loss: 44089.6992\n",
      "Epoch [90/100], Train Loss: 42435.8398, Val Loss: 41869.5000\n",
      "Epoch [100/100], Train Loss: 40334.3789, Val Loss: 39803.4062\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 66601.8438, Val Loss: 65874.0391\n",
      "Epoch [20/100], Train Loss: 62422.2930, Val Loss: 61841.5898\n",
      "Epoch [30/100], Train Loss: 58854.8125, Val Loss: 58257.0742\n",
      "Epoch [40/100], Train Loss: 55520.7891, Val Loss: 54910.0664\n",
      "Epoch [50/100], Train Loss: 52461.8672, Val Loss: 51854.1875\n",
      "Epoch [60/100], Train Loss: 49655.4102, Val Loss: 49060.9492\n",
      "Epoch [70/100], Train Loss: 47058.1289, Val Loss: 46472.2656\n",
      "Epoch [80/100], Train Loss: 44644.3672, Val Loss: 44056.3555\n",
      "Epoch [90/100], Train Loss: 42387.3281, Val Loss: 41810.5781\n",
      "Epoch [100/100], Train Loss: 40265.3320, Val Loss: 39731.3906\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69515.5000, Val Loss: 69356.9141\n",
      "Epoch [20/100], Train Loss: 69409.1797, Val Loss: 69235.5000\n",
      "Epoch [30/100], Train Loss: 69014.6406, Val Loss: 68793.2891\n",
      "Epoch [40/100], Train Loss: 68153.5234, Val Loss: 67901.9219\n",
      "Epoch [50/100], Train Loss: 67154.1172, Val Loss: 66894.7266\n",
      "Epoch [60/100], Train Loss: 66230.7500, Val Loss: 65968.6172\n",
      "Epoch [70/100], Train Loss: 65464.3867, Val Loss: 65205.2305\n",
      "Epoch [80/100], Train Loss: 64830.6250, Val Loss: 64577.9062\n",
      "Epoch [90/100], Train Loss: 64285.6523, Val Loss: 64036.0156\n",
      "Epoch [100/100], Train Loss: 63792.9688, Val Loss: 63543.3242\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69518.7969, Val Loss: 69360.1562\n",
      "Epoch [20/100], Train Loss: 69409.7109, Val Loss: 69235.0000\n",
      "Epoch [30/100], Train Loss: 68996.2500, Val Loss: 68770.3672\n",
      "Epoch [40/100], Train Loss: 68093.9922, Val Loss: 67832.4688\n",
      "Epoch [50/100], Train Loss: 67053.8203, Val Loss: 66787.3906\n",
      "Epoch [60/100], Train Loss: 66123.8203, Val Loss: 65859.2969\n",
      "Epoch [70/100], Train Loss: 65369.7969, Val Loss: 65110.3477\n",
      "Epoch [80/100], Train Loss: 64746.5938, Val Loss: 64493.0938\n",
      "Epoch [90/100], Train Loss: 64208.2109, Val Loss: 63957.1797\n",
      "Epoch [100/100], Train Loss: 63719.3984, Val Loss: 63468.5664\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69506.4531, Val Loss: 69348.0234\n",
      "Epoch [20/100], Train Loss: 69402.4688, Val Loss: 69227.7500\n",
      "Epoch [30/100], Train Loss: 68996.2969, Val Loss: 68766.5469\n",
      "Epoch [40/100], Train Loss: 68090.0234, Val Loss: 67821.8594\n",
      "Epoch [50/100], Train Loss: 67062.4062, Val Loss: 66793.5703\n",
      "Epoch [60/100], Train Loss: 66148.3125, Val Loss: 65880.7344\n",
      "Epoch [70/100], Train Loss: 65394.0820, Val Loss: 65130.9375\n",
      "Epoch [80/100], Train Loss: 64767.4375, Val Loss: 64511.2578\n",
      "Epoch [90/100], Train Loss: 64226.6172, Val Loss: 63973.1719\n",
      "Epoch [100/100], Train Loss: 63735.4219, Val Loss: 63483.2891\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69572.3984, Val Loss: 69420.0547\n",
      "Epoch [20/100], Train Loss: 69571.1016, Val Loss: 69418.8828\n",
      "Epoch [30/100], Train Loss: 69570.7188, Val Loss: 69418.5391\n",
      "Epoch [40/100], Train Loss: 69570.5938, Val Loss: 69418.4219\n",
      "Epoch [50/100], Train Loss: 69570.5547, Val Loss: 69418.3828\n",
      "Epoch [60/100], Train Loss: 69570.5469, Val Loss: 69418.3750\n",
      "Epoch [70/100], Train Loss: 69570.5469, Val Loss: 69418.3672\n",
      "Epoch [80/100], Train Loss: 69570.5469, Val Loss: 69418.3672\n",
      "Epoch [90/100], Train Loss: 69570.5391, Val Loss: 69418.3672\n",
      "Epoch [100/100], Train Loss: 69570.5469, Val Loss: 69418.3672\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69559.1641, Val Loss: 69406.7031\n",
      "Epoch [20/100], Train Loss: 69557.8828, Val Loss: 69405.5625\n",
      "Epoch [30/100], Train Loss: 69557.5156, Val Loss: 69405.2188\n",
      "Epoch [40/100], Train Loss: 69557.4062, Val Loss: 69405.1094\n",
      "Epoch [50/100], Train Loss: 69557.3750, Val Loss: 69405.0781\n",
      "Epoch [60/100], Train Loss: 69557.3516, Val Loss: 69405.0625\n",
      "Epoch [70/100], Train Loss: 69557.3516, Val Loss: 69405.0625\n",
      "Epoch [80/100], Train Loss: 69557.3516, Val Loss: 69405.0625\n",
      "Epoch [90/100], Train Loss: 69557.3359, Val Loss: 69405.0547\n",
      "Epoch [100/100], Train Loss: 69557.3516, Val Loss: 69405.0547\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69520.6641, Val Loss: 69367.8672\n",
      "Epoch [20/100], Train Loss: 69519.3906, Val Loss: 69366.7188\n",
      "Epoch [30/100], Train Loss: 69519.0156, Val Loss: 69366.3828\n",
      "Epoch [40/100], Train Loss: 69518.8906, Val Loss: 69366.2734\n",
      "Epoch [50/100], Train Loss: 69518.8594, Val Loss: 69366.2344\n",
      "Epoch [60/100], Train Loss: 69518.8359, Val Loss: 69366.2266\n",
      "Epoch [70/100], Train Loss: 69518.8438, Val Loss: 69366.2266\n",
      "Epoch [80/100], Train Loss: 69518.8438, Val Loss: 69366.2266\n",
      "Epoch [90/100], Train Loss: 69518.8516, Val Loss: 69366.2266\n",
      "Epoch [100/100], Train Loss: 69518.8438, Val Loss: 69366.2266\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 66128.6328, Val Loss: 65475.8164\n",
      "Epoch [20/100], Train Loss: 62338.4922, Val Loss: 61763.8047\n",
      "Epoch [30/100], Train Loss: 58840.9102, Val Loss: 58250.9727\n",
      "Epoch [40/100], Train Loss: 55545.1875, Val Loss: 54940.6328\n",
      "Epoch [50/100], Train Loss: 52502.3047, Val Loss: 51918.8359\n",
      "Epoch [60/100], Train Loss: 49702.7344, Val Loss: 49122.0078\n",
      "Epoch [70/100], Train Loss: 47123.2539, Val Loss: 46548.8945\n",
      "Epoch [80/100], Train Loss: 44754.3086, Val Loss: 44179.1445\n",
      "Epoch [90/100], Train Loss: 42541.3828, Val Loss: 41949.5547\n",
      "Epoch [100/100], Train Loss: 40439.0391, Val Loss: 39909.9883\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 66085.2734, Val Loss: 65427.4414\n",
      "Epoch [20/100], Train Loss: 62255.3047, Val Loss: 61677.2500\n",
      "Epoch [30/100], Train Loss: 58736.6367, Val Loss: 58143.5273\n",
      "Epoch [40/100], Train Loss: 55455.9258, Val Loss: 54851.3242\n",
      "Epoch [50/100], Train Loss: 52440.5781, Val Loss: 51835.3438\n",
      "Epoch [60/100], Train Loss: 49646.2930, Val Loss: 49064.0664\n",
      "Epoch [70/100], Train Loss: 47084.4766, Val Loss: 46514.1758\n",
      "Epoch [80/100], Train Loss: 44706.1445, Val Loss: 44133.2539\n",
      "Epoch [90/100], Train Loss: 42493.1680, Val Loss: 41911.3711\n",
      "Epoch [100/100], Train Loss: 40380.7188, Val Loss: 39840.8594\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 66566.0078, Val Loss: 65881.6562\n",
      "Epoch [20/100], Train Loss: 62683.8398, Val Loss: 62105.8203\n",
      "Epoch [30/100], Train Loss: 59124.9844, Val Loss: 58531.1641\n",
      "Epoch [40/100], Train Loss: 55805.9727, Val Loss: 55204.1875\n",
      "Epoch [50/100], Train Loss: 52750.8398, Val Loss: 52150.8750\n",
      "Epoch [60/100], Train Loss: 49949.3945, Val Loss: 49361.0195\n",
      "Epoch [70/100], Train Loss: 47358.3359, Val Loss: 46778.0117\n",
      "Epoch [80/100], Train Loss: 44968.4961, Val Loss: 44392.5547\n",
      "Epoch [90/100], Train Loss: 42762.6953, Val Loss: 42192.3203\n",
      "Epoch [100/100], Train Loss: 40665.3281, Val Loss: 40080.0234\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69484.5781, Val Loss: 69325.7656\n",
      "Epoch [20/100], Train Loss: 69372.1016, Val Loss: 69194.0312\n",
      "Epoch [30/100], Train Loss: 68801.1172, Val Loss: 68542.5469\n",
      "Epoch [40/100], Train Loss: 67647.2344, Val Loss: 67371.4688\n",
      "Epoch [50/100], Train Loss: 66585.2891, Val Loss: 66314.9688\n",
      "Epoch [60/100], Train Loss: 65791.1328, Val Loss: 65537.3516\n",
      "Epoch [70/100], Train Loss: 65187.2617, Val Loss: 64941.6523\n",
      "Epoch [80/100], Train Loss: 64666.1250, Val Loss: 64421.8398\n",
      "Epoch [90/100], Train Loss: 64185.3828, Val Loss: 63940.0977\n",
      "Epoch [100/100], Train Loss: 63731.0586, Val Loss: 63484.0586\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69551.1875, Val Loss: 69393.2969\n",
      "Epoch [20/100], Train Loss: 69449.4141, Val Loss: 69274.6875\n",
      "Epoch [30/100], Train Loss: 68935.8984, Val Loss: 68677.6641\n",
      "Epoch [40/100], Train Loss: 67776.6484, Val Loss: 67492.5078\n",
      "Epoch [50/100], Train Loss: 66688.4688, Val Loss: 66413.3906\n",
      "Epoch [60/100], Train Loss: 65867.8047, Val Loss: 65611.0156\n",
      "Epoch [70/100], Train Loss: 65250.4805, Val Loss: 65003.6289\n",
      "Epoch [80/100], Train Loss: 64723.1406, Val Loss: 64478.0547\n",
      "Epoch [90/100], Train Loss: 64238.2656, Val Loss: 63992.4219\n",
      "Epoch [100/100], Train Loss: 63780.7812, Val Loss: 63533.4531\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69542.4297, Val Loss: 69384.6328\n",
      "Epoch [20/100], Train Loss: 69442.6953, Val Loss: 69267.4609\n",
      "Epoch [30/100], Train Loss: 68924.3125, Val Loss: 68660.6641\n",
      "Epoch [40/100], Train Loss: 67752.7031, Val Loss: 67462.5156\n",
      "Epoch [50/100], Train Loss: 66695.8984, Val Loss: 66419.7422\n",
      "Epoch [60/100], Train Loss: 65913.3359, Val Loss: 65657.3750\n",
      "Epoch [70/100], Train Loss: 65313.1680, Val Loss: 65066.7227\n",
      "Epoch [80/100], Train Loss: 64794.1289, Val Loss: 64549.3867\n",
      "Epoch [90/100], Train Loss: 64314.5234, Val Loss: 64069.2227\n",
      "Epoch [100/100], Train Loss: 63861.3906, Val Loss: 63614.3242\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69532.1484, Val Loss: 69379.4688\n",
      "Epoch [20/100], Train Loss: 69530.8359, Val Loss: 69378.2812\n",
      "Epoch [30/100], Train Loss: 69530.4609, Val Loss: 69377.9297\n",
      "Epoch [40/100], Train Loss: 69530.3281, Val Loss: 69377.8125\n",
      "Epoch [50/100], Train Loss: 69530.2891, Val Loss: 69377.7812\n",
      "Epoch [60/100], Train Loss: 69530.2734, Val Loss: 69377.7578\n",
      "Epoch [70/100], Train Loss: 69530.2734, Val Loss: 69377.7578\n",
      "Epoch [80/100], Train Loss: 69530.2656, Val Loss: 69377.7578\n",
      "Epoch [90/100], Train Loss: 69530.2656, Val Loss: 69377.7578\n",
      "Epoch [100/100], Train Loss: 69530.2656, Val Loss: 69377.7578\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69551.7812, Val Loss: 69399.2500\n",
      "Epoch [20/100], Train Loss: 69550.4375, Val Loss: 69398.0391\n",
      "Epoch [30/100], Train Loss: 69550.0547, Val Loss: 69397.6797\n",
      "Epoch [40/100], Train Loss: 69549.9219, Val Loss: 69397.5547\n",
      "Epoch [50/100], Train Loss: 69549.8750, Val Loss: 69397.5156\n",
      "Epoch [60/100], Train Loss: 69549.8594, Val Loss: 69397.5078\n",
      "Epoch [70/100], Train Loss: 69549.8516, Val Loss: 69397.5078\n",
      "Epoch [80/100], Train Loss: 69549.8594, Val Loss: 69397.5078\n",
      "Epoch [90/100], Train Loss: 69549.8594, Val Loss: 69397.5078\n",
      "Epoch [100/100], Train Loss: 69549.8594, Val Loss: 69397.5078\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69525.5078, Val Loss: 69372.7891\n",
      "Epoch [20/100], Train Loss: 69524.2812, Val Loss: 69371.6719\n",
      "Epoch [30/100], Train Loss: 69523.9297, Val Loss: 69371.3359\n",
      "Epoch [40/100], Train Loss: 69523.8047, Val Loss: 69371.2344\n",
      "Epoch [50/100], Train Loss: 69523.7656, Val Loss: 69371.2031\n",
      "Epoch [60/100], Train Loss: 69523.7578, Val Loss: 69371.1875\n",
      "Epoch [70/100], Train Loss: 69523.7578, Val Loss: 69371.1875\n",
      "Epoch [80/100], Train Loss: 69523.7578, Val Loss: 69371.1875\n",
      "Epoch [90/100], Train Loss: 69523.7578, Val Loss: 69371.1875\n",
      "Epoch [100/100], Train Loss: 69523.7578, Val Loss: 69371.1875\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 67498.7266, Val Loss: 66926.3047\n",
      "Epoch [20/100], Train Loss: 61989.3242, Val Loss: 61211.7500\n",
      "Epoch [30/100], Train Loss: 55145.2695, Val Loss: 54342.6328\n",
      "Epoch [40/100], Train Loss: 48156.1562, Val Loss: 47366.8438\n",
      "Epoch [50/100], Train Loss: 41540.8828, Val Loss: 40760.1133\n",
      "Epoch [60/100], Train Loss: 35684.5469, Val Loss: 34927.5117\n",
      "Epoch [70/100], Train Loss: 30906.8477, Val Loss: 30211.1758\n",
      "Epoch [80/100], Train Loss: 27256.8613, Val Loss: 26637.5801\n",
      "Epoch [90/100], Train Loss: 24387.5625, Val Loss: 23849.1055\n",
      "Epoch [100/100], Train Loss: 21956.3848, Val Loss: 21494.5742\n",
      "New best score: 21494.57421875 with params: {'hidden_dim': 256, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 67421.0156, Val Loss: 66836.8906\n",
      "Epoch [20/100], Train Loss: 61822.1406, Val Loss: 61043.4219\n",
      "Epoch [30/100], Train Loss: 54981.4570, Val Loss: 54178.2266\n",
      "Epoch [40/100], Train Loss: 47995.5156, Val Loss: 47203.9219\n",
      "Epoch [50/100], Train Loss: 41382.8945, Val Loss: 40604.4922\n",
      "Epoch [60/100], Train Loss: 35554.2539, Val Loss: 34797.9023\n",
      "Epoch [70/100], Train Loss: 30816.9199, Val Loss: 30120.7363\n",
      "Epoch [80/100], Train Loss: 27193.0195, Val Loss: 26574.5820\n",
      "Epoch [90/100], Train Loss: 24339.3984, Val Loss: 23801.5859\n",
      "Epoch [100/100], Train Loss: 21923.6055, Val Loss: 21462.6367\n",
      "New best score: 21462.63671875 with params: {'hidden_dim': 256, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67475.0391, Val Loss: 66897.2656\n",
      "Epoch [20/100], Train Loss: 61909.9922, Val Loss: 61124.9531\n",
      "Epoch [30/100], Train Loss: 55026.5117, Val Loss: 54218.1367\n",
      "Epoch [40/100], Train Loss: 48027.1523, Val Loss: 47233.2891\n",
      "Epoch [50/100], Train Loss: 41430.2969, Val Loss: 40652.0508\n",
      "Epoch [60/100], Train Loss: 35597.7773, Val Loss: 34843.6719\n",
      "Epoch [70/100], Train Loss: 30828.9766, Val Loss: 30133.8848\n",
      "Epoch [80/100], Train Loss: 27185.0957, Val Loss: 26566.3496\n",
      "Epoch [90/100], Train Loss: 24320.1523, Val Loss: 23783.2559\n",
      "Epoch [100/100], Train Loss: 21897.4883, Val Loss: 21436.3047\n",
      "New best score: 21436.3046875 with params: {'hidden_dim': 256, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.3}\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69444.4766, Val Loss: 69281.5547\n",
      "Epoch [20/100], Train Loss: 69308.7422, Val Loss: 69139.6562\n",
      "Epoch [30/100], Train Loss: 69092.7812, Val Loss: 68914.2578\n",
      "Epoch [40/100], Train Loss: 68768.1094, Val Loss: 68578.0703\n",
      "Epoch [50/100], Train Loss: 68319.2031, Val Loss: 68117.4375\n",
      "Epoch [60/100], Train Loss: 67747.2031, Val Loss: 67535.5078\n",
      "Epoch [70/100], Train Loss: 67067.9688, Val Loss: 66849.5859\n",
      "Epoch [80/100], Train Loss: 66305.3672, Val Loss: 66084.0781\n",
      "Epoch [90/100], Train Loss: 65484.1172, Val Loss: 65263.4414\n",
      "Epoch [100/100], Train Loss: 64625.1641, Val Loss: 64407.9219\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69449.1875, Val Loss: 69285.3984\n",
      "Epoch [20/100], Train Loss: 69310.6406, Val Loss: 69140.3047\n",
      "Epoch [30/100], Train Loss: 69089.6953, Val Loss: 68909.5156\n",
      "Epoch [40/100], Train Loss: 68758.1328, Val Loss: 68566.1172\n",
      "Epoch [50/100], Train Loss: 68301.1641, Val Loss: 68097.3359\n",
      "Epoch [60/100], Train Loss: 67721.3125, Val Loss: 67507.9375\n",
      "Epoch [70/100], Train Loss: 67035.7109, Val Loss: 66816.5078\n",
      "Epoch [80/100], Train Loss: 66268.8203, Val Loss: 66047.8359\n",
      "Epoch [90/100], Train Loss: 65445.2656, Val Loss: 65226.0273\n",
      "Epoch [100/100], Train Loss: 64585.5586, Val Loss: 64370.7148\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69468.7344, Val Loss: 69304.5547\n",
      "Epoch [20/100], Train Loss: 69328.2188, Val Loss: 69157.4609\n",
      "Epoch [30/100], Train Loss: 69104.4609, Val Loss: 68923.8594\n",
      "Epoch [40/100], Train Loss: 68770.0859, Val Loss: 68577.6484\n",
      "Epoch [50/100], Train Loss: 68311.1250, Val Loss: 68106.9062\n",
      "Epoch [60/100], Train Loss: 67730.5312, Val Loss: 67516.6484\n",
      "Epoch [70/100], Train Loss: 67045.2188, Val Loss: 66825.2266\n",
      "Epoch [80/100], Train Loss: 66279.1953, Val Loss: 66057.0469\n",
      "Epoch [90/100], Train Loss: 65456.8828, Val Loss: 65236.1641\n",
      "Epoch [100/100], Train Loss: 64598.8555, Val Loss: 64382.3555\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69534.1641, Val Loss: 69381.2031\n",
      "Epoch [20/100], Train Loss: 69530.1406, Val Loss: 69377.3828\n",
      "Epoch [30/100], Train Loss: 69528.5391, Val Loss: 69375.8828\n",
      "Epoch [40/100], Train Loss: 69528.0391, Val Loss: 69375.4688\n",
      "Epoch [50/100], Train Loss: 69527.8750, Val Loss: 69375.3203\n",
      "Epoch [60/100], Train Loss: 69527.8203, Val Loss: 69375.2656\n",
      "Epoch [70/100], Train Loss: 69527.8047, Val Loss: 69375.2578\n",
      "Epoch [80/100], Train Loss: 69527.8047, Val Loss: 69375.2500\n",
      "Epoch [90/100], Train Loss: 69527.8047, Val Loss: 69375.2500\n",
      "Epoch [100/100], Train Loss: 69527.8047, Val Loss: 69375.2500\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69544.8125, Val Loss: 69392.3828\n",
      "Epoch [20/100], Train Loss: 69540.6875, Val Loss: 69388.5000\n",
      "Epoch [30/100], Train Loss: 69539.0703, Val Loss: 69387.0547\n",
      "Epoch [40/100], Train Loss: 69538.6094, Val Loss: 69386.6172\n",
      "Epoch [50/100], Train Loss: 69538.4531, Val Loss: 69386.4922\n",
      "Epoch [60/100], Train Loss: 69538.3984, Val Loss: 69386.4453\n",
      "Epoch [70/100], Train Loss: 69538.3906, Val Loss: 69386.4297\n",
      "Epoch [80/100], Train Loss: 69538.3750, Val Loss: 69386.4297\n",
      "Epoch [90/100], Train Loss: 69538.3750, Val Loss: 69386.4297\n",
      "Epoch [100/100], Train Loss: 69538.3750, Val Loss: 69386.4297\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69522.2891, Val Loss: 69369.3047\n",
      "Epoch [20/100], Train Loss: 69518.2344, Val Loss: 69365.4766\n",
      "Epoch [30/100], Train Loss: 69516.6562, Val Loss: 69364.0703\n",
      "Epoch [40/100], Train Loss: 69516.2031, Val Loss: 69363.6406\n",
      "Epoch [50/100], Train Loss: 69516.0469, Val Loss: 69363.5156\n",
      "Epoch [60/100], Train Loss: 69516.0000, Val Loss: 69363.4688\n",
      "Epoch [70/100], Train Loss: 69515.9766, Val Loss: 69363.4531\n",
      "Epoch [80/100], Train Loss: 69515.9766, Val Loss: 69363.4453\n",
      "Epoch [90/100], Train Loss: 69515.9766, Val Loss: 69363.4453\n",
      "Epoch [100/100], Train Loss: 69515.9766, Val Loss: 69363.4453\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 64371.7188, Val Loss: 63266.9688\n",
      "Epoch [20/100], Train Loss: 56167.4453, Val Loss: 55215.3047\n",
      "Epoch [30/100], Train Loss: 49870.1602, Val Loss: 48934.3750\n",
      "Epoch [40/100], Train Loss: 44394.4414, Val Loss: 43542.9961\n",
      "Epoch [50/100], Train Loss: 39805.6172, Val Loss: 39003.9141\n",
      "Epoch [60/100], Train Loss: 35850.8281, Val Loss: 35098.1328\n",
      "Epoch [70/100], Train Loss: 32304.7773, Val Loss: 31673.8672\n",
      "Epoch [80/100], Train Loss: 29265.6074, Val Loss: 28745.2734\n",
      "Epoch [90/100], Train Loss: 26648.0879, Val Loss: 26212.1367\n",
      "Epoch [100/100], Train Loss: 24373.9121, Val Loss: 23988.6855\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 64401.2344, Val Loss: 63307.2578\n",
      "Epoch [20/100], Train Loss: 56288.2227, Val Loss: 55336.5039\n",
      "Epoch [30/100], Train Loss: 49978.7070, Val Loss: 49041.9375\n",
      "Epoch [40/100], Train Loss: 44498.3984, Val Loss: 43643.7070\n",
      "Epoch [50/100], Train Loss: 39907.9609, Val Loss: 39104.2500\n",
      "Epoch [60/100], Train Loss: 35965.2031, Val Loss: 35205.5859\n",
      "Epoch [70/100], Train Loss: 32402.0840, Val Loss: 31765.1777\n",
      "Epoch [80/100], Train Loss: 29358.6562, Val Loss: 28826.4375\n",
      "Epoch [90/100], Train Loss: 26729.6133, Val Loss: 26274.9258\n",
      "Epoch [100/100], Train Loss: 24439.7949, Val Loss: 24047.2812\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 64527.9336, Val Loss: 63434.8008\n",
      "Epoch [20/100], Train Loss: 56328.2852, Val Loss: 55371.7812\n",
      "Epoch [30/100], Train Loss: 50017.6211, Val Loss: 49081.7109\n",
      "Epoch [40/100], Train Loss: 44532.2305, Val Loss: 43675.9297\n",
      "Epoch [50/100], Train Loss: 39934.3242, Val Loss: 39124.3047\n",
      "Epoch [60/100], Train Loss: 35981.7148, Val Loss: 35218.2812\n",
      "Epoch [70/100], Train Loss: 32447.0508, Val Loss: 31804.1934\n",
      "Epoch [80/100], Train Loss: 29409.5703, Val Loss: 28868.4277\n",
      "Epoch [90/100], Train Loss: 26801.1543, Val Loss: 26336.1504\n",
      "Epoch [100/100], Train Loss: 24522.9844, Val Loss: 24099.9648\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69458.5547, Val Loss: 69288.7891\n",
      "Epoch [20/100], Train Loss: 69139.4297, Val Loss: 68926.5703\n",
      "Epoch [30/100], Train Loss: 68258.9375, Val Loss: 67984.5469\n",
      "Epoch [40/100], Train Loss: 66864.9844, Val Loss: 66568.1719\n",
      "Epoch [50/100], Train Loss: 65355.2539, Val Loss: 65061.6445\n",
      "Epoch [60/100], Train Loss: 63919.5703, Val Loss: 63617.1523\n",
      "Epoch [70/100], Train Loss: 62549.4648, Val Loss: 62225.6523\n",
      "Epoch [80/100], Train Loss: 61268.3594, Val Loss: 60933.1992\n",
      "Epoch [90/100], Train Loss: 60112.3242, Val Loss: 59770.6836\n",
      "Epoch [100/100], Train Loss: 59074.9570, Val Loss: 58734.7188\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69457.0547, Val Loss: 69287.3906\n",
      "Epoch [20/100], Train Loss: 69138.6250, Val Loss: 68926.0781\n",
      "Epoch [30/100], Train Loss: 68253.6328, Val Loss: 67979.6953\n",
      "Epoch [40/100], Train Loss: 66848.8594, Val Loss: 66549.2969\n",
      "Epoch [50/100], Train Loss: 65327.0234, Val Loss: 65029.9414\n",
      "Epoch [60/100], Train Loss: 63888.2383, Val Loss: 63583.7969\n",
      "Epoch [70/100], Train Loss: 62518.0000, Val Loss: 62195.4766\n",
      "Epoch [80/100], Train Loss: 61237.1602, Val Loss: 60900.5156\n",
      "Epoch [90/100], Train Loss: 60078.7930, Val Loss: 59735.7227\n",
      "Epoch [100/100], Train Loss: 59039.6016, Val Loss: 58696.7930\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69489.7578, Val Loss: 69320.9609\n",
      "Epoch [20/100], Train Loss: 69178.9844, Val Loss: 68968.8359\n",
      "Epoch [30/100], Train Loss: 68308.6484, Val Loss: 68034.4922\n",
      "Epoch [40/100], Train Loss: 66893.1172, Val Loss: 66593.0156\n",
      "Epoch [50/100], Train Loss: 65344.3320, Val Loss: 65048.9922\n",
      "Epoch [60/100], Train Loss: 63890.8555, Val Loss: 63590.0391\n",
      "Epoch [70/100], Train Loss: 62524.8125, Val Loss: 62197.8750\n",
      "Epoch [80/100], Train Loss: 61244.9375, Val Loss: 60903.4375\n",
      "Epoch [90/100], Train Loss: 60090.4648, Val Loss: 59743.2812\n",
      "Epoch [100/100], Train Loss: 59057.5273, Val Loss: 58709.7031\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69554.5625, Val Loss: 69401.9688\n",
      "Epoch [20/100], Train Loss: 69552.0000, Val Loss: 69399.6016\n",
      "Epoch [30/100], Train Loss: 69550.7031, Val Loss: 69398.4141\n",
      "Epoch [40/100], Train Loss: 69550.3125, Val Loss: 69398.0547\n",
      "Epoch [50/100], Train Loss: 69550.2031, Val Loss: 69397.9453\n",
      "Epoch [60/100], Train Loss: 69550.1250, Val Loss: 69397.9141\n",
      "Epoch [70/100], Train Loss: 69550.1328, Val Loss: 69397.8984\n",
      "Epoch [80/100], Train Loss: 69550.1250, Val Loss: 69397.8984\n",
      "Epoch [90/100], Train Loss: 69550.1172, Val Loss: 69397.8828\n",
      "Epoch [100/100], Train Loss: 69550.1406, Val Loss: 69397.8828\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69539.5156, Val Loss: 69386.7031\n",
      "Epoch [20/100], Train Loss: 69537.2500, Val Loss: 69384.6328\n",
      "Epoch [30/100], Train Loss: 69536.5312, Val Loss: 69383.9531\n",
      "Epoch [40/100], Train Loss: 69536.2812, Val Loss: 69383.7188\n",
      "Epoch [50/100], Train Loss: 69536.1953, Val Loss: 69383.6328\n",
      "Epoch [60/100], Train Loss: 69536.1719, Val Loss: 69383.6172\n",
      "Epoch [70/100], Train Loss: 69536.1406, Val Loss: 69383.6094\n",
      "Epoch [80/100], Train Loss: 69536.1797, Val Loss: 69383.6094\n",
      "Epoch [90/100], Train Loss: 69536.1328, Val Loss: 69383.6094\n",
      "Epoch [100/100], Train Loss: 69536.1484, Val Loss: 69383.6094\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69564.8281, Val Loss: 69412.2344\n",
      "Epoch [20/100], Train Loss: 69562.4297, Val Loss: 69410.0234\n",
      "Epoch [30/100], Train Loss: 69561.0625, Val Loss: 69408.7266\n",
      "Epoch [40/100], Train Loss: 69560.5625, Val Loss: 69408.3125\n",
      "Epoch [50/100], Train Loss: 69560.4141, Val Loss: 69408.1719\n",
      "Epoch [60/100], Train Loss: 69560.3672, Val Loss: 69408.1250\n",
      "Epoch [70/100], Train Loss: 69560.3672, Val Loss: 69408.1172\n",
      "Epoch [80/100], Train Loss: 69560.3906, Val Loss: 69408.1172\n",
      "Epoch [90/100], Train Loss: 69560.3672, Val Loss: 69408.1172\n",
      "Epoch [100/100], Train Loss: 69560.3516, Val Loss: 69408.1172\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 63328.0703, Val Loss: 62347.2773\n",
      "Epoch [20/100], Train Loss: 56593.3281, Val Loss: 55681.7305\n",
      "Epoch [30/100], Train Loss: 50600.8789, Val Loss: 49700.5508\n",
      "Epoch [40/100], Train Loss: 45352.0430, Val Loss: 44492.9609\n",
      "Epoch [50/100], Train Loss: 40821.3477, Val Loss: 40039.1836\n",
      "Epoch [60/100], Train Loss: 36979.6484, Val Loss: 36240.9102\n",
      "Epoch [70/100], Train Loss: 33478.2070, Val Loss: 32785.9609\n",
      "Epoch [80/100], Train Loss: 30366.8945, Val Loss: 29842.1055\n",
      "Epoch [90/100], Train Loss: 27704.6484, Val Loss: 27244.4512\n",
      "Epoch [100/100], Train Loss: 25383.7227, Val Loss: 24989.2988\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 63123.8984, Val Loss: 62158.4570\n",
      "Epoch [20/100], Train Loss: 56517.7422, Val Loss: 55611.4336\n",
      "Epoch [30/100], Train Loss: 50575.2734, Val Loss: 49670.5820\n",
      "Epoch [40/100], Train Loss: 45340.1719, Val Loss: 44487.8516\n",
      "Epoch [50/100], Train Loss: 40812.9688, Val Loss: 40027.5508\n",
      "Epoch [60/100], Train Loss: 36919.5820, Val Loss: 36144.5156\n",
      "Epoch [70/100], Train Loss: 33401.2891, Val Loss: 32754.9629\n",
      "Epoch [80/100], Train Loss: 30339.7656, Val Loss: 29793.3926\n",
      "Epoch [90/100], Train Loss: 27706.4551, Val Loss: 27240.7676\n",
      "Epoch [100/100], Train Loss: 25397.2949, Val Loss: 24992.2051\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 63284.2383, Val Loss: 62316.1953\n",
      "Epoch [20/100], Train Loss: 56591.1523, Val Loss: 55679.3516\n",
      "Epoch [30/100], Train Loss: 50602.2031, Val Loss: 49693.3008\n",
      "Epoch [40/100], Train Loss: 45360.7812, Val Loss: 44498.3750\n",
      "Epoch [50/100], Train Loss: 40822.2930, Val Loss: 40034.5625\n",
      "Epoch [60/100], Train Loss: 36935.7930, Val Loss: 36168.3867\n",
      "Epoch [70/100], Train Loss: 33428.1602, Val Loss: 32757.4453\n",
      "Epoch [80/100], Train Loss: 30340.3164, Val Loss: 29774.4609\n",
      "Epoch [90/100], Train Loss: 27697.9395, Val Loss: 27205.2852\n",
      "Epoch [100/100], Train Loss: 25387.0059, Val Loss: 24963.4238\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69488.1328, Val Loss: 69316.4531\n",
      "Epoch [20/100], Train Loss: 68954.4219, Val Loss: 68676.5547\n",
      "Epoch [30/100], Train Loss: 67243.1797, Val Loss: 66898.3516\n",
      "Epoch [40/100], Train Loss: 65454.9844, Val Loss: 65126.0078\n",
      "Epoch [50/100], Train Loss: 64010.6992, Val Loss: 63680.8281\n",
      "Epoch [60/100], Train Loss: 62833.1484, Val Loss: 62509.8008\n",
      "Epoch [70/100], Train Loss: 61830.5156, Val Loss: 61516.7031\n",
      "Epoch [80/100], Train Loss: 60943.4336, Val Loss: 60630.7812\n",
      "Epoch [90/100], Train Loss: 60120.9062, Val Loss: 59805.4570\n",
      "Epoch [100/100], Train Loss: 59338.9688, Val Loss: 59019.3047\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69485.7031, Val Loss: 69316.2344\n",
      "Epoch [20/100], Train Loss: 69001.6797, Val Loss: 68738.6953\n",
      "Epoch [30/100], Train Loss: 67418.1172, Val Loss: 67082.3359\n",
      "Epoch [40/100], Train Loss: 65683.3047, Val Loss: 65351.5938\n",
      "Epoch [50/100], Train Loss: 64244.2891, Val Loss: 63914.1719\n",
      "Epoch [60/100], Train Loss: 63066.5859, Val Loss: 62745.2812\n",
      "Epoch [70/100], Train Loss: 62061.7188, Val Loss: 61748.6445\n",
      "Epoch [80/100], Train Loss: 61168.2891, Val Loss: 60856.4219\n",
      "Epoch [90/100], Train Loss: 60338.5352, Val Loss: 60024.0508\n",
      "Epoch [100/100], Train Loss: 59549.9805, Val Loss: 59231.4141\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69458.5547, Val Loss: 69289.0938\n",
      "Epoch [20/100], Train Loss: 68994.5234, Val Loss: 68732.0391\n",
      "Epoch [30/100], Train Loss: 67379.1484, Val Loss: 67029.9688\n",
      "Epoch [40/100], Train Loss: 65552.5703, Val Loss: 65214.6133\n",
      "Epoch [50/100], Train Loss: 64089.7617, Val Loss: 63756.9219\n",
      "Epoch [60/100], Train Loss: 62907.2969, Val Loss: 62584.4297\n",
      "Epoch [70/100], Train Loss: 61906.2109, Val Loss: 61591.2305\n",
      "Epoch [80/100], Train Loss: 61015.2773, Val Loss: 60701.2148\n",
      "Epoch [90/100], Train Loss: 60182.1211, Val Loss: 59865.1992\n",
      "Epoch [100/100], Train Loss: 59391.5469, Val Loss: 59070.6289\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69531.0859, Val Loss: 69378.2969\n",
      "Epoch [20/100], Train Loss: 69529.1016, Val Loss: 69376.5156\n",
      "Epoch [30/100], Train Loss: 69528.5156, Val Loss: 69375.9609\n",
      "Epoch [40/100], Train Loss: 69528.3203, Val Loss: 69375.7812\n",
      "Epoch [50/100], Train Loss: 69528.2578, Val Loss: 69375.7266\n",
      "Epoch [60/100], Train Loss: 69528.2344, Val Loss: 69375.7109\n",
      "Epoch [70/100], Train Loss: 69528.2188, Val Loss: 69375.7109\n",
      "Epoch [80/100], Train Loss: 69528.2344, Val Loss: 69375.7109\n",
      "Epoch [90/100], Train Loss: 69528.2266, Val Loss: 69375.7109\n",
      "Epoch [100/100], Train Loss: 69528.2266, Val Loss: 69375.7109\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69550.7578, Val Loss: 69398.1094\n",
      "Epoch [20/100], Train Loss: 69548.6172, Val Loss: 69396.1797\n",
      "Epoch [30/100], Train Loss: 69548.0000, Val Loss: 69395.5938\n",
      "Epoch [40/100], Train Loss: 69547.6875, Val Loss: 69395.2969\n",
      "Epoch [50/100], Train Loss: 69547.5859, Val Loss: 69395.2031\n",
      "Epoch [60/100], Train Loss: 69547.5469, Val Loss: 69395.1797\n",
      "Epoch [70/100], Train Loss: 69547.5234, Val Loss: 69395.1562\n",
      "Epoch [80/100], Train Loss: 69547.5391, Val Loss: 69395.1562\n",
      "Epoch [90/100], Train Loss: 69547.5391, Val Loss: 69395.1562\n",
      "Epoch [100/100], Train Loss: 69547.5234, Val Loss: 69395.1562\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69544.6875, Val Loss: 69391.9844\n",
      "Epoch [20/100], Train Loss: 69542.6172, Val Loss: 69390.1094\n",
      "Epoch [30/100], Train Loss: 69542.0156, Val Loss: 69389.5391\n",
      "Epoch [40/100], Train Loss: 69541.7969, Val Loss: 69389.3359\n",
      "Epoch [50/100], Train Loss: 69541.6797, Val Loss: 69389.2422\n",
      "Epoch [60/100], Train Loss: 69541.6562, Val Loss: 69389.2188\n",
      "Epoch [70/100], Train Loss: 69541.6328, Val Loss: 69389.1953\n",
      "Epoch [80/100], Train Loss: 69541.6328, Val Loss: 69389.1953\n",
      "Epoch [90/100], Train Loss: 69541.6250, Val Loss: 69389.1953\n",
      "Epoch [100/100], Train Loss: 69541.6406, Val Loss: 69389.1953\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 63133.7148, Val Loss: 62274.0195\n",
      "Epoch [20/100], Train Loss: 56816.0156, Val Loss: 55918.9805\n",
      "Epoch [30/100], Train Loss: 51000.3125, Val Loss: 50114.1562\n",
      "Epoch [40/100], Train Loss: 45909.1602, Val Loss: 45060.5703\n",
      "Epoch [50/100], Train Loss: 41511.1836, Val Loss: 40726.0859\n",
      "Epoch [60/100], Train Loss: 37644.1289, Val Loss: 36893.9531\n",
      "Epoch [70/100], Train Loss: 34177.5781, Val Loss: 33466.3398\n",
      "Epoch [80/100], Train Loss: 31052.6816, Val Loss: 30492.0684\n",
      "Epoch [90/100], Train Loss: 28357.9727, Val Loss: 27851.4375\n",
      "Epoch [100/100], Train Loss: 25998.8320, Val Loss: 25593.8965\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 63109.6172, Val Loss: 62245.5742\n",
      "Epoch [20/100], Train Loss: 56780.3438, Val Loss: 55892.6602\n",
      "Epoch [30/100], Train Loss: 50999.5195, Val Loss: 50115.2227\n",
      "Epoch [40/100], Train Loss: 45962.1758, Val Loss: 45087.7109\n",
      "Epoch [50/100], Train Loss: 41499.4023, Val Loss: 40699.1055\n",
      "Epoch [60/100], Train Loss: 37604.2344, Val Loss: 36851.8047\n",
      "Epoch [70/100], Train Loss: 34122.3359, Val Loss: 33464.6445\n",
      "Epoch [80/100], Train Loss: 31081.9434, Val Loss: 30516.3125\n",
      "Epoch [90/100], Train Loss: 28422.8477, Val Loss: 27930.0195\n",
      "Epoch [100/100], Train Loss: 26096.8320, Val Loss: 25666.1055\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 63160.7969, Val Loss: 62287.1250\n",
      "Epoch [20/100], Train Loss: 56727.7891, Val Loss: 55829.3984\n",
      "Epoch [30/100], Train Loss: 50858.2070, Val Loss: 49968.5352\n",
      "Epoch [40/100], Train Loss: 45883.3164, Val Loss: 45002.4609\n",
      "Epoch [50/100], Train Loss: 41417.8867, Val Loss: 40611.9883\n",
      "Epoch [60/100], Train Loss: 37540.6250, Val Loss: 36786.4492\n",
      "Epoch [70/100], Train Loss: 34102.9688, Val Loss: 33396.4805\n",
      "Epoch [80/100], Train Loss: 30954.2539, Val Loss: 30365.6348\n",
      "Epoch [90/100], Train Loss: 28257.7520, Val Loss: 27757.8145\n",
      "Epoch [100/100], Train Loss: 25910.7871, Val Loss: 25467.4414\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69468.6094, Val Loss: 69297.2422\n",
      "Epoch [20/100], Train Loss: 68744.7734, Val Loss: 68396.0781\n",
      "Epoch [30/100], Train Loss: 66555.1406, Val Loss: 66194.8516\n",
      "Epoch [40/100], Train Loss: 64927.6094, Val Loss: 64606.2266\n",
      "Epoch [50/100], Train Loss: 63799.1680, Val Loss: 63495.1680\n",
      "Epoch [60/100], Train Loss: 62851.3672, Val Loss: 62551.1055\n",
      "Epoch [70/100], Train Loss: 61981.6016, Val Loss: 61679.5703\n",
      "Epoch [80/100], Train Loss: 61160.6094, Val Loss: 60854.7422\n",
      "Epoch [90/100], Train Loss: 60378.3281, Val Loss: 60068.2617\n",
      "Epoch [100/100], Train Loss: 59628.3438, Val Loss: 59314.6875\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69468.8750, Val Loss: 69297.8203\n",
      "Epoch [20/100], Train Loss: 68770.7188, Val Loss: 68431.3359\n",
      "Epoch [30/100], Train Loss: 66658.9609, Val Loss: 66301.6875\n",
      "Epoch [40/100], Train Loss: 65024.2188, Val Loss: 64700.2500\n",
      "Epoch [50/100], Train Loss: 63873.2422, Val Loss: 63567.1211\n",
      "Epoch [60/100], Train Loss: 62909.0469, Val Loss: 62608.0234\n",
      "Epoch [70/100], Train Loss: 62030.2148, Val Loss: 61727.4297\n",
      "Epoch [80/100], Train Loss: 61200.7148, Val Loss: 60894.1133\n",
      "Epoch [90/100], Train Loss: 60411.1484, Val Loss: 60100.2070\n",
      "Epoch [100/100], Train Loss: 59654.7969, Val Loss: 59340.3633\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69450.2734, Val Loss: 69276.9531\n",
      "Epoch [20/100], Train Loss: 68673.7500, Val Loss: 68319.7031\n",
      "Epoch [30/100], Train Loss: 66513.7500, Val Loss: 66147.5391\n",
      "Epoch [40/100], Train Loss: 64877.6602, Val Loss: 64552.2383\n",
      "Epoch [50/100], Train Loss: 63738.2617, Val Loss: 63433.0078\n",
      "Epoch [60/100], Train Loss: 62790.3633, Val Loss: 62489.3242\n",
      "Epoch [70/100], Train Loss: 61921.9180, Val Loss: 61619.2109\n",
      "Epoch [80/100], Train Loss: 61101.6953, Val Loss: 60795.0078\n",
      "Epoch [90/100], Train Loss: 60318.9062, Val Loss: 60007.6562\n",
      "Epoch [100/100], Train Loss: 59568.5742, Val Loss: 59253.2578\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69544.9688, Val Loss: 69392.2812\n",
      "Epoch [20/100], Train Loss: 69542.9453, Val Loss: 69390.4609\n",
      "Epoch [30/100], Train Loss: 69542.3516, Val Loss: 69389.9062\n",
      "Epoch [40/100], Train Loss: 69542.1562, Val Loss: 69389.7266\n",
      "Epoch [50/100], Train Loss: 69542.0938, Val Loss: 69389.6719\n",
      "Epoch [60/100], Train Loss: 69542.0703, Val Loss: 69389.6562\n",
      "Epoch [70/100], Train Loss: 69542.0625, Val Loss: 69389.6484\n",
      "Epoch [80/100], Train Loss: 69542.0625, Val Loss: 69389.6484\n",
      "Epoch [90/100], Train Loss: 69542.0625, Val Loss: 69389.6484\n",
      "Epoch [100/100], Train Loss: 69542.0625, Val Loss: 69389.6484\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69529.7734, Val Loss: 69376.9609\n",
      "Epoch [20/100], Train Loss: 69527.7969, Val Loss: 69375.1953\n",
      "Epoch [30/100], Train Loss: 69527.2266, Val Loss: 69374.6406\n",
      "Epoch [40/100], Train Loss: 69527.0156, Val Loss: 69374.4688\n",
      "Epoch [50/100], Train Loss: 69526.9688, Val Loss: 69374.4141\n",
      "Epoch [60/100], Train Loss: 69526.9453, Val Loss: 69374.3984\n",
      "Epoch [70/100], Train Loss: 69526.9375, Val Loss: 69374.3906\n",
      "Epoch [80/100], Train Loss: 69526.9297, Val Loss: 69374.3906\n",
      "Epoch [90/100], Train Loss: 69526.9375, Val Loss: 69374.3906\n",
      "Epoch [100/100], Train Loss: 69526.9375, Val Loss: 69374.3906\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69564.9062, Val Loss: 69412.3750\n",
      "Epoch [20/100], Train Loss: 69562.7109, Val Loss: 69410.3750\n",
      "Epoch [30/100], Train Loss: 69561.8984, Val Loss: 69409.5625\n",
      "Epoch [40/100], Train Loss: 69561.4531, Val Loss: 69409.1797\n",
      "Epoch [50/100], Train Loss: 69561.3125, Val Loss: 69409.0547\n",
      "Epoch [60/100], Train Loss: 69561.2578, Val Loss: 69409.0156\n",
      "Epoch [70/100], Train Loss: 69561.2578, Val Loss: 69409.0000\n",
      "Epoch [80/100], Train Loss: 69561.2422, Val Loss: 69408.9922\n",
      "Epoch [90/100], Train Loss: 69561.2500, Val Loss: 69408.9922\n",
      "Epoch [100/100], Train Loss: 69561.2500, Val Loss: 69408.9922\n",
      "Best Parameters for LSTM: {'hidden_dim': 256, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.3}\n",
      "Tuning GRU Model\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69033.4922, Val Loss: 68799.0078\n",
      "Epoch [20/100], Train Loss: 68013.3359, Val Loss: 67744.3594\n",
      "Epoch [30/100], Train Loss: 66694.3359, Val Loss: 66405.1797\n",
      "Epoch [40/100], Train Loss: 65200.9336, Val Loss: 64895.1523\n",
      "Epoch [50/100], Train Loss: 63613.8867, Val Loss: 63292.6523\n",
      "Epoch [60/100], Train Loss: 62007.4258, Val Loss: 61668.8281\n",
      "Epoch [70/100], Train Loss: 60438.3633, Val Loss: 60084.1133\n",
      "Epoch [80/100], Train Loss: 58949.7031, Val Loss: 58584.1914\n",
      "Epoch [90/100], Train Loss: 57560.8945, Val Loss: 57187.8203\n",
      "Epoch [100/100], Train Loss: 56262.5195, Val Loss: 55883.0391\n",
      "New best score: 55883.0390625 with params: {'hidden_dim': 32, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68937.7734, Val Loss: 68694.5625\n",
      "Epoch [20/100], Train Loss: 67805.4219, Val Loss: 67534.9531\n",
      "Epoch [30/100], Train Loss: 66436.6719, Val Loss: 66149.4688\n",
      "Epoch [40/100], Train Loss: 64949.3594, Val Loss: 64647.9844\n",
      "Epoch [50/100], Train Loss: 63395.0391, Val Loss: 63075.7969\n",
      "Epoch [60/100], Train Loss: 61822.9297, Val Loss: 61482.8867\n",
      "Epoch [70/100], Train Loss: 60280.9570, Val Loss: 59923.8164\n",
      "Epoch [80/100], Train Loss: 58805.2578, Val Loss: 58435.8672\n",
      "Epoch [90/100], Train Loss: 57418.3398, Val Loss: 57039.9883\n",
      "Epoch [100/100], Train Loss: 56118.4805, Val Loss: 55733.4961\n",
      "New best score: 55733.49609375 with params: {'hidden_dim': 32, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68984.9766, Val Loss: 68744.3516\n",
      "Epoch [20/100], Train Loss: 67893.4844, Val Loss: 67623.3906\n",
      "Epoch [30/100], Train Loss: 66544.6172, Val Loss: 66257.1875\n",
      "Epoch [40/100], Train Loss: 65058.1289, Val Loss: 64751.9414\n",
      "Epoch [50/100], Train Loss: 63492.0781, Val Loss: 63166.8945\n",
      "Epoch [60/100], Train Loss: 61905.7539, Val Loss: 61561.7695\n",
      "Epoch [70/100], Train Loss: 60356.3750, Val Loss: 59997.0938\n",
      "Epoch [80/100], Train Loss: 58882.4570, Val Loss: 58513.1055\n",
      "Epoch [90/100], Train Loss: 57502.3359, Val Loss: 57126.0312\n",
      "Epoch [100/100], Train Loss: 56213.0391, Val Loss: 55831.3633\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69522.3828, Val Loss: 69363.5469\n",
      "Epoch [20/100], Train Loss: 69458.0938, Val Loss: 69298.5000\n",
      "Epoch [30/100], Train Loss: 69385.3750, Val Loss: 69224.5859\n",
      "Epoch [40/100], Train Loss: 69300.1562, Val Loss: 69137.9844\n",
      "Epoch [50/100], Train Loss: 69200.0781, Val Loss: 69036.5547\n",
      "Epoch [60/100], Train Loss: 69084.5156, Val Loss: 68919.7969\n",
      "Epoch [70/100], Train Loss: 68954.2031, Val Loss: 68788.5703\n",
      "Epoch [80/100], Train Loss: 68810.7656, Val Loss: 68644.5547\n",
      "Epoch [90/100], Train Loss: 68656.3047, Val Loss: 68489.9219\n",
      "Epoch [100/100], Train Loss: 68493.0625, Val Loss: 68326.9141\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69472.5312, Val Loss: 69316.7031\n",
      "Epoch [20/100], Train Loss: 69416.0625, Val Loss: 69259.8047\n",
      "Epoch [30/100], Train Loss: 69352.7266, Val Loss: 69195.5938\n",
      "Epoch [40/100], Train Loss: 69278.1719, Val Loss: 69119.8984\n",
      "Epoch [50/100], Train Loss: 69190.0078, Val Loss: 69030.4922\n",
      "Epoch [60/100], Train Loss: 69087.3750, Val Loss: 68926.5938\n",
      "Epoch [70/100], Train Loss: 68970.5938, Val Loss: 68808.6953\n",
      "Epoch [80/100], Train Loss: 68840.7109, Val Loss: 68677.9141\n",
      "Epoch [90/100], Train Loss: 68699.1641, Val Loss: 68535.7188\n",
      "Epoch [100/100], Train Loss: 68547.5547, Val Loss: 68383.7500\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69501.1875, Val Loss: 69342.7422\n",
      "Epoch [20/100], Train Loss: 69437.0859, Val Loss: 69278.1562\n",
      "Epoch [30/100], Train Loss: 69363.3828, Val Loss: 69203.7188\n",
      "Epoch [40/100], Train Loss: 69277.0859, Val Loss: 69116.5703\n",
      "Epoch [50/100], Train Loss: 69176.8047, Val Loss: 69015.5078\n",
      "Epoch [60/100], Train Loss: 69062.5000, Val Loss: 68900.5547\n",
      "Epoch [70/100], Train Loss: 68934.9062, Val Loss: 68772.4453\n",
      "Epoch [80/100], Train Loss: 68795.1406, Val Loss: 68632.3281\n",
      "Epoch [90/100], Train Loss: 68644.7578, Val Loss: 68481.7656\n",
      "Epoch [100/100], Train Loss: 68485.5859, Val Loss: 68322.6328\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69563.5469, Val Loss: 69409.5625\n",
      "Epoch [20/100], Train Loss: 69561.6250, Val Loss: 69407.8281\n",
      "Epoch [30/100], Train Loss: 69561.0703, Val Loss: 69407.3047\n",
      "Epoch [40/100], Train Loss: 69560.8906, Val Loss: 69407.1484\n",
      "Epoch [50/100], Train Loss: 69560.8281, Val Loss: 69407.0859\n",
      "Epoch [60/100], Train Loss: 69560.8047, Val Loss: 69407.0703\n",
      "Epoch [70/100], Train Loss: 69560.8047, Val Loss: 69407.0703\n",
      "Epoch [80/100], Train Loss: 69560.7969, Val Loss: 69407.0625\n",
      "Epoch [90/100], Train Loss: 69560.7969, Val Loss: 69407.0625\n",
      "Epoch [100/100], Train Loss: 69560.7969, Val Loss: 69407.0625\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69541.9766, Val Loss: 69389.6797\n",
      "Epoch [20/100], Train Loss: 69540.2656, Val Loss: 69388.1641\n",
      "Epoch [30/100], Train Loss: 69539.7734, Val Loss: 69387.7109\n",
      "Epoch [40/100], Train Loss: 69539.6094, Val Loss: 69387.5625\n",
      "Epoch [50/100], Train Loss: 69539.5469, Val Loss: 69387.5156\n",
      "Epoch [60/100], Train Loss: 69539.5312, Val Loss: 69387.5000\n",
      "Epoch [70/100], Train Loss: 69539.5312, Val Loss: 69387.5000\n",
      "Epoch [80/100], Train Loss: 69539.5312, Val Loss: 69387.4922\n",
      "Epoch [90/100], Train Loss: 69539.5312, Val Loss: 69387.4922\n",
      "Epoch [100/100], Train Loss: 69539.5312, Val Loss: 69387.4922\n",
      "Trying params: hidden_dim=32, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69477.3750, Val Loss: 69324.7812\n",
      "Epoch [20/100], Train Loss: 69475.6797, Val Loss: 69323.2734\n",
      "Epoch [30/100], Train Loss: 69475.1953, Val Loss: 69322.8125\n",
      "Epoch [40/100], Train Loss: 69475.0312, Val Loss: 69322.6641\n",
      "Epoch [50/100], Train Loss: 69474.9844, Val Loss: 69322.6172\n",
      "Epoch [60/100], Train Loss: 69474.9609, Val Loss: 69322.6094\n",
      "Epoch [70/100], Train Loss: 69474.9609, Val Loss: 69322.6094\n",
      "Epoch [80/100], Train Loss: 69474.9609, Val Loss: 69322.6016\n",
      "Epoch [90/100], Train Loss: 69474.9609, Val Loss: 69322.6016\n",
      "Epoch [100/100], Train Loss: 69474.9609, Val Loss: 69322.6016\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68657.2578, Val Loss: 68340.2734\n",
      "Epoch [20/100], Train Loss: 66881.5547, Val Loss: 66525.7656\n",
      "Epoch [30/100], Train Loss: 65180.9102, Val Loss: 64829.7734\n",
      "Epoch [40/100], Train Loss: 63693.0352, Val Loss: 63346.2617\n",
      "Epoch [50/100], Train Loss: 62326.9766, Val Loss: 61977.8555\n",
      "Epoch [60/100], Train Loss: 61039.9453, Val Loss: 60685.9883\n",
      "Epoch [70/100], Train Loss: 59816.3906, Val Loss: 59456.0117\n",
      "Epoch [80/100], Train Loss: 58646.4062, Val Loss: 58281.0000\n",
      "Epoch [90/100], Train Loss: 57521.7070, Val Loss: 57152.1133\n",
      "Epoch [100/100], Train Loss: 56437.4062, Val Loss: 56064.2773\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68796.6797, Val Loss: 68484.9375\n",
      "Epoch [20/100], Train Loss: 67048.1562, Val Loss: 66690.8438\n",
      "Epoch [30/100], Train Loss: 65363.3633, Val Loss: 65008.4258\n",
      "Epoch [40/100], Train Loss: 63848.3945, Val Loss: 63497.5391\n",
      "Epoch [50/100], Train Loss: 62458.2227, Val Loss: 62106.8438\n",
      "Epoch [60/100], Train Loss: 61154.9336, Val Loss: 60799.4258\n",
      "Epoch [70/100], Train Loss: 59918.0000, Val Loss: 59557.2422\n",
      "Epoch [80/100], Train Loss: 58736.6016, Val Loss: 58371.2617\n",
      "Epoch [90/100], Train Loss: 57605.4453, Val Loss: 57234.2031\n",
      "Epoch [100/100], Train Loss: 56516.1953, Val Loss: 56139.4453\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68893.6328, Val Loss: 68582.7812\n",
      "Epoch [20/100], Train Loss: 67185.0000, Val Loss: 66815.2031\n",
      "Epoch [30/100], Train Loss: 65439.7852, Val Loss: 65073.1445\n",
      "Epoch [40/100], Train Loss: 63902.5078, Val Loss: 63546.2305\n",
      "Epoch [50/100], Train Loss: 62505.9531, Val Loss: 62151.5664\n",
      "Epoch [60/100], Train Loss: 61196.8438, Val Loss: 60838.4648\n",
      "Epoch [70/100], Train Loss: 59955.0000, Val Loss: 59592.0781\n",
      "Epoch [80/100], Train Loss: 58772.0352, Val Loss: 58403.7344\n",
      "Epoch [90/100], Train Loss: 57637.3164, Val Loss: 57263.5000\n",
      "Epoch [100/100], Train Loss: 56542.4883, Val Loss: 56163.4805\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69539.3984, Val Loss: 69382.8672\n",
      "Epoch [20/100], Train Loss: 69494.5156, Val Loss: 69335.8438\n",
      "Epoch [30/100], Train Loss: 69426.6016, Val Loss: 69263.9453\n",
      "Epoch [40/100], Train Loss: 69318.9062, Val Loss: 69150.3281\n",
      "Epoch [50/100], Train Loss: 69156.0312, Val Loss: 68981.2656\n",
      "Epoch [60/100], Train Loss: 68938.6797, Val Loss: 68757.5312\n",
      "Epoch [70/100], Train Loss: 68679.1875, Val Loss: 68495.4688\n",
      "Epoch [80/100], Train Loss: 68401.6250, Val Loss: 68215.3438\n",
      "Epoch [90/100], Train Loss: 68118.5547, Val Loss: 67931.5703\n",
      "Epoch [100/100], Train Loss: 67842.3125, Val Loss: 67651.5391\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69486.2812, Val Loss: 69329.8672\n",
      "Epoch [20/100], Train Loss: 69438.9219, Val Loss: 69280.4453\n",
      "Epoch [30/100], Train Loss: 69369.3594, Val Loss: 69207.6953\n",
      "Epoch [40/100], Train Loss: 69260.7031, Val Loss: 69093.4453\n",
      "Epoch [50/100], Train Loss: 69095.2812, Val Loss: 68920.0312\n",
      "Epoch [60/100], Train Loss: 68864.0312, Val Loss: 68682.9453\n",
      "Epoch [70/100], Train Loss: 68582.3750, Val Loss: 68396.1484\n",
      "Epoch [80/100], Train Loss: 68273.2656, Val Loss: 68083.5547\n",
      "Epoch [90/100], Train Loss: 67959.5703, Val Loss: 67766.7109\n",
      "Epoch [100/100], Train Loss: 67651.7422, Val Loss: 67457.4531\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69463.7344, Val Loss: 69305.0469\n",
      "Epoch [20/100], Train Loss: 69410.8906, Val Loss: 69248.8594\n",
      "Epoch [30/100], Train Loss: 69331.2344, Val Loss: 69163.3359\n",
      "Epoch [40/100], Train Loss: 69207.0859, Val Loss: 69031.8984\n",
      "Epoch [50/100], Train Loss: 69028.8125, Val Loss: 68842.8438\n",
      "Epoch [60/100], Train Loss: 68793.6953, Val Loss: 68598.6797\n",
      "Epoch [70/100], Train Loss: 68516.2734, Val Loss: 68316.6406\n",
      "Epoch [80/100], Train Loss: 68221.2109, Val Loss: 68017.4141\n",
      "Epoch [90/100], Train Loss: 67923.6719, Val Loss: 67715.6250\n",
      "Epoch [100/100], Train Loss: 67626.2578, Val Loss: 67418.5938\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69583.1562, Val Loss: 69430.4297\n",
      "Epoch [20/100], Train Loss: 69581.6406, Val Loss: 69429.0469\n",
      "Epoch [30/100], Train Loss: 69581.1484, Val Loss: 69428.6172\n",
      "Epoch [40/100], Train Loss: 69581.0312, Val Loss: 69428.4922\n",
      "Epoch [50/100], Train Loss: 69580.8672, Val Loss: 69428.4531\n",
      "Epoch [60/100], Train Loss: 69581.0391, Val Loss: 69428.4453\n",
      "Epoch [70/100], Train Loss: 69580.9609, Val Loss: 69428.4375\n",
      "Epoch [80/100], Train Loss: 69580.9531, Val Loss: 69428.4375\n",
      "Epoch [90/100], Train Loss: 69580.8984, Val Loss: 69428.4375\n",
      "Epoch [100/100], Train Loss: 69581.1250, Val Loss: 69428.4375\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69577.8047, Val Loss: 69425.3203\n",
      "Epoch [20/100], Train Loss: 69576.5234, Val Loss: 69423.9922\n",
      "Epoch [30/100], Train Loss: 69575.9766, Val Loss: 69423.5938\n",
      "Epoch [40/100], Train Loss: 69575.8125, Val Loss: 69423.4688\n",
      "Epoch [50/100], Train Loss: 69576.0234, Val Loss: 69423.4219\n",
      "Epoch [60/100], Train Loss: 69575.8281, Val Loss: 69423.4219\n",
      "Epoch [70/100], Train Loss: 69575.8516, Val Loss: 69423.3984\n",
      "Epoch [80/100], Train Loss: 69576.0234, Val Loss: 69423.3984\n",
      "Epoch [90/100], Train Loss: 69575.8750, Val Loss: 69423.3984\n",
      "Epoch [100/100], Train Loss: 69575.8047, Val Loss: 69423.3984\n",
      "Trying params: hidden_dim=32, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69587.5156, Val Loss: 69435.7500\n",
      "Epoch [20/100], Train Loss: 69586.3828, Val Loss: 69434.5547\n",
      "Epoch [30/100], Train Loss: 69585.8828, Val Loss: 69434.2031\n",
      "Epoch [40/100], Train Loss: 69585.5938, Val Loss: 69434.0703\n",
      "Epoch [50/100], Train Loss: 69585.8047, Val Loss: 69434.0469\n",
      "Epoch [60/100], Train Loss: 69585.8516, Val Loss: 69434.0312\n",
      "Epoch [70/100], Train Loss: 69585.5938, Val Loss: 69434.0312\n",
      "Epoch [80/100], Train Loss: 69585.5938, Val Loss: 69434.0312\n",
      "Epoch [90/100], Train Loss: 69585.6797, Val Loss: 69434.0312\n",
      "Epoch [100/100], Train Loss: 69585.8047, Val Loss: 69434.0312\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68712.4297, Val Loss: 68361.2891\n",
      "Epoch [20/100], Train Loss: 66850.0938, Val Loss: 66507.8438\n",
      "Epoch [30/100], Train Loss: 65372.4375, Val Loss: 65043.4648\n",
      "Epoch [40/100], Train Loss: 64007.3438, Val Loss: 63673.2227\n",
      "Epoch [50/100], Train Loss: 62704.7812, Val Loss: 62365.3086\n",
      "Epoch [60/100], Train Loss: 61464.7656, Val Loss: 61119.8320\n",
      "Epoch [70/100], Train Loss: 60278.3555, Val Loss: 59927.4375\n",
      "Epoch [80/100], Train Loss: 59135.8633, Val Loss: 58777.5898\n",
      "Epoch [90/100], Train Loss: 58032.1758, Val Loss: 57670.0195\n",
      "Epoch [100/100], Train Loss: 56957.5273, Val Loss: 56588.0117\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68556.6328, Val Loss: 68180.6875\n",
      "Epoch [20/100], Train Loss: 66659.9141, Val Loss: 66316.5469\n",
      "Epoch [30/100], Train Loss: 65210.1406, Val Loss: 64881.0586\n",
      "Epoch [40/100], Train Loss: 63862.5391, Val Loss: 63528.7383\n",
      "Epoch [50/100], Train Loss: 62573.6562, Val Loss: 62235.0234\n",
      "Epoch [60/100], Train Loss: 61344.0508, Val Loss: 60997.7773\n",
      "Epoch [70/100], Train Loss: 60164.7891, Val Loss: 59813.2773\n",
      "Epoch [80/100], Train Loss: 59029.2148, Val Loss: 58672.0469\n",
      "Epoch [90/100], Train Loss: 57931.0938, Val Loss: 57565.8359\n",
      "Epoch [100/100], Train Loss: 56869.1484, Val Loss: 56496.4531\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68448.0234, Val Loss: 68061.0391\n",
      "Epoch [20/100], Train Loss: 66648.4531, Val Loss: 66308.9141\n",
      "Epoch [30/100], Train Loss: 65201.0039, Val Loss: 64866.9336\n",
      "Epoch [40/100], Train Loss: 63835.5977, Val Loss: 63499.8359\n",
      "Epoch [50/100], Train Loss: 62540.8672, Val Loss: 62200.0430\n",
      "Epoch [60/100], Train Loss: 61306.6211, Val Loss: 60959.9883\n",
      "Epoch [70/100], Train Loss: 60122.6016, Val Loss: 59770.0039\n",
      "Epoch [80/100], Train Loss: 58986.3633, Val Loss: 58629.9102\n",
      "Epoch [90/100], Train Loss: 57890.0938, Val Loss: 57525.9414\n",
      "Epoch [100/100], Train Loss: 56828.4336, Val Loss: 56458.5938\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69530.9922, Val Loss: 69374.1953\n",
      "Epoch [20/100], Train Loss: 69479.1094, Val Loss: 69319.4531\n",
      "Epoch [30/100], Train Loss: 69390.6328, Val Loss: 69224.5000\n",
      "Epoch [40/100], Train Loss: 69229.3984, Val Loss: 69052.3750\n",
      "Epoch [50/100], Train Loss: 68971.3047, Val Loss: 68782.2734\n",
      "Epoch [60/100], Train Loss: 68640.3125, Val Loss: 68443.0781\n",
      "Epoch [70/100], Train Loss: 68284.9375, Val Loss: 68084.3516\n",
      "Epoch [80/100], Train Loss: 67940.9922, Val Loss: 67738.0078\n",
      "Epoch [90/100], Train Loss: 67624.5547, Val Loss: 67421.0625\n",
      "Epoch [100/100], Train Loss: 67342.0156, Val Loss: 67137.9141\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69568.9922, Val Loss: 69412.4375\n",
      "Epoch [20/100], Train Loss: 69516.9922, Val Loss: 69357.6172\n",
      "Epoch [30/100], Train Loss: 69426.8594, Val Loss: 69260.8438\n",
      "Epoch [40/100], Train Loss: 69258.4922, Val Loss: 69077.8516\n",
      "Epoch [50/100], Train Loss: 68972.3516, Val Loss: 68775.3828\n",
      "Epoch [60/100], Train Loss: 68593.9688, Val Loss: 68386.1094\n",
      "Epoch [70/100], Train Loss: 68191.2891, Val Loss: 67977.8516\n",
      "Epoch [80/100], Train Loss: 67809.1953, Val Loss: 67592.5625\n",
      "Epoch [90/100], Train Loss: 67470.4922, Val Loss: 67252.7031\n",
      "Epoch [100/100], Train Loss: 67177.8203, Val Loss: 66962.5234\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69441.0234, Val Loss: 69283.7734\n",
      "Epoch [20/100], Train Loss: 69396.2578, Val Loss: 69236.5312\n",
      "Epoch [30/100], Train Loss: 69322.1875, Val Loss: 69155.8516\n",
      "Epoch [40/100], Train Loss: 69181.1953, Val Loss: 69002.5156\n",
      "Epoch [50/100], Train Loss: 68933.5938, Val Loss: 68735.4141\n",
      "Epoch [60/100], Train Loss: 68583.3281, Val Loss: 68367.2656\n",
      "Epoch [70/100], Train Loss: 68189.5938, Val Loss: 67965.0391\n",
      "Epoch [80/100], Train Loss: 67808.1641, Val Loss: 67582.3281\n",
      "Epoch [90/100], Train Loss: 67470.8672, Val Loss: 67245.0156\n",
      "Epoch [100/100], Train Loss: 67180.2422, Val Loss: 66957.3281\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69494.1797, Val Loss: 69341.1797\n",
      "Epoch [20/100], Train Loss: 69493.1250, Val Loss: 69340.2109\n",
      "Epoch [30/100], Train Loss: 69492.7812, Val Loss: 69339.9141\n",
      "Epoch [40/100], Train Loss: 69492.6562, Val Loss: 69339.8203\n",
      "Epoch [50/100], Train Loss: 69492.6328, Val Loss: 69339.7891\n",
      "Epoch [60/100], Train Loss: 69492.6406, Val Loss: 69339.7812\n",
      "Epoch [70/100], Train Loss: 69492.6172, Val Loss: 69339.7734\n",
      "Epoch [80/100], Train Loss: 69492.5859, Val Loss: 69339.7734\n",
      "Epoch [90/100], Train Loss: 69492.5859, Val Loss: 69339.7734\n",
      "Epoch [100/100], Train Loss: 69492.5781, Val Loss: 69339.7734\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69607.7422, Val Loss: 69456.1172\n",
      "Epoch [20/100], Train Loss: 69606.3672, Val Loss: 69454.9375\n",
      "Epoch [30/100], Train Loss: 69605.9531, Val Loss: 69454.5859\n",
      "Epoch [40/100], Train Loss: 69605.9609, Val Loss: 69454.4766\n",
      "Epoch [50/100], Train Loss: 69605.8438, Val Loss: 69454.4375\n",
      "Epoch [60/100], Train Loss: 69605.7109, Val Loss: 69454.4219\n",
      "Epoch [70/100], Train Loss: 69605.7422, Val Loss: 69454.4141\n",
      "Epoch [80/100], Train Loss: 69605.9219, Val Loss: 69454.4141\n",
      "Epoch [90/100], Train Loss: 69605.8672, Val Loss: 69454.4141\n",
      "Epoch [100/100], Train Loss: 69605.7734, Val Loss: 69454.4141\n",
      "Trying params: hidden_dim=32, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69622.3125, Val Loss: 69470.5078\n",
      "Epoch [20/100], Train Loss: 69621.0625, Val Loss: 69469.3516\n",
      "Epoch [30/100], Train Loss: 69620.5234, Val Loss: 69469.0156\n",
      "Epoch [40/100], Train Loss: 69620.6953, Val Loss: 69468.9141\n",
      "Epoch [50/100], Train Loss: 69620.6562, Val Loss: 69468.8672\n",
      "Epoch [60/100], Train Loss: 69620.3906, Val Loss: 69468.8594\n",
      "Epoch [70/100], Train Loss: 69620.3672, Val Loss: 69468.8594\n",
      "Epoch [80/100], Train Loss: 69620.5078, Val Loss: 69468.8594\n",
      "Epoch [90/100], Train Loss: 69620.4766, Val Loss: 69468.8516\n",
      "Epoch [100/100], Train Loss: 69620.5469, Val Loss: 69468.8438\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68532.1953, Val Loss: 68167.0156\n",
      "Epoch [20/100], Train Loss: 66821.9688, Val Loss: 66497.8516\n",
      "Epoch [30/100], Train Loss: 65432.0703, Val Loss: 65109.8789\n",
      "Epoch [40/100], Train Loss: 64123.2812, Val Loss: 63795.6289\n",
      "Epoch [50/100], Train Loss: 62871.8359, Val Loss: 62538.6133\n",
      "Epoch [60/100], Train Loss: 61675.8789, Val Loss: 61337.4688\n",
      "Epoch [70/100], Train Loss: 60529.1953, Val Loss: 60183.9492\n",
      "Epoch [80/100], Train Loss: 59420.9219, Val Loss: 59069.1680\n",
      "Epoch [90/100], Train Loss: 58353.0195, Val Loss: 57994.7461\n",
      "Epoch [100/100], Train Loss: 57317.5352, Val Loss: 56954.8750\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68385.2109, Val Loss: 67990.2344\n",
      "Epoch [20/100], Train Loss: 66590.1953, Val Loss: 66264.9141\n",
      "Epoch [30/100], Train Loss: 65212.5117, Val Loss: 64883.3125\n",
      "Epoch [40/100], Train Loss: 63858.9844, Val Loss: 63524.6758\n",
      "Epoch [50/100], Train Loss: 62572.4023, Val Loss: 62233.1836\n",
      "Epoch [60/100], Train Loss: 61343.2617, Val Loss: 60997.7930\n",
      "Epoch [70/100], Train Loss: 60161.1953, Val Loss: 59812.0039\n",
      "Epoch [80/100], Train Loss: 59027.6172, Val Loss: 58669.2383\n",
      "Epoch [90/100], Train Loss: 57933.6016, Val Loss: 57571.1523\n",
      "Epoch [100/100], Train Loss: 56876.6289, Val Loss: 56507.3281\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68507.3984, Val Loss: 68113.7500\n",
      "Epoch [20/100], Train Loss: 66739.1562, Val Loss: 66405.1250\n",
      "Epoch [30/100], Train Loss: 65336.7422, Val Loss: 65006.6484\n",
      "Epoch [40/100], Train Loss: 63968.2891, Val Loss: 63633.4727\n",
      "Epoch [50/100], Train Loss: 62672.8359, Val Loss: 62333.5391\n",
      "Epoch [60/100], Train Loss: 61434.4453, Val Loss: 61088.7539\n",
      "Epoch [70/100], Train Loss: 60248.3984, Val Loss: 59894.6055\n",
      "Epoch [80/100], Train Loss: 59108.7578, Val Loss: 58750.0859\n",
      "Epoch [90/100], Train Loss: 58009.8789, Val Loss: 57647.8438\n",
      "Epoch [100/100], Train Loss: 56947.8828, Val Loss: 56578.7891\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69505.8203, Val Loss: 69349.1016\n",
      "Epoch [20/100], Train Loss: 69458.4062, Val Loss: 69299.0234\n",
      "Epoch [30/100], Train Loss: 69370.4219, Val Loss: 69203.0859\n",
      "Epoch [40/100], Train Loss: 69176.4219, Val Loss: 68990.3125\n",
      "Epoch [50/100], Train Loss: 68809.6953, Val Loss: 68602.1797\n",
      "Epoch [60/100], Train Loss: 68340.3203, Val Loss: 68125.1719\n",
      "Epoch [70/100], Train Loss: 67901.9375, Val Loss: 67689.3125\n",
      "Epoch [80/100], Train Loss: 67545.5703, Val Loss: 67337.9609\n",
      "Epoch [90/100], Train Loss: 67261.6094, Val Loss: 67058.7266\n",
      "Epoch [100/100], Train Loss: 67026.4453, Val Loss: 66825.9531\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69583.0391, Val Loss: 69426.8750\n",
      "Epoch [20/100], Train Loss: 69536.8203, Val Loss: 69378.1875\n",
      "Epoch [30/100], Train Loss: 69456.1094, Val Loss: 69289.5391\n",
      "Epoch [40/100], Train Loss: 69285.6641, Val Loss: 69101.5156\n",
      "Epoch [50/100], Train Loss: 68971.0000, Val Loss: 68764.4766\n",
      "Epoch [60/100], Train Loss: 68564.7109, Val Loss: 68347.2500\n",
      "Epoch [70/100], Train Loss: 68163.3750, Val Loss: 67941.7578\n",
      "Epoch [80/100], Train Loss: 67807.3516, Val Loss: 67592.2578\n",
      "Epoch [90/100], Train Loss: 67513.1016, Val Loss: 67301.9844\n",
      "Epoch [100/100], Train Loss: 67263.0234, Val Loss: 67056.3750\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69525.7109, Val Loss: 69369.7656\n",
      "Epoch [20/100], Train Loss: 69487.1562, Val Loss: 69328.7734\n",
      "Epoch [30/100], Train Loss: 69419.4844, Val Loss: 69254.0781\n",
      "Epoch [40/100], Train Loss: 69272.6484, Val Loss: 69089.0781\n",
      "Epoch [50/100], Train Loss: 68985.6172, Val Loss: 68772.0469\n",
      "Epoch [60/100], Train Loss: 68583.2891, Val Loss: 68351.2500\n",
      "Epoch [70/100], Train Loss: 68177.8359, Val Loss: 67941.4219\n",
      "Epoch [80/100], Train Loss: 67822.8828, Val Loss: 67596.7422\n",
      "Epoch [90/100], Train Loss: 67532.8203, Val Loss: 67313.2266\n",
      "Epoch [100/100], Train Loss: 67287.5234, Val Loss: 67073.1562\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69488.3906, Val Loss: 69335.3906\n",
      "Epoch [20/100], Train Loss: 69487.3203, Val Loss: 69334.4141\n",
      "Epoch [30/100], Train Loss: 69487.0000, Val Loss: 69334.1094\n",
      "Epoch [40/100], Train Loss: 69486.8594, Val Loss: 69334.0156\n",
      "Epoch [50/100], Train Loss: 69486.8359, Val Loss: 69334.0000\n",
      "Epoch [60/100], Train Loss: 69486.7969, Val Loss: 69333.9922\n",
      "Epoch [70/100], Train Loss: 69486.8203, Val Loss: 69333.9766\n",
      "Epoch [80/100], Train Loss: 69486.8203, Val Loss: 69333.9766\n",
      "Epoch [90/100], Train Loss: 69486.8438, Val Loss: 69333.9766\n",
      "Epoch [100/100], Train Loss: 69486.8828, Val Loss: 69333.9766\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69603.4844, Val Loss: 69451.4844\n",
      "Epoch [20/100], Train Loss: 69602.3906, Val Loss: 69450.4922\n",
      "Epoch [30/100], Train Loss: 69602.1094, Val Loss: 69450.1875\n",
      "Epoch [40/100], Train Loss: 69602.0000, Val Loss: 69450.1016\n",
      "Epoch [50/100], Train Loss: 69601.9453, Val Loss: 69450.0703\n",
      "Epoch [60/100], Train Loss: 69601.9375, Val Loss: 69450.0547\n",
      "Epoch [70/100], Train Loss: 69601.9141, Val Loss: 69450.0547\n",
      "Epoch [80/100], Train Loss: 69601.9219, Val Loss: 69450.0547\n",
      "Epoch [90/100], Train Loss: 69601.9141, Val Loss: 69450.0547\n",
      "Epoch [100/100], Train Loss: 69601.9453, Val Loss: 69450.0547\n",
      "Trying params: hidden_dim=32, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69592.5938, Val Loss: 69440.2969\n",
      "Epoch [20/100], Train Loss: 69591.3203, Val Loss: 69439.1406\n",
      "Epoch [30/100], Train Loss: 69590.9062, Val Loss: 69438.7891\n",
      "Epoch [40/100], Train Loss: 69590.7266, Val Loss: 69438.6719\n",
      "Epoch [50/100], Train Loss: 69590.7969, Val Loss: 69438.6328\n",
      "Epoch [60/100], Train Loss: 69590.7266, Val Loss: 69438.6172\n",
      "Epoch [70/100], Train Loss: 69590.7422, Val Loss: 69438.6172\n",
      "Epoch [80/100], Train Loss: 69590.7734, Val Loss: 69438.6172\n",
      "Epoch [90/100], Train Loss: 69590.6875, Val Loss: 69438.6172\n",
      "Epoch [100/100], Train Loss: 69590.7266, Val Loss: 69438.6172\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 68586.3125, Val Loss: 68269.6953\n",
      "Epoch [20/100], Train Loss: 66547.8438, Val Loss: 66168.1094\n",
      "Epoch [30/100], Train Loss: 63995.6836, Val Loss: 63586.6367\n",
      "Epoch [40/100], Train Loss: 61209.4570, Val Loss: 60772.8047\n",
      "Epoch [50/100], Train Loss: 58323.8477, Val Loss: 57855.5508\n",
      "Epoch [60/100], Train Loss: 55469.6094, Val Loss: 54975.9141\n",
      "Epoch [70/100], Train Loss: 52778.0195, Val Loss: 52269.9062\n",
      "Epoch [80/100], Train Loss: 50318.8008, Val Loss: 49805.0273\n",
      "Epoch [90/100], Train Loss: 48094.9688, Val Loss: 47580.0078\n",
      "Epoch [100/100], Train Loss: 46079.4023, Val Loss: 45565.3359\n",
      "New best score: 45565.3359375 with params: {'hidden_dim': 64, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68650.7344, Val Loss: 68346.2578\n",
      "Epoch [20/100], Train Loss: 66701.6406, Val Loss: 66325.9141\n",
      "Epoch [30/100], Train Loss: 64174.1055, Val Loss: 63759.4922\n",
      "Epoch [40/100], Train Loss: 61366.3164, Val Loss: 60922.0273\n",
      "Epoch [50/100], Train Loss: 58450.6211, Val Loss: 57980.0547\n",
      "Epoch [60/100], Train Loss: 55586.4570, Val Loss: 55094.4727\n",
      "Epoch [70/100], Train Loss: 52896.0625, Val Loss: 52391.1211\n",
      "Epoch [80/100], Train Loss: 50436.8203, Val Loss: 49927.0469\n",
      "Epoch [90/100], Train Loss: 48204.8164, Val Loss: 47694.7344\n",
      "Epoch [100/100], Train Loss: 46170.7422, Val Loss: 45660.3242\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 68620.3438, Val Loss: 68313.7266\n",
      "Epoch [20/100], Train Loss: 66618.7891, Val Loss: 66244.0859\n",
      "Epoch [30/100], Train Loss: 64062.8359, Val Loss: 63657.1016\n",
      "Epoch [40/100], Train Loss: 61287.0547, Val Loss: 60854.1094\n",
      "Epoch [50/100], Train Loss: 58424.0859, Val Loss: 57961.1484\n",
      "Epoch [60/100], Train Loss: 55597.9727, Val Loss: 55110.4570\n",
      "Epoch [70/100], Train Loss: 52925.3008, Val Loss: 52422.7617\n",
      "Epoch [80/100], Train Loss: 50469.7188, Val Loss: 49960.1914\n",
      "Epoch [90/100], Train Loss: 48239.2500, Val Loss: 47727.6875\n",
      "Epoch [100/100], Train Loss: 46209.3320, Val Loss: 45697.7656\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69480.1484, Val Loss: 69321.1016\n",
      "Epoch [20/100], Train Loss: 69401.4766, Val Loss: 69241.5156\n",
      "Epoch [30/100], Train Loss: 69304.6016, Val Loss: 69142.6719\n",
      "Epoch [40/100], Train Loss: 69180.0547, Val Loss: 69015.5781\n",
      "Epoch [50/100], Train Loss: 69022.5156, Val Loss: 68855.2891\n",
      "Epoch [60/100], Train Loss: 68830.1094, Val Loss: 68660.3438\n",
      "Epoch [70/100], Train Loss: 68604.2969, Val Loss: 68432.6016\n",
      "Epoch [80/100], Train Loss: 68348.9766, Val Loss: 68176.1641\n",
      "Epoch [90/100], Train Loss: 68069.2969, Val Loss: 67896.2266\n",
      "Epoch [100/100], Train Loss: 67770.7188, Val Loss: 67598.1562\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69509.0781, Val Loss: 69349.8438\n",
      "Epoch [20/100], Train Loss: 69416.2344, Val Loss: 69255.9531\n",
      "Epoch [30/100], Train Loss: 69303.9922, Val Loss: 69141.8828\n",
      "Epoch [40/100], Train Loss: 69164.4297, Val Loss: 69000.2656\n",
      "Epoch [50/100], Train Loss: 68993.8359, Val Loss: 68827.7578\n",
      "Epoch [60/100], Train Loss: 68791.6484, Val Loss: 68624.0312\n",
      "Epoch [70/100], Train Loss: 68559.6250, Val Loss: 68391.0234\n",
      "Epoch [80/100], Train Loss: 68300.8828, Val Loss: 68131.9141\n",
      "Epoch [90/100], Train Loss: 68019.3203, Val Loss: 67850.5547\n",
      "Epoch [100/100], Train Loss: 67719.1953, Val Loss: 67551.1797\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69481.1406, Val Loss: 69320.6797\n",
      "Epoch [20/100], Train Loss: 69403.7969, Val Loss: 69241.6953\n",
      "Epoch [30/100], Train Loss: 69307.9609, Val Loss: 69143.2266\n",
      "Epoch [40/100], Train Loss: 69184.7500, Val Loss: 69016.8750\n",
      "Epoch [50/100], Train Loss: 69028.7656, Val Loss: 68857.6484\n",
      "Epoch [60/100], Train Loss: 68838.2969, Val Loss: 68664.2344\n",
      "Epoch [70/100], Train Loss: 68614.7109, Val Loss: 68438.3828\n",
      "Epoch [80/100], Train Loss: 68361.8594, Val Loss: 68184.2109\n",
      "Epoch [90/100], Train Loss: 68084.9062, Val Loss: 67906.9375\n",
      "Epoch [100/100], Train Loss: 67789.0078, Val Loss: 67611.5938\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69544.0469, Val Loss: 69391.4219\n",
      "Epoch [20/100], Train Loss: 69541.4531, Val Loss: 69389.0234\n",
      "Epoch [30/100], Train Loss: 69540.4297, Val Loss: 69388.1562\n",
      "Epoch [40/100], Train Loss: 69540.1094, Val Loss: 69387.8672\n",
      "Epoch [50/100], Train Loss: 69540.0156, Val Loss: 69387.7734\n",
      "Epoch [60/100], Train Loss: 69539.9844, Val Loss: 69387.7500\n",
      "Epoch [70/100], Train Loss: 69539.9688, Val Loss: 69387.7344\n",
      "Epoch [80/100], Train Loss: 69539.9688, Val Loss: 69387.7344\n",
      "Epoch [90/100], Train Loss: 69539.9688, Val Loss: 69387.7344\n",
      "Epoch [100/100], Train Loss: 69539.9688, Val Loss: 69387.7344\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69546.3984, Val Loss: 69391.7734\n",
      "Epoch [20/100], Train Loss: 69542.3828, Val Loss: 69387.9531\n",
      "Epoch [30/100], Train Loss: 69540.7734, Val Loss: 69386.4453\n",
      "Epoch [40/100], Train Loss: 69540.2109, Val Loss: 69385.9609\n",
      "Epoch [50/100], Train Loss: 69540.0391, Val Loss: 69385.8125\n",
      "Epoch [60/100], Train Loss: 69539.9844, Val Loss: 69385.7656\n",
      "Epoch [70/100], Train Loss: 69539.9688, Val Loss: 69385.7500\n",
      "Epoch [80/100], Train Loss: 69539.9609, Val Loss: 69385.7500\n",
      "Epoch [90/100], Train Loss: 69539.9609, Val Loss: 69385.7500\n",
      "Epoch [100/100], Train Loss: 69539.9609, Val Loss: 69385.7422\n",
      "Trying params: hidden_dim=64, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69569.0156, Val Loss: 69417.7109\n",
      "Epoch [20/100], Train Loss: 69566.6484, Val Loss: 69415.5234\n",
      "Epoch [30/100], Train Loss: 69565.2891, Val Loss: 69414.2500\n",
      "Epoch [40/100], Train Loss: 69564.8203, Val Loss: 69413.8516\n",
      "Epoch [50/100], Train Loss: 69564.6719, Val Loss: 69413.7109\n",
      "Epoch [60/100], Train Loss: 69564.6328, Val Loss: 69413.6797\n",
      "Epoch [70/100], Train Loss: 69564.6172, Val Loss: 69413.6641\n",
      "Epoch [80/100], Train Loss: 69564.6094, Val Loss: 69413.6641\n",
      "Epoch [90/100], Train Loss: 69564.6094, Val Loss: 69413.6641\n",
      "Epoch [100/100], Train Loss: 69564.6094, Val Loss: 69413.6641\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 67797.3281, Val Loss: 67311.0859\n",
      "Epoch [20/100], Train Loss: 64534.1445, Val Loss: 64020.7734\n",
      "Epoch [30/100], Train Loss: 61690.0000, Val Loss: 61204.2969\n",
      "Epoch [40/100], Train Loss: 59186.5938, Val Loss: 58700.1523\n",
      "Epoch [50/100], Train Loss: 56860.9492, Val Loss: 56365.5312\n",
      "Epoch [60/100], Train Loss: 54689.9375, Val Loss: 54195.6016\n",
      "Epoch [70/100], Train Loss: 52658.2656, Val Loss: 52161.8867\n",
      "Epoch [80/100], Train Loss: 50750.8672, Val Loss: 50257.1641\n",
      "Epoch [90/100], Train Loss: 48958.9648, Val Loss: 48465.7266\n",
      "Epoch [100/100], Train Loss: 47247.1523, Val Loss: 46749.6914\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 68066.2188, Val Loss: 67606.4062\n",
      "Epoch [20/100], Train Loss: 64910.6836, Val Loss: 64404.7812\n",
      "Epoch [30/100], Train Loss: 62073.0664, Val Loss: 61587.6836\n",
      "Epoch [40/100], Train Loss: 59554.7383, Val Loss: 59069.0039\n",
      "Epoch [50/100], Train Loss: 57217.2266, Val Loss: 56723.8594\n",
      "Epoch [60/100], Train Loss: 55036.7852, Val Loss: 54544.3125\n",
      "Epoch [70/100], Train Loss: 52999.8711, Val Loss: 52503.7500\n",
      "Epoch [80/100], Train Loss: 51084.7383, Val Loss: 50590.0898\n",
      "Epoch [90/100], Train Loss: 49286.2070, Val Loss: 48791.6250\n",
      "Epoch [100/100], Train Loss: 47573.2500, Val Loss: 47071.9297\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67635.9375, Val Loss: 67137.0469\n",
      "Epoch [20/100], Train Loss: 64367.1328, Val Loss: 63854.6523\n",
      "Epoch [30/100], Train Loss: 61560.2891, Val Loss: 61071.2227\n",
      "Epoch [40/100], Train Loss: 59073.3594, Val Loss: 58585.9180\n",
      "Epoch [50/100], Train Loss: 56770.9336, Val Loss: 56275.2109\n",
      "Epoch [60/100], Train Loss: 54620.1406, Val Loss: 54126.8555\n",
      "Epoch [70/100], Train Loss: 52606.8594, Val Loss: 52108.1055\n",
      "Epoch [80/100], Train Loss: 50713.8125, Val Loss: 50216.8008\n",
      "Epoch [90/100], Train Loss: 48925.3242, Val Loss: 48426.5625\n",
      "Epoch [100/100], Train Loss: 47221.1094, Val Loss: 46719.2422\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69505.9141, Val Loss: 69346.0469\n",
      "Epoch [20/100], Train Loss: 69402.0938, Val Loss: 69235.9219\n",
      "Epoch [30/100], Train Loss: 69205.0000, Val Loss: 69026.8828\n",
      "Epoch [40/100], Train Loss: 68864.3594, Val Loss: 68671.4609\n",
      "Epoch [50/100], Train Loss: 68382.8984, Val Loss: 68181.0000\n",
      "Epoch [60/100], Train Loss: 67825.5938, Val Loss: 67621.3125\n",
      "Epoch [70/100], Train Loss: 67258.1875, Val Loss: 67052.1797\n",
      "Epoch [80/100], Train Loss: 66711.3672, Val Loss: 66499.2422\n",
      "Epoch [90/100], Train Loss: 66189.3359, Val Loss: 65970.4688\n",
      "Epoch [100/100], Train Loss: 65699.2422, Val Loss: 65474.3203\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69505.6641, Val Loss: 69345.6562\n",
      "Epoch [20/100], Train Loss: 69409.0156, Val Loss: 69241.6406\n",
      "Epoch [30/100], Train Loss: 69220.6406, Val Loss: 69038.7109\n",
      "Epoch [40/100], Train Loss: 68889.4453, Val Loss: 68690.6484\n",
      "Epoch [50/100], Train Loss: 68424.5469, Val Loss: 68215.3672\n",
      "Epoch [60/100], Train Loss: 67892.1875, Val Loss: 67681.8516\n",
      "Epoch [70/100], Train Loss: 67359.0938, Val Loss: 67144.5078\n",
      "Epoch [80/100], Train Loss: 66840.8281, Val Loss: 66621.5547\n",
      "Epoch [90/100], Train Loss: 66340.9453, Val Loss: 66113.8203\n",
      "Epoch [100/100], Train Loss: 65862.0625, Val Loss: 65627.5703\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69523.3438, Val Loss: 69363.8125\n",
      "Epoch [20/100], Train Loss: 69415.4766, Val Loss: 69247.6406\n",
      "Epoch [30/100], Train Loss: 69204.7656, Val Loss: 69022.1328\n",
      "Epoch [40/100], Train Loss: 68841.4844, Val Loss: 68644.5469\n",
      "Epoch [50/100], Train Loss: 68347.3516, Val Loss: 68140.5781\n",
      "Epoch [60/100], Train Loss: 67795.7969, Val Loss: 67584.7734\n",
      "Epoch [70/100], Train Loss: 67248.8438, Val Loss: 67032.2812\n",
      "Epoch [80/100], Train Loss: 66720.9922, Val Loss: 66497.7578\n",
      "Epoch [90/100], Train Loss: 66215.6484, Val Loss: 65982.1953\n",
      "Epoch [100/100], Train Loss: 65729.8516, Val Loss: 65494.0586\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69524.5000, Val Loss: 69371.6797\n",
      "Epoch [20/100], Train Loss: 69522.8047, Val Loss: 69370.1250\n",
      "Epoch [30/100], Train Loss: 69522.2891, Val Loss: 69369.6562\n",
      "Epoch [40/100], Train Loss: 69522.2734, Val Loss: 69369.5078\n",
      "Epoch [50/100], Train Loss: 69522.1406, Val Loss: 69369.4688\n",
      "Epoch [60/100], Train Loss: 69521.9844, Val Loss: 69369.4375\n",
      "Epoch [70/100], Train Loss: 69521.9688, Val Loss: 69369.4375\n",
      "Epoch [80/100], Train Loss: 69522.0000, Val Loss: 69369.4375\n",
      "Epoch [90/100], Train Loss: 69522.0078, Val Loss: 69369.4375\n",
      "Epoch [100/100], Train Loss: 69521.9766, Val Loss: 69369.4375\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69582.0000, Val Loss: 69429.5312\n",
      "Epoch [20/100], Train Loss: 69579.9922, Val Loss: 69427.6094\n",
      "Epoch [30/100], Train Loss: 69579.0859, Val Loss: 69427.0312\n",
      "Epoch [40/100], Train Loss: 69579.2812, Val Loss: 69426.7578\n",
      "Epoch [50/100], Train Loss: 69578.9453, Val Loss: 69426.6875\n",
      "Epoch [60/100], Train Loss: 69578.7578, Val Loss: 69426.6641\n",
      "Epoch [70/100], Train Loss: 69578.9531, Val Loss: 69426.6484\n",
      "Epoch [80/100], Train Loss: 69579.1094, Val Loss: 69426.6406\n",
      "Epoch [90/100], Train Loss: 69578.8125, Val Loss: 69426.6406\n",
      "Epoch [100/100], Train Loss: 69578.8906, Val Loss: 69426.6406\n",
      "Trying params: hidden_dim=64, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69514.8125, Val Loss: 69362.3516\n",
      "Epoch [20/100], Train Loss: 69513.1172, Val Loss: 69360.6406\n",
      "Epoch [30/100], Train Loss: 69512.6406, Val Loss: 69360.1172\n",
      "Epoch [40/100], Train Loss: 69512.4844, Val Loss: 69359.9609\n",
      "Epoch [50/100], Train Loss: 69512.3281, Val Loss: 69359.9062\n",
      "Epoch [60/100], Train Loss: 69512.2891, Val Loss: 69359.8906\n",
      "Epoch [70/100], Train Loss: 69512.3125, Val Loss: 69359.8828\n",
      "Epoch [80/100], Train Loss: 69512.2344, Val Loss: 69359.8828\n",
      "Epoch [90/100], Train Loss: 69512.2031, Val Loss: 69359.8828\n",
      "Epoch [100/100], Train Loss: 69512.2031, Val Loss: 69359.8828\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 67314.5859, Val Loss: 66784.0938\n",
      "Epoch [20/100], Train Loss: 64378.1484, Val Loss: 63925.6914\n",
      "Epoch [30/100], Train Loss: 61886.7227, Val Loss: 61422.5430\n",
      "Epoch [40/100], Train Loss: 59520.5703, Val Loss: 59049.3438\n",
      "Epoch [50/100], Train Loss: 57305.1250, Val Loss: 56823.6875\n",
      "Epoch [60/100], Train Loss: 55220.7227, Val Loss: 54728.5625\n",
      "Epoch [70/100], Train Loss: 53251.6484, Val Loss: 52761.2930\n",
      "Epoch [80/100], Train Loss: 51396.4531, Val Loss: 50906.4531\n",
      "Epoch [90/100], Train Loss: 49635.4531, Val Loss: 49145.3438\n",
      "Epoch [100/100], Train Loss: 47965.3008, Val Loss: 47472.3320\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 67182.1875, Val Loss: 66642.5000\n",
      "Epoch [20/100], Train Loss: 64241.6875, Val Loss: 63789.6680\n",
      "Epoch [30/100], Train Loss: 61742.7617, Val Loss: 61276.5039\n",
      "Epoch [40/100], Train Loss: 59370.6094, Val Loss: 58896.1914\n",
      "Epoch [50/100], Train Loss: 57151.2500, Val Loss: 56666.9883\n",
      "Epoch [60/100], Train Loss: 55070.0156, Val Loss: 54576.1602\n",
      "Epoch [70/100], Train Loss: 53113.3477, Val Loss: 52608.7930\n",
      "Epoch [80/100], Train Loss: 51258.4062, Val Loss: 50764.8203\n",
      "Epoch [90/100], Train Loss: 49521.1641, Val Loss: 49018.7656\n",
      "Epoch [100/100], Train Loss: 47859.5820, Val Loss: 47370.5664\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67183.5391, Val Loss: 66632.9688\n",
      "Epoch [20/100], Train Loss: 64267.7148, Val Loss: 63808.1445\n",
      "Epoch [30/100], Train Loss: 61764.1328, Val Loss: 61299.7148\n",
      "Epoch [40/100], Train Loss: 59398.0078, Val Loss: 58925.2773\n",
      "Epoch [50/100], Train Loss: 57180.6758, Val Loss: 56695.7109\n",
      "Epoch [60/100], Train Loss: 55108.6797, Val Loss: 54615.8203\n",
      "Epoch [70/100], Train Loss: 53175.7188, Val Loss: 52672.5117\n",
      "Epoch [80/100], Train Loss: 51322.3164, Val Loss: 50816.0352\n",
      "Epoch [90/100], Train Loss: 49572.4023, Val Loss: 49074.3125\n",
      "Epoch [100/100], Train Loss: 47915.3242, Val Loss: 47419.1250\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69445.5391, Val Loss: 69285.6016\n",
      "Epoch [20/100], Train Loss: 69343.3125, Val Loss: 69173.2344\n",
      "Epoch [30/100], Train Loss: 69076.6875, Val Loss: 68879.8672\n",
      "Epoch [40/100], Train Loss: 68511.0078, Val Loss: 68283.8125\n",
      "Epoch [50/100], Train Loss: 67785.1172, Val Loss: 67550.4219\n",
      "Epoch [60/100], Train Loss: 67103.1406, Val Loss: 66866.1719\n",
      "Epoch [70/100], Train Loss: 66526.5469, Val Loss: 66290.8516\n",
      "Epoch [80/100], Train Loss: 66047.0156, Val Loss: 65816.9375\n",
      "Epoch [90/100], Train Loss: 65642.2031, Val Loss: 65415.5234\n",
      "Epoch [100/100], Train Loss: 65282.6641, Val Loss: 65057.4141\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69500.5781, Val Loss: 69340.1172\n",
      "Epoch [20/100], Train Loss: 69383.8594, Val Loss: 69210.1484\n",
      "Epoch [30/100], Train Loss: 69074.9453, Val Loss: 68870.2734\n",
      "Epoch [40/100], Train Loss: 68490.0391, Val Loss: 68261.9453\n",
      "Epoch [50/100], Train Loss: 67811.0000, Val Loss: 67578.5078\n",
      "Epoch [60/100], Train Loss: 67170.2188, Val Loss: 66933.4062\n",
      "Epoch [70/100], Train Loss: 66603.0312, Val Loss: 66362.1641\n",
      "Epoch [80/100], Train Loss: 66119.2266, Val Loss: 65882.8594\n",
      "Epoch [90/100], Train Loss: 65708.1172, Val Loss: 65476.1094\n",
      "Epoch [100/100], Train Loss: 65344.5000, Val Loss: 65114.2227\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69466.1797, Val Loss: 69306.1641\n",
      "Epoch [20/100], Train Loss: 69357.6562, Val Loss: 69185.6328\n",
      "Epoch [30/100], Train Loss: 69072.1406, Val Loss: 68868.6719\n",
      "Epoch [40/100], Train Loss: 68510.6562, Val Loss: 68273.8594\n",
      "Epoch [50/100], Train Loss: 67826.2812, Val Loss: 67577.5000\n",
      "Epoch [60/100], Train Loss: 67164.8359, Val Loss: 66912.0078\n",
      "Epoch [70/100], Train Loss: 66588.3438, Val Loss: 66338.3672\n",
      "Epoch [80/100], Train Loss: 66102.9922, Val Loss: 65860.6797\n",
      "Epoch [90/100], Train Loss: 65692.7734, Val Loss: 65456.4844\n",
      "Epoch [100/100], Train Loss: 65328.9414, Val Loss: 65096.0508\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69536.9844, Val Loss: 69384.3516\n",
      "Epoch [20/100], Train Loss: 69535.1172, Val Loss: 69382.6953\n",
      "Epoch [30/100], Train Loss: 69534.6250, Val Loss: 69382.1953\n",
      "Epoch [40/100], Train Loss: 69534.4062, Val Loss: 69382.0391\n",
      "Epoch [50/100], Train Loss: 69534.3750, Val Loss: 69381.9844\n",
      "Epoch [60/100], Train Loss: 69534.3906, Val Loss: 69381.9688\n",
      "Epoch [70/100], Train Loss: 69534.3516, Val Loss: 69381.9688\n",
      "Epoch [80/100], Train Loss: 69534.3047, Val Loss: 69381.9609\n",
      "Epoch [90/100], Train Loss: 69534.3281, Val Loss: 69381.9609\n",
      "Epoch [100/100], Train Loss: 69534.3516, Val Loss: 69381.9609\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69594.3828, Val Loss: 69442.0391\n",
      "Epoch [20/100], Train Loss: 69592.7656, Val Loss: 69440.5312\n",
      "Epoch [30/100], Train Loss: 69592.2109, Val Loss: 69440.0781\n",
      "Epoch [40/100], Train Loss: 69592.1250, Val Loss: 69439.9375\n",
      "Epoch [50/100], Train Loss: 69592.0156, Val Loss: 69439.8984\n",
      "Epoch [60/100], Train Loss: 69591.9844, Val Loss: 69439.8828\n",
      "Epoch [70/100], Train Loss: 69592.0312, Val Loss: 69439.8750\n",
      "Epoch [80/100], Train Loss: 69591.9453, Val Loss: 69439.8750\n",
      "Epoch [90/100], Train Loss: 69591.9922, Val Loss: 69439.8750\n",
      "Epoch [100/100], Train Loss: 69591.9609, Val Loss: 69439.8672\n",
      "Trying params: hidden_dim=64, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69548.6172, Val Loss: 69395.8281\n",
      "Epoch [20/100], Train Loss: 69546.6016, Val Loss: 69394.0234\n",
      "Epoch [30/100], Train Loss: 69546.1797, Val Loss: 69393.4922\n",
      "Epoch [40/100], Train Loss: 69545.8750, Val Loss: 69393.3203\n",
      "Epoch [50/100], Train Loss: 69545.8125, Val Loss: 69393.2578\n",
      "Epoch [60/100], Train Loss: 69545.7734, Val Loss: 69393.2344\n",
      "Epoch [70/100], Train Loss: 69545.8828, Val Loss: 69393.2344\n",
      "Epoch [80/100], Train Loss: 69545.7188, Val Loss: 69393.2344\n",
      "Epoch [90/100], Train Loss: 69545.8438, Val Loss: 69393.2344\n",
      "Epoch [100/100], Train Loss: 69545.7734, Val Loss: 69393.2266\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 66842.4375, Val Loss: 66348.3438\n",
      "Epoch [20/100], Train Loss: 64192.0000, Val Loss: 63743.9180\n",
      "Epoch [30/100], Train Loss: 61755.0352, Val Loss: 61295.3711\n",
      "Epoch [40/100], Train Loss: 59439.6758, Val Loss: 58970.7773\n",
      "Epoch [50/100], Train Loss: 57272.5430, Val Loss: 56795.7695\n",
      "Epoch [60/100], Train Loss: 55249.9297, Val Loss: 54766.2969\n",
      "Epoch [70/100], Train Loss: 53364.0312, Val Loss: 52869.8789\n",
      "Epoch [80/100], Train Loss: 51601.6992, Val Loss: 51102.5039\n",
      "Epoch [90/100], Train Loss: 49801.3008, Val Loss: 49307.1367\n",
      "Epoch [100/100], Train Loss: 48144.6055, Val Loss: 47656.3008\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 67243.8281, Val Loss: 66722.0469\n",
      "Epoch [20/100], Train Loss: 64494.9688, Val Loss: 64046.2852\n",
      "Epoch [30/100], Train Loss: 62024.0391, Val Loss: 61563.3281\n",
      "Epoch [40/100], Train Loss: 59675.3320, Val Loss: 59205.3320\n",
      "Epoch [50/100], Train Loss: 57473.7852, Val Loss: 56997.4883\n",
      "Epoch [60/100], Train Loss: 55419.5352, Val Loss: 54932.4766\n",
      "Epoch [70/100], Train Loss: 53463.2578, Val Loss: 52963.9414\n",
      "Epoch [80/100], Train Loss: 51612.3242, Val Loss: 51129.0039\n",
      "Epoch [90/100], Train Loss: 49866.6172, Val Loss: 49376.3555\n",
      "Epoch [100/100], Train Loss: 48212.4141, Val Loss: 47728.4531\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67042.6953, Val Loss: 66515.6797\n",
      "Epoch [20/100], Train Loss: 64279.4336, Val Loss: 63828.5039\n",
      "Epoch [30/100], Train Loss: 61817.4180, Val Loss: 61355.0820\n",
      "Epoch [40/100], Train Loss: 59474.0820, Val Loss: 59003.3906\n",
      "Epoch [50/100], Train Loss: 57285.4961, Val Loss: 56807.1328\n",
      "Epoch [60/100], Train Loss: 55248.7930, Val Loss: 54763.0742\n",
      "Epoch [70/100], Train Loss: 53351.9648, Val Loss: 52855.7422\n",
      "Epoch [80/100], Train Loss: 51580.4766, Val Loss: 51075.4219\n",
      "Epoch [90/100], Train Loss: 49869.7461, Val Loss: 49314.6797\n",
      "Epoch [100/100], Train Loss: 48169.6914, Val Loss: 47657.5195\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69463.6484, Val Loss: 69302.2969\n",
      "Epoch [20/100], Train Loss: 69325.6250, Val Loss: 69147.2891\n",
      "Epoch [30/100], Train Loss: 68898.2031, Val Loss: 68675.4609\n",
      "Epoch [40/100], Train Loss: 68145.0859, Val Loss: 67904.9531\n",
      "Epoch [50/100], Train Loss: 67396.4766, Val Loss: 67154.5469\n",
      "Epoch [60/100], Train Loss: 66786.0391, Val Loss: 66552.8047\n",
      "Epoch [70/100], Train Loss: 66318.5781, Val Loss: 66094.6484\n",
      "Epoch [80/100], Train Loss: 65936.4375, Val Loss: 65716.1406\n",
      "Epoch [90/100], Train Loss: 65595.7500, Val Loss: 65375.7227\n",
      "Epoch [100/100], Train Loss: 65278.0273, Val Loss: 65057.3008\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69478.6719, Val Loss: 69318.9609\n",
      "Epoch [20/100], Train Loss: 69378.9844, Val Loss: 69207.4062\n",
      "Epoch [30/100], Train Loss: 69061.5156, Val Loss: 68845.0703\n",
      "Epoch [40/100], Train Loss: 68326.1406, Val Loss: 68069.6250\n",
      "Epoch [50/100], Train Loss: 67513.1094, Val Loss: 67259.7188\n",
      "Epoch [60/100], Train Loss: 66840.1016, Val Loss: 66594.1250\n",
      "Epoch [70/100], Train Loss: 66318.7344, Val Loss: 66084.1016\n",
      "Epoch [80/100], Train Loss: 65901.9062, Val Loss: 65674.3047\n",
      "Epoch [90/100], Train Loss: 65540.0312, Val Loss: 65314.7266\n",
      "Epoch [100/100], Train Loss: 65207.6328, Val Loss: 64982.8633\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69513.0000, Val Loss: 69353.1094\n",
      "Epoch [20/100], Train Loss: 69399.3828, Val Loss: 69223.6719\n",
      "Epoch [30/100], Train Loss: 69024.4922, Val Loss: 68790.4922\n",
      "Epoch [40/100], Train Loss: 68246.0312, Val Loss: 67976.1953\n",
      "Epoch [50/100], Train Loss: 67460.5312, Val Loss: 67194.9219\n",
      "Epoch [60/100], Train Loss: 66830.8672, Val Loss: 66580.7266\n",
      "Epoch [70/100], Train Loss: 66345.6641, Val Loss: 66110.4609\n",
      "Epoch [80/100], Train Loss: 65953.1875, Val Loss: 65724.6719\n",
      "Epoch [90/100], Train Loss: 65605.3438, Val Loss: 65379.5898\n",
      "Epoch [100/100], Train Loss: 65283.1953, Val Loss: 65057.4961\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69533.5391, Val Loss: 69380.7969\n",
      "Epoch [20/100], Train Loss: 69531.4531, Val Loss: 69378.9375\n",
      "Epoch [30/100], Train Loss: 69530.8516, Val Loss: 69378.3750\n",
      "Epoch [40/100], Train Loss: 69530.5547, Val Loss: 69378.0625\n",
      "Epoch [50/100], Train Loss: 69530.4297, Val Loss: 69377.9531\n",
      "Epoch [60/100], Train Loss: 69530.3750, Val Loss: 69377.9062\n",
      "Epoch [70/100], Train Loss: 69530.3594, Val Loss: 69377.8984\n",
      "Epoch [80/100], Train Loss: 69530.3516, Val Loss: 69377.8984\n",
      "Epoch [90/100], Train Loss: 69530.3594, Val Loss: 69377.8984\n",
      "Epoch [100/100], Train Loss: 69530.3672, Val Loss: 69377.8984\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69563.7734, Val Loss: 69411.2266\n",
      "Epoch [20/100], Train Loss: 69561.9766, Val Loss: 69409.6094\n",
      "Epoch [30/100], Train Loss: 69561.4609, Val Loss: 69409.1250\n",
      "Epoch [40/100], Train Loss: 69561.3047, Val Loss: 69408.9688\n",
      "Epoch [50/100], Train Loss: 69561.2109, Val Loss: 69408.9219\n",
      "Epoch [60/100], Train Loss: 69561.2031, Val Loss: 69408.9062\n",
      "Epoch [70/100], Train Loss: 69561.1484, Val Loss: 69408.8984\n",
      "Epoch [80/100], Train Loss: 69561.1953, Val Loss: 69408.8984\n",
      "Epoch [90/100], Train Loss: 69561.1719, Val Loss: 69408.8984\n",
      "Epoch [100/100], Train Loss: 69561.1719, Val Loss: 69408.8984\n",
      "Trying params: hidden_dim=64, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69560.2969, Val Loss: 69407.8125\n",
      "Epoch [20/100], Train Loss: 69558.5938, Val Loss: 69406.2109\n",
      "Epoch [30/100], Train Loss: 69558.0469, Val Loss: 69405.7344\n",
      "Epoch [40/100], Train Loss: 69557.8359, Val Loss: 69405.5859\n",
      "Epoch [50/100], Train Loss: 69557.7891, Val Loss: 69405.5391\n",
      "Epoch [60/100], Train Loss: 69557.7734, Val Loss: 69405.5156\n",
      "Epoch [70/100], Train Loss: 69557.7891, Val Loss: 69405.5156\n",
      "Epoch [80/100], Train Loss: 69557.7578, Val Loss: 69405.5078\n",
      "Epoch [90/100], Train Loss: 69557.8359, Val Loss: 69405.5000\n",
      "Epoch [100/100], Train Loss: 69557.8125, Val Loss: 69405.5000\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 67711.6406, Val Loss: 67245.9375\n",
      "Epoch [20/100], Train Loss: 63752.9648, Val Loss: 63176.5547\n",
      "Epoch [30/100], Train Loss: 58961.0977, Val Loss: 58346.2422\n",
      "Epoch [40/100], Train Loss: 53920.3008, Val Loss: 53266.0117\n",
      "Epoch [50/100], Train Loss: 48901.9609, Val Loss: 48214.2070\n",
      "Epoch [60/100], Train Loss: 44245.6719, Val Loss: 43552.9297\n",
      "Epoch [70/100], Train Loss: 40216.5273, Val Loss: 39544.5664\n",
      "Epoch [80/100], Train Loss: 36830.2617, Val Loss: 36190.2812\n",
      "Epoch [90/100], Train Loss: 33977.4297, Val Loss: 33371.3789\n",
      "Epoch [100/100], Train Loss: 31515.3066, Val Loss: 30950.4609\n",
      "New best score: 30950.4609375 with params: {'hidden_dim': 128, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 67657.6797, Val Loss: 67189.0234\n",
      "Epoch [20/100], Train Loss: 63681.2773, Val Loss: 63106.9531\n",
      "Epoch [30/100], Train Loss: 58890.5664, Val Loss: 58272.5625\n",
      "Epoch [40/100], Train Loss: 53828.6836, Val Loss: 53171.5664\n",
      "Epoch [50/100], Train Loss: 48799.6992, Val Loss: 48111.4297\n",
      "Epoch [60/100], Train Loss: 44157.3203, Val Loss: 43466.4531\n",
      "Epoch [70/100], Train Loss: 40146.4922, Val Loss: 39476.4922\n",
      "Epoch [80/100], Train Loss: 36777.2461, Val Loss: 36138.2852\n",
      "Epoch [90/100], Train Loss: 33939.1016, Val Loss: 33332.4023\n",
      "Epoch [100/100], Train Loss: 31483.9609, Val Loss: 30918.2598\n",
      "New best score: 30918.259765625 with params: {'hidden_dim': 128, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 67942.9219, Val Loss: 67494.2109\n",
      "Epoch [20/100], Train Loss: 64110.0273, Val Loss: 63533.3633\n",
      "Epoch [30/100], Train Loss: 59332.0898, Val Loss: 58705.9141\n",
      "Epoch [40/100], Train Loss: 54241.4922, Val Loss: 53573.5586\n",
      "Epoch [50/100], Train Loss: 49176.4961, Val Loss: 48481.1562\n",
      "Epoch [60/100], Train Loss: 44493.9609, Val Loss: 43798.0312\n",
      "Epoch [70/100], Train Loss: 40430.3164, Val Loss: 39756.1172\n",
      "Epoch [80/100], Train Loss: 37009.1172, Val Loss: 36366.5469\n",
      "Epoch [90/100], Train Loss: 34131.0039, Val Loss: 33522.8359\n",
      "Epoch [100/100], Train Loss: 31648.5488, Val Loss: 31083.5293\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69480.8438, Val Loss: 69315.2422\n",
      "Epoch [20/100], Train Loss: 69343.4141, Val Loss: 69173.5781\n",
      "Epoch [30/100], Train Loss: 69153.5156, Val Loss: 68977.5859\n",
      "Epoch [40/100], Train Loss: 68896.3594, Val Loss: 68713.7422\n",
      "Epoch [50/100], Train Loss: 68565.8750, Val Loss: 68377.1562\n",
      "Epoch [60/100], Train Loss: 68163.7188, Val Loss: 67970.4688\n",
      "Epoch [70/100], Train Loss: 67697.8281, Val Loss: 67502.1953\n",
      "Epoch [80/100], Train Loss: 67180.1797, Val Loss: 66984.3828\n",
      "Epoch [90/100], Train Loss: 66623.7344, Val Loss: 66429.7500\n",
      "Epoch [100/100], Train Loss: 66040.2578, Val Loss: 65849.5312\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69478.3516, Val Loss: 69313.1719\n",
      "Epoch [20/100], Train Loss: 69339.2031, Val Loss: 69169.9766\n",
      "Epoch [30/100], Train Loss: 69147.8359, Val Loss: 68972.8125\n",
      "Epoch [40/100], Train Loss: 68888.7344, Val Loss: 68707.4609\n",
      "Epoch [50/100], Train Loss: 68555.8203, Val Loss: 68368.9922\n",
      "Epoch [60/100], Train Loss: 68151.5391, Val Loss: 67960.7188\n",
      "Epoch [70/100], Train Loss: 67684.6719, Val Loss: 67491.8906\n",
      "Epoch [80/100], Train Loss: 67167.3047, Val Loss: 66974.6562\n",
      "Epoch [90/100], Train Loss: 66612.0078, Val Loss: 66421.2891\n",
      "Epoch [100/100], Train Loss: 66029.9844, Val Loss: 65842.5312\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69445.1172, Val Loss: 69280.9297\n",
      "Epoch [20/100], Train Loss: 69313.8594, Val Loss: 69145.7656\n",
      "Epoch [30/100], Train Loss: 69133.1094, Val Loss: 68959.1250\n",
      "Epoch [40/100], Train Loss: 68885.5859, Val Loss: 68704.9297\n",
      "Epoch [50/100], Train Loss: 68563.7031, Val Loss: 68376.8125\n",
      "Epoch [60/100], Train Loss: 68168.5312, Val Loss: 67976.8438\n",
      "Epoch [70/100], Train Loss: 67707.9688, Val Loss: 67513.5859\n",
      "Epoch [80/100], Train Loss: 67194.0469, Val Loss: 66999.1641\n",
      "Epoch [90/100], Train Loss: 66639.8594, Val Loss: 66446.3906\n",
      "Epoch [100/100], Train Loss: 66057.3828, Val Loss: 65866.7969\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69552.4141, Val Loss: 69399.9922\n",
      "Epoch [20/100], Train Loss: 69548.1875, Val Loss: 69396.0703\n",
      "Epoch [30/100], Train Loss: 69546.8438, Val Loss: 69394.9375\n",
      "Epoch [40/100], Train Loss: 69546.4062, Val Loss: 69394.5547\n",
      "Epoch [50/100], Train Loss: 69546.2734, Val Loss: 69394.4375\n",
      "Epoch [60/100], Train Loss: 69546.2344, Val Loss: 69394.3984\n",
      "Epoch [70/100], Train Loss: 69546.2109, Val Loss: 69394.3906\n",
      "Epoch [80/100], Train Loss: 69546.2109, Val Loss: 69394.3750\n",
      "Epoch [90/100], Train Loss: 69546.2109, Val Loss: 69394.3750\n",
      "Epoch [100/100], Train Loss: 69546.2109, Val Loss: 69394.3750\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69522.9062, Val Loss: 69371.0156\n",
      "Epoch [20/100], Train Loss: 69518.6484, Val Loss: 69367.0391\n",
      "Epoch [30/100], Train Loss: 69517.1953, Val Loss: 69365.7734\n",
      "Epoch [40/100], Train Loss: 69516.7266, Val Loss: 69365.3594\n",
      "Epoch [50/100], Train Loss: 69516.5859, Val Loss: 69365.2344\n",
      "Epoch [60/100], Train Loss: 69516.5469, Val Loss: 69365.1953\n",
      "Epoch [70/100], Train Loss: 69516.5312, Val Loss: 69365.1875\n",
      "Epoch [80/100], Train Loss: 69516.5234, Val Loss: 69365.1875\n",
      "Epoch [90/100], Train Loss: 69516.5234, Val Loss: 69365.1719\n",
      "Epoch [100/100], Train Loss: 69516.5234, Val Loss: 69365.1719\n",
      "Trying params: hidden_dim=128, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69519.5156, Val Loss: 69368.2422\n",
      "Epoch [20/100], Train Loss: 69515.3281, Val Loss: 69364.3281\n",
      "Epoch [30/100], Train Loss: 69513.8750, Val Loss: 69363.0781\n",
      "Epoch [40/100], Train Loss: 69513.4141, Val Loss: 69362.6641\n",
      "Epoch [50/100], Train Loss: 69513.2812, Val Loss: 69362.5469\n",
      "Epoch [60/100], Train Loss: 69513.2344, Val Loss: 69362.5078\n",
      "Epoch [70/100], Train Loss: 69513.2266, Val Loss: 69362.4922\n",
      "Epoch [80/100], Train Loss: 69513.2266, Val Loss: 69362.4922\n",
      "Epoch [90/100], Train Loss: 69513.2109, Val Loss: 69362.4922\n",
      "Epoch [100/100], Train Loss: 69513.2266, Val Loss: 69362.4922\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 65814.5312, Val Loss: 65051.7422\n",
      "Epoch [20/100], Train Loss: 60131.7852, Val Loss: 59376.8125\n",
      "Epoch [30/100], Train Loss: 55425.4492, Val Loss: 54704.3164\n",
      "Epoch [40/100], Train Loss: 51267.6641, Val Loss: 50550.7500\n",
      "Epoch [50/100], Train Loss: 47569.3242, Val Loss: 46876.0664\n",
      "Epoch [60/100], Train Loss: 44273.6836, Val Loss: 43605.7656\n",
      "Epoch [70/100], Train Loss: 41299.8320, Val Loss: 40642.7148\n",
      "Epoch [80/100], Train Loss: 38524.4531, Val Loss: 37916.3398\n",
      "Epoch [90/100], Train Loss: 36033.4375, Val Loss: 35469.8438\n",
      "Epoch [100/100], Train Loss: 33749.4258, Val Loss: 33245.5195\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 65814.7109, Val Loss: 65047.1875\n",
      "Epoch [20/100], Train Loss: 60146.7695, Val Loss: 59389.6289\n",
      "Epoch [30/100], Train Loss: 55452.3242, Val Loss: 54731.2656\n",
      "Epoch [40/100], Train Loss: 51298.8789, Val Loss: 50581.4258\n",
      "Epoch [50/100], Train Loss: 47604.7266, Val Loss: 46913.8242\n",
      "Epoch [60/100], Train Loss: 44306.0391, Val Loss: 43642.6914\n",
      "Epoch [70/100], Train Loss: 41323.6289, Val Loss: 40665.6094\n",
      "Epoch [80/100], Train Loss: 38555.6445, Val Loss: 37954.1172\n",
      "Epoch [90/100], Train Loss: 36078.0039, Val Loss: 35508.1328\n",
      "Epoch [100/100], Train Loss: 33815.0938, Val Loss: 33303.6719\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 65669.5234, Val Loss: 64895.6992\n",
      "Epoch [20/100], Train Loss: 60037.0977, Val Loss: 59280.0820\n",
      "Epoch [30/100], Train Loss: 55383.4844, Val Loss: 54665.4922\n",
      "Epoch [40/100], Train Loss: 51280.7578, Val Loss: 50563.2734\n",
      "Epoch [50/100], Train Loss: 47614.2461, Val Loss: 46929.6289\n",
      "Epoch [60/100], Train Loss: 44344.2500, Val Loss: 43684.1758\n",
      "Epoch [70/100], Train Loss: 41391.2852, Val Loss: 40725.3281\n",
      "Epoch [80/100], Train Loss: 38649.3789, Val Loss: 38024.4609\n",
      "Epoch [90/100], Train Loss: 36166.5664, Val Loss: 35592.2852\n",
      "Epoch [100/100], Train Loss: 33912.3516, Val Loss: 33396.0273\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69460.1797, Val Loss: 69288.3750\n",
      "Epoch [20/100], Train Loss: 69177.8750, Val Loss: 68975.1797\n",
      "Epoch [30/100], Train Loss: 68558.7188, Val Loss: 68320.6016\n",
      "Epoch [40/100], Train Loss: 67647.7500, Val Loss: 67397.0938\n",
      "Epoch [50/100], Train Loss: 66648.5000, Val Loss: 66399.2031\n",
      "Epoch [60/100], Train Loss: 65681.0469, Val Loss: 65430.6562\n",
      "Epoch [70/100], Train Loss: 64756.7148, Val Loss: 64492.7109\n",
      "Epoch [80/100], Train Loss: 63859.6680, Val Loss: 63584.0938\n",
      "Epoch [90/100], Train Loss: 63027.8945, Val Loss: 62745.5859\n",
      "Epoch [100/100], Train Loss: 62270.6133, Val Loss: 61984.0586\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69444.1719, Val Loss: 69275.5547\n",
      "Epoch [20/100], Train Loss: 69167.2969, Val Loss: 68972.8672\n",
      "Epoch [30/100], Train Loss: 68546.2344, Val Loss: 68314.7031\n",
      "Epoch [40/100], Train Loss: 67606.7891, Val Loss: 67360.4062\n",
      "Epoch [50/100], Train Loss: 66579.4922, Val Loss: 66333.0938\n",
      "Epoch [60/100], Train Loss: 65596.2188, Val Loss: 65345.4688\n",
      "Epoch [70/100], Train Loss: 64657.8711, Val Loss: 64391.2305\n",
      "Epoch [80/100], Train Loss: 63758.6445, Val Loss: 63479.2188\n",
      "Epoch [90/100], Train Loss: 62929.3594, Val Loss: 62641.2734\n",
      "Epoch [100/100], Train Loss: 62173.3477, Val Loss: 61879.1758\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69440.2344, Val Loss: 69267.2578\n",
      "Epoch [20/100], Train Loss: 69111.9531, Val Loss: 68907.2891\n",
      "Epoch [30/100], Train Loss: 68412.8438, Val Loss: 68166.8203\n",
      "Epoch [40/100], Train Loss: 67409.8438, Val Loss: 67150.6562\n",
      "Epoch [50/100], Train Loss: 66344.3125, Val Loss: 66092.4531\n",
      "Epoch [60/100], Train Loss: 65353.0352, Val Loss: 65093.8125\n",
      "Epoch [70/100], Train Loss: 64420.2656, Val Loss: 64141.7734\n",
      "Epoch [80/100], Train Loss: 63533.5859, Val Loss: 63243.5352\n",
      "Epoch [90/100], Train Loss: 62729.9766, Val Loss: 62432.2188\n",
      "Epoch [100/100], Train Loss: 61987.6328, Val Loss: 61685.2539\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69543.2578, Val Loss: 69390.3906\n",
      "Epoch [20/100], Train Loss: 69539.1953, Val Loss: 69386.4375\n",
      "Epoch [30/100], Train Loss: 69537.6328, Val Loss: 69385.0703\n",
      "Epoch [40/100], Train Loss: 69537.2109, Val Loss: 69384.6172\n",
      "Epoch [50/100], Train Loss: 69536.9453, Val Loss: 69384.4922\n",
      "Epoch [60/100], Train Loss: 69537.0625, Val Loss: 69384.4609\n",
      "Epoch [70/100], Train Loss: 69536.9688, Val Loss: 69384.4375\n",
      "Epoch [80/100], Train Loss: 69536.9531, Val Loss: 69384.4297\n",
      "Epoch [90/100], Train Loss: 69537.0078, Val Loss: 69384.4297\n",
      "Epoch [100/100], Train Loss: 69536.8750, Val Loss: 69384.4297\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69550.9531, Val Loss: 69398.1406\n",
      "Epoch [20/100], Train Loss: 69546.8828, Val Loss: 69394.3438\n",
      "Epoch [30/100], Train Loss: 69545.3125, Val Loss: 69393.0312\n",
      "Epoch [40/100], Train Loss: 69544.9219, Val Loss: 69392.6016\n",
      "Epoch [50/100], Train Loss: 69544.7734, Val Loss: 69392.4844\n",
      "Epoch [60/100], Train Loss: 69544.7031, Val Loss: 69392.4453\n",
      "Epoch [70/100], Train Loss: 69544.6797, Val Loss: 69392.4297\n",
      "Epoch [80/100], Train Loss: 69544.5781, Val Loss: 69392.4219\n",
      "Epoch [90/100], Train Loss: 69544.7500, Val Loss: 69392.4219\n",
      "Epoch [100/100], Train Loss: 69544.6641, Val Loss: 69392.4219\n",
      "Trying params: hidden_dim=128, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69539.5312, Val Loss: 69386.3594\n",
      "Epoch [20/100], Train Loss: 69535.6562, Val Loss: 69382.6094\n",
      "Epoch [30/100], Train Loss: 69534.2578, Val Loss: 69381.4141\n",
      "Epoch [40/100], Train Loss: 69533.8516, Val Loss: 69381.0312\n",
      "Epoch [50/100], Train Loss: 69533.9062, Val Loss: 69380.9141\n",
      "Epoch [60/100], Train Loss: 69533.7500, Val Loss: 69380.8750\n",
      "Epoch [70/100], Train Loss: 69533.8984, Val Loss: 69380.8594\n",
      "Epoch [80/100], Train Loss: 69533.5547, Val Loss: 69380.8594\n",
      "Epoch [90/100], Train Loss: 69533.5859, Val Loss: 69380.8594\n",
      "Epoch [100/100], Train Loss: 69533.7578, Val Loss: 69380.8516\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 64657.0898, Val Loss: 63896.6992\n",
      "Epoch [20/100], Train Loss: 59818.8945, Val Loss: 59126.1055\n",
      "Epoch [30/100], Train Loss: 55486.3594, Val Loss: 54786.3516\n",
      "Epoch [40/100], Train Loss: 51560.4180, Val Loss: 50858.9180\n",
      "Epoch [50/100], Train Loss: 48117.9102, Val Loss: 47417.9609\n",
      "Epoch [60/100], Train Loss: 45120.0859, Val Loss: 44411.4570\n",
      "Epoch [70/100], Train Loss: 42437.0977, Val Loss: 41697.7539\n",
      "Epoch [80/100], Train Loss: 39665.3828, Val Loss: 39035.4102\n",
      "Epoch [90/100], Train Loss: 37214.1484, Val Loss: 36602.0820\n",
      "Epoch [100/100], Train Loss: 34867.3477, Val Loss: 34276.4297\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 64806.2070, Val Loss: 64043.6680\n",
      "Epoch [20/100], Train Loss: 59959.3867, Val Loss: 59263.6328\n",
      "Epoch [30/100], Train Loss: 55594.9961, Val Loss: 54888.6992\n",
      "Epoch [40/100], Train Loss: 51656.2578, Val Loss: 50952.4922\n",
      "Epoch [50/100], Train Loss: 48147.2773, Val Loss: 47440.5312\n",
      "Epoch [60/100], Train Loss: 44916.4102, Val Loss: 44226.5820\n",
      "Epoch [70/100], Train Loss: 42014.2461, Val Loss: 41376.8203\n",
      "Epoch [80/100], Train Loss: 39388.7070, Val Loss: 38749.3828\n",
      "Epoch [90/100], Train Loss: 36904.1445, Val Loss: 36246.1172\n",
      "Epoch [100/100], Train Loss: 34526.1484, Val Loss: 33978.7031\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 64813.3828, Val Loss: 64060.5195\n",
      "Epoch [20/100], Train Loss: 59979.6992, Val Loss: 59287.5703\n",
      "Epoch [30/100], Train Loss: 55634.1328, Val Loss: 54930.3633\n",
      "Epoch [40/100], Train Loss: 51704.2188, Val Loss: 51002.5273\n",
      "Epoch [50/100], Train Loss: 48211.1445, Val Loss: 47514.8750\n",
      "Epoch [60/100], Train Loss: 45002.2305, Val Loss: 44317.9219\n",
      "Epoch [70/100], Train Loss: 42087.7344, Val Loss: 41459.4883\n",
      "Epoch [80/100], Train Loss: 39480.2070, Val Loss: 38859.0078\n",
      "Epoch [90/100], Train Loss: 37001.3438, Val Loss: 36356.3320\n",
      "Epoch [100/100], Train Loss: 34656.5859, Val Loss: 34083.4219\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69423.7188, Val Loss: 69248.7500\n",
      "Epoch [20/100], Train Loss: 68961.3906, Val Loss: 68718.1094\n",
      "Epoch [30/100], Train Loss: 67806.7891, Val Loss: 67524.8125\n",
      "Epoch [40/100], Train Loss: 66594.0781, Val Loss: 66318.5781\n",
      "Epoch [50/100], Train Loss: 65565.0859, Val Loss: 65284.3164\n",
      "Epoch [60/100], Train Loss: 64704.6602, Val Loss: 64421.8398\n",
      "Epoch [70/100], Train Loss: 63963.1133, Val Loss: 63691.0195\n",
      "Epoch [80/100], Train Loss: 63310.4492, Val Loss: 63040.5391\n",
      "Epoch [90/100], Train Loss: 62711.8008, Val Loss: 62441.8438\n",
      "Epoch [100/100], Train Loss: 62151.4062, Val Loss: 61879.8398\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69451.6719, Val Loss: 69276.7344\n",
      "Epoch [20/100], Train Loss: 68995.9844, Val Loss: 68752.8828\n",
      "Epoch [30/100], Train Loss: 67857.2891, Val Loss: 67568.6875\n",
      "Epoch [40/100], Train Loss: 66606.7266, Val Loss: 66317.8125\n",
      "Epoch [50/100], Train Loss: 65530.8203, Val Loss: 65240.9922\n",
      "Epoch [60/100], Train Loss: 64659.4492, Val Loss: 64373.4727\n",
      "Epoch [70/100], Train Loss: 63918.6094, Val Loss: 63640.6836\n",
      "Epoch [80/100], Train Loss: 63268.2344, Val Loss: 62994.0195\n",
      "Epoch [90/100], Train Loss: 62670.6094, Val Loss: 62396.3828\n",
      "Epoch [100/100], Train Loss: 62102.3516, Val Loss: 61826.6367\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69459.2891, Val Loss: 69285.9922\n",
      "Epoch [20/100], Train Loss: 69029.2812, Val Loss: 68789.8359\n",
      "Epoch [30/100], Train Loss: 67889.9453, Val Loss: 67591.6094\n",
      "Epoch [40/100], Train Loss: 66605.1406, Val Loss: 66310.3672\n",
      "Epoch [50/100], Train Loss: 65506.5273, Val Loss: 65209.8438\n",
      "Epoch [60/100], Train Loss: 64614.5312, Val Loss: 64322.3945\n",
      "Epoch [70/100], Train Loss: 63860.1797, Val Loss: 63582.6953\n",
      "Epoch [80/100], Train Loss: 63200.5195, Val Loss: 62924.4688\n",
      "Epoch [90/100], Train Loss: 62595.8203, Val Loss: 62320.0117\n",
      "Epoch [100/100], Train Loss: 62030.1328, Val Loss: 61754.3047\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69515.7812, Val Loss: 69362.6094\n",
      "Epoch [20/100], Train Loss: 69511.7422, Val Loss: 69358.8359\n",
      "Epoch [30/100], Train Loss: 69510.2812, Val Loss: 69357.5234\n",
      "Epoch [40/100], Train Loss: 69509.8047, Val Loss: 69357.1016\n",
      "Epoch [50/100], Train Loss: 69509.6797, Val Loss: 69356.9766\n",
      "Epoch [60/100], Train Loss: 69509.6719, Val Loss: 69356.9297\n",
      "Epoch [70/100], Train Loss: 69509.6250, Val Loss: 69356.9141\n",
      "Epoch [80/100], Train Loss: 69509.6172, Val Loss: 69356.9141\n",
      "Epoch [90/100], Train Loss: 69509.6406, Val Loss: 69356.9141\n",
      "Epoch [100/100], Train Loss: 69509.6719, Val Loss: 69356.9141\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69564.9766, Val Loss: 69412.2812\n",
      "Epoch [20/100], Train Loss: 69561.0547, Val Loss: 69408.4375\n",
      "Epoch [30/100], Train Loss: 69559.5781, Val Loss: 69407.0938\n",
      "Epoch [40/100], Train Loss: 69559.0156, Val Loss: 69406.6641\n",
      "Epoch [50/100], Train Loss: 69558.8828, Val Loss: 69406.5312\n",
      "Epoch [60/100], Train Loss: 69558.9375, Val Loss: 69406.5000\n",
      "Epoch [70/100], Train Loss: 69558.9219, Val Loss: 69406.4844\n",
      "Epoch [80/100], Train Loss: 69558.8359, Val Loss: 69406.4844\n",
      "Epoch [90/100], Train Loss: 69558.8516, Val Loss: 69406.4844\n",
      "Epoch [100/100], Train Loss: 69558.8750, Val Loss: 69406.4844\n",
      "Trying params: hidden_dim=128, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69556.3438, Val Loss: 69403.6250\n",
      "Epoch [20/100], Train Loss: 69552.2578, Val Loss: 69399.7812\n",
      "Epoch [30/100], Train Loss: 69550.8359, Val Loss: 69398.4219\n",
      "Epoch [40/100], Train Loss: 69550.2969, Val Loss: 69397.9922\n",
      "Epoch [50/100], Train Loss: 69550.1016, Val Loss: 69397.8594\n",
      "Epoch [60/100], Train Loss: 69550.1484, Val Loss: 69397.8125\n",
      "Epoch [70/100], Train Loss: 69550.0078, Val Loss: 69397.8047\n",
      "Epoch [80/100], Train Loss: 69550.0781, Val Loss: 69397.8047\n",
      "Epoch [90/100], Train Loss: 69550.1016, Val Loss: 69397.8047\n",
      "Epoch [100/100], Train Loss: 69550.1016, Val Loss: 69397.7969\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 64614.5781, Val Loss: 63931.9219\n",
      "Epoch [20/100], Train Loss: 60052.9766, Val Loss: 59368.7812\n",
      "Epoch [30/100], Train Loss: 55790.2422, Val Loss: 55101.2383\n",
      "Epoch [40/100], Train Loss: 51977.9766, Val Loss: 51293.5820\n",
      "Epoch [50/100], Train Loss: 48628.8477, Val Loss: 47945.4180\n",
      "Epoch [60/100], Train Loss: 45713.4258, Val Loss: 45028.4375\n",
      "Epoch [70/100], Train Loss: 43190.9531, Val Loss: 42500.8281\n",
      "Epoch [80/100], Train Loss: 41020.5508, Val Loss: 40323.1172\n",
      "Epoch [90/100], Train Loss: 39176.8828, Val Loss: 38460.6211\n",
      "Epoch [100/100], Train Loss: 37602.1289, Val Loss: 36873.3359\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 64397.1953, Val Loss: 63719.3594\n",
      "Epoch [20/100], Train Loss: 59826.3516, Val Loss: 59143.3086\n",
      "Epoch [30/100], Train Loss: 55564.7305, Val Loss: 54875.6914\n",
      "Epoch [40/100], Train Loss: 51755.2227, Val Loss: 51065.5352\n",
      "Epoch [50/100], Train Loss: 48409.2891, Val Loss: 47721.8398\n",
      "Epoch [60/100], Train Loss: 45502.6797, Val Loss: 44813.5391\n",
      "Epoch [70/100], Train Loss: 42994.7188, Val Loss: 42302.8047\n",
      "Epoch [80/100], Train Loss: 40837.1289, Val Loss: 40135.8086\n",
      "Epoch [90/100], Train Loss: 38994.9883, Val Loss: 38285.1289\n",
      "Epoch [100/100], Train Loss: 37433.8750, Val Loss: 36712.8398\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 64507.6953, Val Loss: 63832.4922\n",
      "Epoch [20/100], Train Loss: 60048.5781, Val Loss: 59374.2383\n",
      "Epoch [30/100], Train Loss: 55871.2500, Val Loss: 55191.8086\n",
      "Epoch [40/100], Train Loss: 52126.3945, Val Loss: 51449.0898\n",
      "Epoch [50/100], Train Loss: 48830.7812, Val Loss: 48150.6211\n",
      "Epoch [60/100], Train Loss: 45954.2695, Val Loss: 45273.2930\n",
      "Epoch [70/100], Train Loss: 43455.9727, Val Loss: 42772.0547\n",
      "Epoch [80/100], Train Loss: 41298.6367, Val Loss: 40607.8594\n",
      "Epoch [90/100], Train Loss: 39446.3086, Val Loss: 38746.4492\n",
      "Epoch [100/100], Train Loss: 37866.7734, Val Loss: 37154.5039\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69461.7891, Val Loss: 69288.5000\n",
      "Epoch [20/100], Train Loss: 68913.4609, Val Loss: 68635.5312\n",
      "Epoch [30/100], Train Loss: 67411.3516, Val Loss: 67107.7031\n",
      "Epoch [40/100], Train Loss: 66151.3438, Val Loss: 65860.3125\n",
      "Epoch [50/100], Train Loss: 65245.3945, Val Loss: 64974.6289\n",
      "Epoch [60/100], Train Loss: 64538.5625, Val Loss: 64276.3438\n",
      "Epoch [70/100], Train Loss: 63916.8555, Val Loss: 63656.7578\n",
      "Epoch [80/100], Train Loss: 63339.4922, Val Loss: 63077.8281\n",
      "Epoch [90/100], Train Loss: 62790.2070, Val Loss: 62525.7773\n",
      "Epoch [100/100], Train Loss: 62261.9375, Val Loss: 61994.1211\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69423.0625, Val Loss: 69247.3125\n",
      "Epoch [20/100], Train Loss: 68828.0703, Val Loss: 68542.2188\n",
      "Epoch [30/100], Train Loss: 67351.5078, Val Loss: 67045.4531\n",
      "Epoch [40/100], Train Loss: 66112.3828, Val Loss: 65818.9688\n",
      "Epoch [50/100], Train Loss: 65210.2812, Val Loss: 64935.0117\n",
      "Epoch [60/100], Train Loss: 64492.5625, Val Loss: 64227.6719\n",
      "Epoch [70/100], Train Loss: 63866.7227, Val Loss: 63604.5156\n",
      "Epoch [80/100], Train Loss: 63288.2305, Val Loss: 63025.0352\n",
      "Epoch [90/100], Train Loss: 62738.3906, Val Loss: 62473.0781\n",
      "Epoch [100/100], Train Loss: 62210.2617, Val Loss: 61941.4883\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69467.4922, Val Loss: 69293.7188\n",
      "Epoch [20/100], Train Loss: 68924.5391, Val Loss: 68643.0625\n",
      "Epoch [30/100], Train Loss: 67492.1172, Val Loss: 67176.2656\n",
      "Epoch [40/100], Train Loss: 66236.6172, Val Loss: 65932.5391\n",
      "Epoch [50/100], Train Loss: 65312.2422, Val Loss: 65034.6836\n",
      "Epoch [60/100], Train Loss: 64598.4375, Val Loss: 64334.0625\n",
      "Epoch [70/100], Train Loss: 63975.3828, Val Loss: 63713.2148\n",
      "Epoch [80/100], Train Loss: 63394.8008, Val Loss: 63131.3242\n",
      "Epoch [90/100], Train Loss: 62842.6523, Val Loss: 62576.5625\n",
      "Epoch [100/100], Train Loss: 62311.8086, Val Loss: 62042.7578\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69546.4141, Val Loss: 69393.5938\n",
      "Epoch [20/100], Train Loss: 69542.2734, Val Loss: 69389.6406\n",
      "Epoch [30/100], Train Loss: 69540.6172, Val Loss: 69388.0781\n",
      "Epoch [40/100], Train Loss: 69540.0859, Val Loss: 69387.6250\n",
      "Epoch [50/100], Train Loss: 69539.9141, Val Loss: 69387.4609\n",
      "Epoch [60/100], Train Loss: 69539.8516, Val Loss: 69387.4141\n",
      "Epoch [70/100], Train Loss: 69539.8125, Val Loss: 69387.4062\n",
      "Epoch [80/100], Train Loss: 69539.8125, Val Loss: 69387.3984\n",
      "Epoch [90/100], Train Loss: 69539.8359, Val Loss: 69387.3984\n",
      "Epoch [100/100], Train Loss: 69539.8359, Val Loss: 69387.3984\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69513.7031, Val Loss: 69360.5859\n",
      "Epoch [20/100], Train Loss: 69509.6797, Val Loss: 69356.6953\n",
      "Epoch [30/100], Train Loss: 69507.9844, Val Loss: 69355.1328\n",
      "Epoch [40/100], Train Loss: 69507.4375, Val Loss: 69354.6797\n",
      "Epoch [50/100], Train Loss: 69507.2812, Val Loss: 69354.5156\n",
      "Epoch [60/100], Train Loss: 69507.2500, Val Loss: 69354.4688\n",
      "Epoch [70/100], Train Loss: 69507.2031, Val Loss: 69354.4531\n",
      "Epoch [80/100], Train Loss: 69507.2031, Val Loss: 69354.4531\n",
      "Epoch [90/100], Train Loss: 69507.2109, Val Loss: 69354.4531\n",
      "Epoch [100/100], Train Loss: 69507.1953, Val Loss: 69354.4453\n",
      "Trying params: hidden_dim=128, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69551.7344, Val Loss: 69398.9219\n",
      "Epoch [20/100], Train Loss: 69547.5469, Val Loss: 69394.9297\n",
      "Epoch [30/100], Train Loss: 69545.8750, Val Loss: 69393.3281\n",
      "Epoch [40/100], Train Loss: 69545.2578, Val Loss: 69392.8594\n",
      "Epoch [50/100], Train Loss: 69545.1172, Val Loss: 69392.7109\n",
      "Epoch [60/100], Train Loss: 69545.0859, Val Loss: 69392.6484\n",
      "Epoch [70/100], Train Loss: 69545.0547, Val Loss: 69392.6406\n",
      "Epoch [80/100], Train Loss: 69545.0547, Val Loss: 69392.6328\n",
      "Epoch [90/100], Train Loss: 69545.0547, Val Loss: 69392.6328\n",
      "Epoch [100/100], Train Loss: 69545.0312, Val Loss: 69392.6328\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 66395.9688, Val Loss: 65660.8203\n",
      "Epoch [20/100], Train Loss: 58971.8867, Val Loss: 58046.0195\n",
      "Epoch [30/100], Train Loss: 50352.9336, Val Loss: 49393.6875\n",
      "Epoch [40/100], Train Loss: 41894.5781, Val Loss: 40937.0859\n",
      "Epoch [50/100], Train Loss: 34331.3047, Val Loss: 33433.1797\n",
      "Epoch [60/100], Train Loss: 28410.4004, Val Loss: 27639.1836\n",
      "Epoch [70/100], Train Loss: 24149.2656, Val Loss: 23506.5977\n",
      "Epoch [80/100], Train Loss: 20888.6895, Val Loss: 20364.3359\n",
      "Epoch [90/100], Train Loss: 18249.5215, Val Loss: 17800.4258\n",
      "Epoch [100/100], Train Loss: 16121.8242, Val Loss: 15755.9023\n",
      "New best score: 15755.90234375 with params: {'hidden_dim': 256, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 66329.5312, Val Loss: 65593.3281\n",
      "Epoch [20/100], Train Loss: 58847.7422, Val Loss: 57923.9805\n",
      "Epoch [30/100], Train Loss: 50217.6914, Val Loss: 49264.7695\n",
      "Epoch [40/100], Train Loss: 41782.9883, Val Loss: 40830.5742\n",
      "Epoch [50/100], Train Loss: 34252.2148, Val Loss: 33355.3945\n",
      "Epoch [60/100], Train Loss: 28365.7754, Val Loss: 27595.3047\n",
      "Epoch [70/100], Train Loss: 24138.0957, Val Loss: 23496.9512\n",
      "Epoch [80/100], Train Loss: 20899.6738, Val Loss: 20374.5273\n",
      "Epoch [90/100], Train Loss: 18273.1230, Val Loss: 17824.8652\n",
      "Epoch [100/100], Train Loss: 16151.0518, Val Loss: 15783.5537\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 66513.5703, Val Loss: 65789.6797\n",
      "Epoch [20/100], Train Loss: 59158.1133, Val Loss: 58230.5977\n",
      "Epoch [30/100], Train Loss: 50526.2109, Val Loss: 49562.5352\n",
      "Epoch [40/100], Train Loss: 42039.4336, Val Loss: 41076.0000\n",
      "Epoch [50/100], Train Loss: 34445.5430, Val Loss: 33539.9023\n",
      "Epoch [60/100], Train Loss: 28498.2207, Val Loss: 27722.5742\n",
      "Epoch [70/100], Train Loss: 24223.1172, Val Loss: 23580.3320\n",
      "Epoch [80/100], Train Loss: 20942.3750, Val Loss: 20418.8340\n",
      "Epoch [90/100], Train Loss: 18288.4258, Val Loss: 17843.7227\n",
      "Epoch [100/100], Train Loss: 16159.5137, Val Loss: 15796.9854\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69421.5781, Val Loss: 69252.6250\n",
      "Epoch [20/100], Train Loss: 69212.8281, Val Loss: 69035.0312\n",
      "Epoch [30/100], Train Loss: 68887.9922, Val Loss: 68698.2734\n",
      "Epoch [40/100], Train Loss: 68417.6484, Val Loss: 68215.2266\n",
      "Epoch [50/100], Train Loss: 67791.5391, Val Loss: 67578.0625\n",
      "Epoch [60/100], Train Loss: 67018.9688, Val Loss: 66798.3438\n",
      "Epoch [70/100], Train Loss: 66123.5859, Val Loss: 65900.6953\n",
      "Epoch [80/100], Train Loss: 65134.7930, Val Loss: 64914.1328\n",
      "Epoch [90/100], Train Loss: 64080.9766, Val Loss: 63865.9258\n",
      "Epoch [100/100], Train Loss: 62986.0469, Val Loss: 62778.6953\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69415.2109, Val Loss: 69244.4062\n",
      "Epoch [20/100], Train Loss: 69190.9531, Val Loss: 69011.1328\n",
      "Epoch [30/100], Train Loss: 68849.1172, Val Loss: 68657.2344\n",
      "Epoch [40/100], Train Loss: 68360.9062, Val Loss: 68156.2578\n",
      "Epoch [50/100], Train Loss: 67716.9297, Val Loss: 67501.3047\n",
      "Epoch [60/100], Train Loss: 66927.6484, Val Loss: 66704.8594\n",
      "Epoch [70/100], Train Loss: 66017.5625, Val Loss: 65792.5234\n",
      "Epoch [80/100], Train Loss: 65016.6758, Val Loss: 64793.9609\n",
      "Epoch [90/100], Train Loss: 63953.3320, Val Loss: 63736.3125\n",
      "Epoch [100/100], Train Loss: 62851.0156, Val Loss: 62641.7305\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69414.5781, Val Loss: 69245.6641\n",
      "Epoch [20/100], Train Loss: 69202.2891, Val Loss: 69024.8984\n",
      "Epoch [30/100], Train Loss: 68875.2578, Val Loss: 68686.1172\n",
      "Epoch [40/100], Train Loss: 68402.2812, Val Loss: 68200.3750\n",
      "Epoch [50/100], Train Loss: 67771.9297, Val Loss: 67558.7578\n",
      "Epoch [60/100], Train Loss: 66993.0938, Val Loss: 66772.3125\n",
      "Epoch [70/100], Train Loss: 66090.0625, Val Loss: 65866.5078\n",
      "Epoch [80/100], Train Loss: 65093.3750, Val Loss: 64871.8516\n",
      "Epoch [90/100], Train Loss: 64032.1680, Val Loss: 63816.4219\n",
      "Epoch [100/100], Train Loss: 62930.7461, Val Loss: 62722.9062\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69547.3438, Val Loss: 69393.8438\n",
      "Epoch [20/100], Train Loss: 69532.1094, Val Loss: 69378.6484\n",
      "Epoch [30/100], Train Loss: 69516.5703, Val Loss: 69363.1094\n",
      "Epoch [40/100], Train Loss: 69500.4766, Val Loss: 69347.0234\n",
      "Epoch [50/100], Train Loss: 69483.6406, Val Loss: 69330.1875\n",
      "Epoch [60/100], Train Loss: 69465.8359, Val Loss: 69312.3750\n",
      "Epoch [70/100], Train Loss: 69446.8906, Val Loss: 69293.4141\n",
      "Epoch [80/100], Train Loss: 69426.6328, Val Loss: 69273.1328\n",
      "Epoch [90/100], Train Loss: 69404.9062, Val Loss: 69251.3750\n",
      "Epoch [100/100], Train Loss: 69381.5625, Val Loss: 69227.9922\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69533.2422, Val Loss: 69378.7422\n",
      "Epoch [20/100], Train Loss: 69517.5547, Val Loss: 69363.0469\n",
      "Epoch [30/100], Train Loss: 69501.5547, Val Loss: 69347.0391\n",
      "Epoch [40/100], Train Loss: 69485.0703, Val Loss: 69330.5156\n",
      "Epoch [50/100], Train Loss: 69467.8828, Val Loss: 69313.2578\n",
      "Epoch [60/100], Train Loss: 69449.7656, Val Loss: 69295.0703\n",
      "Epoch [70/100], Train Loss: 69430.5547, Val Loss: 69275.7500\n",
      "Epoch [80/100], Train Loss: 69410.0625, Val Loss: 69255.1406\n",
      "Epoch [90/100], Train Loss: 69388.1328, Val Loss: 69233.0938\n",
      "Epoch [100/100], Train Loss: 69364.6250, Val Loss: 69209.4609\n",
      "Trying params: hidden_dim=256, num_layers=1, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69514.3672, Val Loss: 69360.0547\n",
      "Epoch [20/100], Train Loss: 69499.2344, Val Loss: 69344.9766\n",
      "Epoch [30/100], Train Loss: 69483.8281, Val Loss: 69329.6016\n",
      "Epoch [40/100], Train Loss: 69467.9531, Val Loss: 69313.7266\n",
      "Epoch [50/100], Train Loss: 69451.3359, Val Loss: 69297.1016\n",
      "Epoch [60/100], Train Loss: 69433.7578, Val Loss: 69279.4844\n",
      "Epoch [70/100], Train Loss: 69414.9766, Val Loss: 69260.6484\n",
      "Epoch [80/100], Train Loss: 69394.8359, Val Loss: 69240.4375\n",
      "Epoch [90/100], Train Loss: 69373.1484, Val Loss: 69218.6719\n",
      "Epoch [100/100], Train Loss: 69349.8047, Val Loss: 69195.2422\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 61850.8320, Val Loss: 60573.6914\n",
      "Epoch [20/100], Train Loss: 52366.5938, Val Loss: 51226.7422\n",
      "Epoch [30/100], Train Loss: 44920.0547, Val Loss: 43874.0430\n",
      "Epoch [40/100], Train Loss: 38954.0312, Val Loss: 38047.9453\n",
      "Epoch [50/100], Train Loss: 34138.9141, Val Loss: 33274.5859\n",
      "Epoch [60/100], Train Loss: 29820.9883, Val Loss: 29134.0234\n",
      "Epoch [70/100], Train Loss: 26235.5352, Val Loss: 25667.3828\n",
      "Epoch [80/100], Train Loss: 23286.5391, Val Loss: 22838.0293\n",
      "Epoch [90/100], Train Loss: 20807.4746, Val Loss: 20441.6699\n",
      "Epoch [100/100], Train Loss: 18703.2051, Val Loss: 18403.3379\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 61773.8398, Val Loss: 60471.1328\n",
      "Epoch [20/100], Train Loss: 52258.5391, Val Loss: 51122.5586\n",
      "Epoch [30/100], Train Loss: 44851.6211, Val Loss: 43805.5078\n",
      "Epoch [40/100], Train Loss: 38903.1016, Val Loss: 37996.9062\n",
      "Epoch [50/100], Train Loss: 34118.9141, Val Loss: 33249.6875\n",
      "Epoch [60/100], Train Loss: 29787.6270, Val Loss: 29104.6758\n",
      "Epoch [70/100], Train Loss: 26193.9219, Val Loss: 25612.7266\n",
      "Epoch [80/100], Train Loss: 23258.1582, Val Loss: 22793.9727\n",
      "Epoch [90/100], Train Loss: 20792.7852, Val Loss: 20401.3281\n",
      "Epoch [100/100], Train Loss: 18701.7090, Val Loss: 18366.3496\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 61764.1133, Val Loss: 60488.2891\n",
      "Epoch [20/100], Train Loss: 52343.4844, Val Loss: 51204.5742\n",
      "Epoch [30/100], Train Loss: 44952.9141, Val Loss: 43915.8789\n",
      "Epoch [40/100], Train Loss: 39026.3477, Val Loss: 38117.4141\n",
      "Epoch [50/100], Train Loss: 34250.0352, Val Loss: 33393.6758\n",
      "Epoch [60/100], Train Loss: 29960.7129, Val Loss: 29273.7480\n",
      "Epoch [70/100], Train Loss: 26366.5469, Val Loss: 25800.1289\n",
      "Epoch [80/100], Train Loss: 23423.7188, Val Loss: 22962.8438\n",
      "Epoch [90/100], Train Loss: 20965.1934, Val Loss: 20569.1113\n",
      "Epoch [100/100], Train Loss: 18854.5508, Val Loss: 18529.7988\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69293.5781, Val Loss: 69089.2891\n",
      "Epoch [20/100], Train Loss: 68411.9375, Val Loss: 68120.5547\n",
      "Epoch [30/100], Train Loss: 66753.0781, Val Loss: 66431.0078\n",
      "Epoch [40/100], Train Loss: 64925.8984, Val Loss: 64615.4062\n",
      "Epoch [50/100], Train Loss: 63238.9844, Val Loss: 62917.8398\n",
      "Epoch [60/100], Train Loss: 61619.5312, Val Loss: 61265.5898\n",
      "Epoch [70/100], Train Loss: 60064.0391, Val Loss: 59697.6055\n",
      "Epoch [80/100], Train Loss: 58677.3555, Val Loss: 58306.5898\n",
      "Epoch [90/100], Train Loss: 57436.3789, Val Loss: 57059.9883\n",
      "Epoch [100/100], Train Loss: 56310.3203, Val Loss: 55923.9922\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69342.3125, Val Loss: 69142.4297\n",
      "Epoch [20/100], Train Loss: 68523.7500, Val Loss: 68238.0234\n",
      "Epoch [30/100], Train Loss: 66927.1250, Val Loss: 66603.1250\n",
      "Epoch [40/100], Train Loss: 65099.6602, Val Loss: 64780.0430\n",
      "Epoch [50/100], Train Loss: 63366.9688, Val Loss: 63035.9453\n",
      "Epoch [60/100], Train Loss: 61700.8906, Val Loss: 61343.7969\n",
      "Epoch [70/100], Train Loss: 60175.1523, Val Loss: 59810.5078\n",
      "Epoch [80/100], Train Loss: 58815.2422, Val Loss: 58453.0586\n",
      "Epoch [90/100], Train Loss: 57590.2344, Val Loss: 57231.5898\n",
      "Epoch [100/100], Train Loss: 56497.3320, Val Loss: 56130.1172\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69321.6562, Val Loss: 69117.9453\n",
      "Epoch [20/100], Train Loss: 68424.5625, Val Loss: 68131.0703\n",
      "Epoch [30/100], Train Loss: 66749.1094, Val Loss: 66423.3438\n",
      "Epoch [40/100], Train Loss: 64916.7461, Val Loss: 64601.5000\n",
      "Epoch [50/100], Train Loss: 63230.0312, Val Loss: 62904.3945\n",
      "Epoch [60/100], Train Loss: 61618.8281, Val Loss: 61259.1172\n",
      "Epoch [70/100], Train Loss: 60087.0703, Val Loss: 59719.2500\n",
      "Epoch [80/100], Train Loss: 58751.4531, Val Loss: 58383.2070\n",
      "Epoch [90/100], Train Loss: 57526.3672, Val Loss: 57158.6914\n",
      "Epoch [100/100], Train Loss: 56384.0820, Val Loss: 56009.5625\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69561.7656, Val Loss: 69407.8359\n",
      "Epoch [20/100], Train Loss: 69544.3516, Val Loss: 69390.0156\n",
      "Epoch [30/100], Train Loss: 69523.8672, Val Loss: 69369.0859\n",
      "Epoch [40/100], Train Loss: 69498.3281, Val Loss: 69342.8281\n",
      "Epoch [50/100], Train Loss: 69465.1328, Val Loss: 69308.6875\n",
      "Epoch [60/100], Train Loss: 69421.7266, Val Loss: 69263.7109\n",
      "Epoch [70/100], Train Loss: 69363.9219, Val Loss: 69204.2344\n",
      "Epoch [80/100], Train Loss: 69288.0469, Val Loss: 69126.0078\n",
      "Epoch [90/100], Train Loss: 69188.6562, Val Loss: 69024.5703\n",
      "Epoch [100/100], Train Loss: 69063.1875, Val Loss: 68895.7812\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69506.1719, Val Loss: 69351.4062\n",
      "Epoch [20/100], Train Loss: 69487.5625, Val Loss: 69332.3359\n",
      "Epoch [30/100], Train Loss: 69464.9922, Val Loss: 69309.3984\n",
      "Epoch [40/100], Train Loss: 69437.0312, Val Loss: 69280.3672\n",
      "Epoch [50/100], Train Loss: 69400.5625, Val Loss: 69242.7656\n",
      "Epoch [60/100], Train Loss: 69352.4609, Val Loss: 69193.6484\n",
      "Epoch [70/100], Train Loss: 69290.4297, Val Loss: 69129.4844\n",
      "Epoch [80/100], Train Loss: 69209.9297, Val Loss: 69046.1406\n",
      "Epoch [90/100], Train Loss: 69105.9844, Val Loss: 68939.2734\n",
      "Epoch [100/100], Train Loss: 68974.3594, Val Loss: 68804.8516\n",
      "Trying params: hidden_dim=256, num_layers=2, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69517.2969, Val Loss: 69363.1484\n",
      "Epoch [20/100], Train Loss: 69499.3516, Val Loss: 69344.8672\n",
      "Epoch [30/100], Train Loss: 69477.9766, Val Loss: 69323.2031\n",
      "Epoch [40/100], Train Loss: 69451.2578, Val Loss: 69295.8828\n",
      "Epoch [50/100], Train Loss: 69416.4531, Val Loss: 69260.2812\n",
      "Epoch [60/100], Train Loss: 69371.2344, Val Loss: 69213.3906\n",
      "Epoch [70/100], Train Loss: 69309.9609, Val Loss: 69151.6172\n",
      "Epoch [80/100], Train Loss: 69232.4922, Val Loss: 69070.8828\n",
      "Epoch [90/100], Train Loss: 69131.0781, Val Loss: 68966.8516\n",
      "Epoch [100/100], Train Loss: 69001.9531, Val Loss: 68835.4688\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 61102.0273, Val Loss: 59978.1016\n",
      "Epoch [20/100], Train Loss: 52900.3438, Val Loss: 51810.1523\n",
      "Epoch [30/100], Train Loss: 45844.0469, Val Loss: 44802.0156\n",
      "Epoch [40/100], Train Loss: 40090.5273, Val Loss: 39158.8789\n",
      "Epoch [50/100], Train Loss: 35693.7461, Val Loss: 34840.8555\n",
      "Epoch [60/100], Train Loss: 31671.9961, Val Loss: 30868.5371\n",
      "Epoch [70/100], Train Loss: 27848.3086, Val Loss: 27248.1328\n",
      "Epoch [80/100], Train Loss: 24723.9844, Val Loss: 24212.2598\n",
      "Epoch [90/100], Train Loss: 22133.6191, Val Loss: 21704.4668\n",
      "Epoch [100/100], Train Loss: 19900.2129, Val Loss: 19561.3887\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 60957.4336, Val Loss: 59807.5117\n",
      "Epoch [20/100], Train Loss: 52712.2109, Val Loss: 51626.8398\n",
      "Epoch [30/100], Train Loss: 45685.3633, Val Loss: 44649.2812\n",
      "Epoch [40/100], Train Loss: 39929.6797, Val Loss: 39012.0234\n",
      "Epoch [50/100], Train Loss: 35335.8125, Val Loss: 34493.7461\n",
      "Epoch [60/100], Train Loss: 30969.0664, Val Loss: 30254.5938\n",
      "Epoch [70/100], Train Loss: 27307.4922, Val Loss: 26723.0742\n",
      "Epoch [80/100], Train Loss: 24310.1016, Val Loss: 23799.7051\n",
      "Epoch [90/100], Train Loss: 21773.6855, Val Loss: 21383.1484\n",
      "Epoch [100/100], Train Loss: 19608.1309, Val Loss: 19272.7617\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 60826.3945, Val Loss: 59715.3438\n",
      "Epoch [20/100], Train Loss: 52663.5586, Val Loss: 51581.7891\n",
      "Epoch [30/100], Train Loss: 45621.6094, Val Loss: 44622.3945\n",
      "Epoch [40/100], Train Loss: 39973.4492, Val Loss: 39054.2422\n",
      "Epoch [50/100], Train Loss: 35576.5039, Val Loss: 34725.6250\n",
      "Epoch [60/100], Train Loss: 31307.6016, Val Loss: 30554.6719\n",
      "Epoch [70/100], Train Loss: 27628.3496, Val Loss: 27030.9277\n",
      "Epoch [80/100], Train Loss: 24607.7676, Val Loss: 24064.2402\n",
      "Epoch [90/100], Train Loss: 22053.8633, Val Loss: 21620.7402\n",
      "Epoch [100/100], Train Loss: 19869.1309, Val Loss: 19507.5820\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69208.5000, Val Loss: 68964.0859\n",
      "Epoch [20/100], Train Loss: 67339.3828, Val Loss: 66947.1406\n",
      "Epoch [30/100], Train Loss: 65123.3906, Val Loss: 64768.3906\n",
      "Epoch [40/100], Train Loss: 63404.6211, Val Loss: 63036.9727\n",
      "Epoch [50/100], Train Loss: 61995.1289, Val Loss: 61636.4766\n",
      "Epoch [60/100], Train Loss: 60783.7227, Val Loss: 60438.7617\n",
      "Epoch [70/100], Train Loss: 59695.3359, Val Loss: 59348.9766\n",
      "Epoch [80/100], Train Loss: 58676.5078, Val Loss: 58326.2539\n",
      "Epoch [90/100], Train Loss: 57717.7227, Val Loss: 57364.0000\n",
      "Epoch [100/100], Train Loss: 56802.6406, Val Loss: 56444.4727\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69266.4219, Val Loss: 69027.4609\n",
      "Epoch [20/100], Train Loss: 67551.1484, Val Loss: 67155.7031\n",
      "Epoch [30/100], Train Loss: 65348.0508, Val Loss: 64981.1250\n",
      "Epoch [40/100], Train Loss: 63618.6406, Val Loss: 63248.7656\n",
      "Epoch [50/100], Train Loss: 62192.7461, Val Loss: 61835.8867\n",
      "Epoch [60/100], Train Loss: 60968.9180, Val Loss: 60619.9023\n",
      "Epoch [70/100], Train Loss: 59856.1133, Val Loss: 59508.4805\n",
      "Epoch [80/100], Train Loss: 58824.8633, Val Loss: 58475.2188\n",
      "Epoch [90/100], Train Loss: 57857.3750, Val Loss: 57503.7188\n",
      "Epoch [100/100], Train Loss: 56936.1875, Val Loss: 56577.9688\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69272.0234, Val Loss: 69030.7578\n",
      "Epoch [20/100], Train Loss: 67465.5078, Val Loss: 67050.5703\n",
      "Epoch [30/100], Train Loss: 65182.6016, Val Loss: 64813.3594\n",
      "Epoch [40/100], Train Loss: 63440.8555, Val Loss: 63073.4453\n",
      "Epoch [50/100], Train Loss: 62030.5625, Val Loss: 61669.6914\n",
      "Epoch [60/100], Train Loss: 60801.1172, Val Loss: 60448.6680\n",
      "Epoch [70/100], Train Loss: 59692.4062, Val Loss: 59341.6875\n",
      "Epoch [80/100], Train Loss: 58664.8828, Val Loss: 58312.9180\n",
      "Epoch [90/100], Train Loss: 57697.0312, Val Loss: 57341.7227\n",
      "Epoch [100/100], Train Loss: 56780.2148, Val Loss: 56420.0781\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69551.4297, Val Loss: 69397.5547\n",
      "Epoch [20/100], Train Loss: 69534.5781, Val Loss: 69380.1797\n",
      "Epoch [30/100], Train Loss: 69512.7109, Val Loss: 69357.4375\n",
      "Epoch [40/100], Train Loss: 69481.1641, Val Loss: 69324.2656\n",
      "Epoch [50/100], Train Loss: 69432.1875, Val Loss: 69272.4141\n",
      "Epoch [60/100], Train Loss: 69352.8828, Val Loss: 69188.4375\n",
      "Epoch [70/100], Train Loss: 69222.8672, Val Loss: 69051.7422\n",
      "Epoch [80/100], Train Loss: 69016.5312, Val Loss: 68836.0938\n",
      "Epoch [90/100], Train Loss: 68712.4766, Val Loss: 68521.5312\n",
      "Epoch [100/100], Train Loss: 68313.8750, Val Loss: 68116.2656\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69558.1250, Val Loss: 69404.2188\n",
      "Epoch [20/100], Train Loss: 69540.8359, Val Loss: 69386.4922\n",
      "Epoch [30/100], Train Loss: 69518.8125, Val Loss: 69363.5859\n",
      "Epoch [40/100], Train Loss: 69487.2578, Val Loss: 69330.6641\n",
      "Epoch [50/100], Train Loss: 69438.6406, Val Loss: 69279.7266\n",
      "Epoch [60/100], Train Loss: 69361.0625, Val Loss: 69197.6328\n",
      "Epoch [70/100], Train Loss: 69233.6641, Val Loss: 69063.9844\n",
      "Epoch [80/100], Train Loss: 69031.6797, Val Loss: 68852.2656\n",
      "Epoch [90/100], Train Loss: 68732.0781, Val Loss: 68540.6875\n",
      "Epoch [100/100], Train Loss: 68334.7031, Val Loss: 68134.7031\n",
      "Trying params: hidden_dim=256, num_layers=3, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69517.6797, Val Loss: 69363.2812\n",
      "Epoch [20/100], Train Loss: 69499.7969, Val Loss: 69344.8750\n",
      "Epoch [30/100], Train Loss: 69476.6016, Val Loss: 69320.8438\n",
      "Epoch [40/100], Train Loss: 69443.0078, Val Loss: 69285.7344\n",
      "Epoch [50/100], Train Loss: 69390.7266, Val Loss: 69230.6562\n",
      "Epoch [60/100], Train Loss: 69306.4766, Val Loss: 69140.9219\n",
      "Epoch [70/100], Train Loss: 69167.7109, Val Loss: 68994.0234\n",
      "Epoch [80/100], Train Loss: 68946.2734, Val Loss: 68761.4219\n",
      "Epoch [90/100], Train Loss: 68618.7578, Val Loss: 68421.9062\n",
      "Epoch [100/100], Train Loss: 68194.6016, Val Loss: 67986.1016\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.01, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 60582.1172, Val Loss: 59533.3477\n",
      "Epoch [20/100], Train Loss: 52711.9648, Val Loss: 51657.5898\n",
      "Epoch [30/100], Train Loss: 46025.3008, Val Loss: 45018.1992\n",
      "Epoch [40/100], Train Loss: 40801.9961, Val Loss: 39847.2500\n",
      "Epoch [50/100], Train Loss: 36791.0195, Val Loss: 35776.1992\n",
      "Epoch [60/100], Train Loss: 32921.3359, Val Loss: 32176.7188\n",
      "Epoch [70/100], Train Loss: 29461.8125, Val Loss: 28741.2500\n",
      "Epoch [80/100], Train Loss: 26098.1270, Val Loss: 25542.9844\n",
      "Epoch [90/100], Train Loss: 23307.6113, Val Loss: 22770.5137\n",
      "Epoch [100/100], Train Loss: 20907.0273, Val Loss: 20497.2773\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.01, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 60548.7773, Val Loss: 59469.5039\n",
      "Epoch [20/100], Train Loss: 52586.3711, Val Loss: 51526.3398\n",
      "Epoch [30/100], Train Loss: 45883.0859, Val Loss: 44846.7266\n",
      "Epoch [40/100], Train Loss: 40561.3477, Val Loss: 39597.3359\n",
      "Epoch [50/100], Train Loss: 36361.9219, Val Loss: 35396.1758\n",
      "Epoch [60/100], Train Loss: 32627.7891, Val Loss: 31834.1309\n",
      "Epoch [70/100], Train Loss: 28992.5586, Val Loss: 28228.9102\n",
      "Epoch [80/100], Train Loss: 25878.8164, Val Loss: 25252.6914\n",
      "Epoch [90/100], Train Loss: 23141.4062, Val Loss: 22666.6875\n",
      "Epoch [100/100], Train Loss: 20776.4941, Val Loss: 20331.2441\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.01, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 60831.5391, Val Loss: 59763.2734\n",
      "Epoch [20/100], Train Loss: 52874.6875, Val Loss: 51821.7578\n",
      "Epoch [30/100], Train Loss: 46126.3008, Val Loss: 45109.5977\n",
      "Epoch [40/100], Train Loss: 40846.9922, Val Loss: 39935.6172\n",
      "Epoch [50/100], Train Loss: 36895.5391, Val Loss: 36002.7930\n",
      "Epoch [60/100], Train Loss: 33952.8086, Val Loss: 32980.6211\n",
      "Epoch [70/100], Train Loss: 30459.8711, Val Loss: 29668.3145\n",
      "Epoch [80/100], Train Loss: 26434.3477, Val Loss: 25838.0234\n",
      "Epoch [90/100], Train Loss: 23577.5449, Val Loss: 23031.4082\n",
      "Epoch [100/100], Train Loss: 21072.8105, Val Loss: 20644.4844\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69177.3281, Val Loss: 68892.1875\n",
      "Epoch [20/100], Train Loss: 66638.2969, Val Loss: 66218.6328\n",
      "Epoch [30/100], Train Loss: 64523.9297, Val Loss: 64158.5625\n",
      "Epoch [40/100], Train Loss: 63121.6836, Val Loss: 62789.3086\n",
      "Epoch [50/100], Train Loss: 61970.8516, Val Loss: 61642.2070\n",
      "Epoch [60/100], Train Loss: 60911.6016, Val Loss: 60581.0898\n",
      "Epoch [70/100], Train Loss: 59909.7617, Val Loss: 59572.0859\n",
      "Epoch [80/100], Train Loss: 58952.2500, Val Loss: 58611.0273\n",
      "Epoch [90/100], Train Loss: 58034.3086, Val Loss: 57687.4492\n",
      "Epoch [100/100], Train Loss: 57150.6172, Val Loss: 56800.9766\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69157.8047, Val Loss: 68863.0703\n",
      "Epoch [20/100], Train Loss: 66611.0156, Val Loss: 66184.4609\n",
      "Epoch [30/100], Train Loss: 64504.5078, Val Loss: 64137.9766\n",
      "Epoch [40/100], Train Loss: 63101.2773, Val Loss: 62766.8047\n",
      "Epoch [50/100], Train Loss: 61942.4805, Val Loss: 61611.9688\n",
      "Epoch [60/100], Train Loss: 60879.5352, Val Loss: 60547.4883\n",
      "Epoch [70/100], Train Loss: 59876.5469, Val Loss: 59539.2148\n",
      "Epoch [80/100], Train Loss: 58919.0938, Val Loss: 58577.0586\n",
      "Epoch [90/100], Train Loss: 58000.5781, Val Loss: 57652.8672\n",
      "Epoch [100/100], Train Loss: 57117.8594, Val Loss: 56766.8164\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69150.8047, Val Loss: 68852.2500\n",
      "Epoch [20/100], Train Loss: 66618.7031, Val Loss: 66186.5625\n",
      "Epoch [30/100], Train Loss: 64557.7812, Val Loss: 64193.7969\n",
      "Epoch [40/100], Train Loss: 63184.2383, Val Loss: 62850.5195\n",
      "Epoch [50/100], Train Loss: 62039.3516, Val Loss: 61710.3906\n",
      "Epoch [60/100], Train Loss: 60982.9844, Val Loss: 60651.5820\n",
      "Epoch [70/100], Train Loss: 59976.7539, Val Loss: 59640.5898\n",
      "Epoch [80/100], Train Loss: 59017.3320, Val Loss: 58676.9141\n",
      "Epoch [90/100], Train Loss: 58098.4961, Val Loss: 57752.1406\n",
      "Epoch [100/100], Train Loss: 57213.9102, Val Loss: 56864.4375\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.0001, dropout=0.1\n",
      "Epoch [10/100], Train Loss: 69528.9609, Val Loss: 69374.7422\n",
      "Epoch [20/100], Train Loss: 69510.8125, Val Loss: 69356.0156\n",
      "Epoch [30/100], Train Loss: 69486.0781, Val Loss: 69330.0703\n",
      "Epoch [40/100], Train Loss: 69446.5391, Val Loss: 69287.9297\n",
      "Epoch [50/100], Train Loss: 69374.7969, Val Loss: 69210.8438\n",
      "Epoch [60/100], Train Loss: 69234.8203, Val Loss: 69059.9219\n",
      "Epoch [70/100], Train Loss: 68959.4297, Val Loss: 68764.4375\n",
      "Epoch [80/100], Train Loss: 68473.0938, Val Loss: 68252.9922\n",
      "Epoch [90/100], Train Loss: 67809.2266, Val Loss: 67580.0547\n",
      "Epoch [100/100], Train Loss: 67137.2578, Val Loss: 66917.1641\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.0001, dropout=0.2\n",
      "Epoch [10/100], Train Loss: 69511.9141, Val Loss: 69357.6172\n",
      "Epoch [20/100], Train Loss: 69494.7969, Val Loss: 69339.9531\n",
      "Epoch [30/100], Train Loss: 69471.7891, Val Loss: 69315.8047\n",
      "Epoch [40/100], Train Loss: 69435.1328, Val Loss: 69276.7188\n",
      "Epoch [50/100], Train Loss: 69368.5781, Val Loss: 69204.8281\n",
      "Epoch [60/100], Train Loss: 69237.5391, Val Loss: 69062.5781\n",
      "Epoch [70/100], Train Loss: 68977.7656, Val Loss: 68782.2188\n",
      "Epoch [80/100], Train Loss: 68519.8203, Val Loss: 68298.9375\n",
      "Epoch [90/100], Train Loss: 67897.4844, Val Loss: 67668.0156\n",
      "Epoch [100/100], Train Loss: 67270.6172, Val Loss: 67046.0312\n",
      "Trying params: hidden_dim=256, num_layers=4, lr=0.0001, dropout=0.3\n",
      "Epoch [10/100], Train Loss: 69533.0547, Val Loss: 69378.9141\n",
      "Epoch [20/100], Train Loss: 69515.5859, Val Loss: 69360.8750\n",
      "Epoch [30/100], Train Loss: 69492.1250, Val Loss: 69336.4609\n",
      "Epoch [40/100], Train Loss: 69456.0781, Val Loss: 69297.9844\n",
      "Epoch [50/100], Train Loss: 69391.9375, Val Loss: 69229.0547\n",
      "Epoch [60/100], Train Loss: 69268.6719, Val Loss: 69095.4922\n",
      "Epoch [70/100], Train Loss: 69025.3359, Val Loss: 68833.9531\n",
      "Epoch [80/100], Train Loss: 68590.0078, Val Loss: 68373.5312\n",
      "Epoch [90/100], Train Loss: 67979.9453, Val Loss: 67745.7109\n",
      "Epoch [100/100], Train Loss: 67337.4531, Val Loss: 67103.6875\n",
      "Best Parameters for GRU: {'hidden_dim': 256, 'num_layers': 1, 'learning_rate': 0.01, 'dropout': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Parameters for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'hidden_dim': [32, 64, 128, 256],\n",
    "    'num_layers': [1, 2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'dropout': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Tuning RNN Model\n",
    "print(\"Tuning RNN Model\")\n",
    "best_params_rnn = search_model(param_dist, RNNNet)\n",
    "print(f'Best Parameters for RNN: {best_params_rnn}')\n",
    "\n",
    "# Tuning LSTM Model\n",
    "print(\"Tuning LSTM Model\")\n",
    "best_params_lstm = search_model(param_dist, LSTMNet)\n",
    "print(f'Best Parameters for LSTM: {best_params_lstm}')\n",
    "\n",
    "# Tuning GRU Model\n",
    "print(\"Tuning GRU Model\")\n",
    "best_params_gru = search_model(param_dist, GRUNet)\n",
    "print(f'Best Parameters for GRU: {best_params_gru}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the models with the best parameters\n",
    "def retrain_best_model(model_class, best_params):\n",
    "    best_hidden_dim = best_params['hidden_dim']\n",
    "    best_num_layers = best_params['num_layers']\n",
    "    best_lr = best_params['learning_rate']\n",
    "    best_dropout = best_params['dropout']\n",
    "\n",
    "    model = model_class(input_dim, best_hidden_dim, best_num_layers, best_dropout).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train.to(device))\n",
    "        loss = criterion(outputs, y_train.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_outputs = model(X_val.to(device))\n",
    "        val_loss = criterion(val_outputs, y_val.to(device))\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining best RNN model\n",
      "Epoch [10/100], Train Loss: 65007.6562, Val Loss: 64180.5000\n",
      "Epoch [20/100], Train Loss: 57163.1562, Val Loss: 56160.4141\n",
      "Epoch [30/100], Train Loss: 48955.7031, Val Loss: 47835.1367\n",
      "Epoch [40/100], Train Loss: 41478.4961, Val Loss: 40387.7695\n",
      "Epoch [50/100], Train Loss: 34715.9648, Val Loss: 33663.4805\n",
      "Epoch [60/100], Train Loss: 29157.6738, Val Loss: 28284.6738\n",
      "Epoch [70/100], Train Loss: 25178.2090, Val Loss: 24463.9590\n",
      "Epoch [80/100], Train Loss: 22019.0273, Val Loss: 21431.4180\n",
      "Epoch [90/100], Train Loss: 19475.5996, Val Loss: 18988.7266\n",
      "Epoch [100/100], Train Loss: 17395.1680, Val Loss: 16992.8320\n",
      "Retraining best LSTM model\n",
      "Epoch [10/100], Train Loss: 67423.3828, Val Loss: 66842.7812\n",
      "Epoch [20/100], Train Loss: 61861.5664, Val Loss: 61082.2422\n",
      "Epoch [30/100], Train Loss: 55008.5703, Val Loss: 54198.4609\n",
      "Epoch [40/100], Train Loss: 48015.2070, Val Loss: 47218.6562\n",
      "Epoch [50/100], Train Loss: 41412.2734, Val Loss: 40627.1719\n",
      "Epoch [60/100], Train Loss: 35584.7266, Val Loss: 34821.2773\n",
      "Epoch [70/100], Train Loss: 30839.9102, Val Loss: 30139.3750\n",
      "Epoch [80/100], Train Loss: 27214.6680, Val Loss: 26593.4414\n",
      "Epoch [90/100], Train Loss: 24355.4219, Val Loss: 23815.8770\n",
      "Epoch [100/100], Train Loss: 21922.5078, Val Loss: 21457.3809\n",
      "Retraining best GRU model\n",
      "Epoch [10/100], Train Loss: 66480.6953, Val Loss: 65759.6641\n",
      "Epoch [20/100], Train Loss: 59144.2617, Val Loss: 58218.1367\n",
      "Epoch [30/100], Train Loss: 50525.6758, Val Loss: 49560.0820\n",
      "Epoch [40/100], Train Loss: 42039.8008, Val Loss: 41075.4141\n",
      "Epoch [50/100], Train Loss: 34439.3867, Val Loss: 33533.2930\n",
      "Epoch [60/100], Train Loss: 28487.4922, Val Loss: 27709.8633\n",
      "Epoch [70/100], Train Loss: 24204.6973, Val Loss: 23558.0664\n",
      "Epoch [80/100], Train Loss: 20921.1172, Val Loss: 20394.9609\n",
      "Epoch [90/100], Train Loss: 18266.1875, Val Loss: 17817.1816\n",
      "Epoch [100/100], Train Loss: 16134.8291, Val Loss: 15767.0684\n"
     ]
    }
   ],
   "source": [
    "# Retrain the best models\n",
    "print(\"Retraining best RNN model\")\n",
    "best_rnn_model, rnn_train_losses, rnn_val_losses = retrain_best_model(RNNNet, best_params_rnn)\n",
    "\n",
    "print(\"Retraining best LSTM model\")\n",
    "best_lstm_model, lstm_train_losses, lstm_val_losses = retrain_best_model(LSTMNet, best_params_lstm)\n",
    "\n",
    "print(\"Retraining best GRU model\")\n",
    "best_gru_model, gru_train_losses, gru_val_losses = retrain_best_model(GRUNet, best_params_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting training and validation loss for each model\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(rnn_train_losses, label='Train Loss')\n",
    "plt.plot(rnn_val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('RNN Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(lstm_train_losses, label='Train Loss')\n",
    "plt.plot(lstm_val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('LSTM Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(gru_train_losses, label='Train Loss')\n",
    "plt.plot(gru_val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('GRU Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Test Loss: 16094.8994, MAE: 70.8875, RMSE: 126.8657, R-squared: 0.6819\n",
      "LSTM Test Loss: 20302.3203, MAE: 81.7619, RMSE: 142.4862, R-squared: 0.6489\n",
      "GRU Test Loss: 14917.9980, MAE: 65.8543, RMSE: 122.1393, R-squared: 0.6256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\Downloads\\Documents\\Bike Sharing ANN\\ann\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Bike Sharing ANN\\ann\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Bike Sharing ANN\\ann\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Define the MAE and RMSE calculation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_loss = criterion(test_outputs, y_test.to(device))\n",
    "        \n",
    "        # Convert outputs and targets to lists and then to numpy arrays\n",
    "        test_outputs_list = test_outputs.cpu().detach().tolist()\n",
    "        y_test_list = y_test.cpu().detach().tolist()\n",
    "        \n",
    "        # Convert lists to numpy arrays\n",
    "        test_outputs_np = np.array(test_outputs_list)\n",
    "        y_test_np = np.array(y_test_list)\n",
    "        \n",
    "        # Calculate MAE and RMSE\n",
    "        mae = mean_absolute_error(y_test_np, test_outputs_np)\n",
    "        rmse = mean_squared_error(y_test_np, test_outputs_np, squared=False)\n",
    "        \n",
    "        # Calculate R-squared\n",
    "        total_variance = ((y_test_np - y_test_np.mean())**2).sum()\n",
    "        explained_variance = ((test_outputs_np - y_test_np.mean())**2).sum()\n",
    "        r2_score = 1 - (explained_variance / total_variance)\n",
    "    \n",
    "    return test_loss.item(), mae, rmse, r2_score.item()\n",
    "\n",
    "# Evaluate all models\n",
    "rnn_test_loss, rnn_mae, rnn_rmse, rnn_r2 = evaluate_model(best_rnn_model, X_test, y_test)\n",
    "lstm_test_loss, lstm_mae, lstm_rmse, lstm_r2 = evaluate_model(best_lstm_model, X_test, y_test)\n",
    "gru_test_loss, gru_mae, gru_rmse, gru_r2 = evaluate_model(best_gru_model, X_test, y_test)\n",
    "\n",
    "print(f'RNN Test Loss: {rnn_test_loss:.4f}, MAE: {rnn_mae:.4f}, RMSE: {rnn_rmse:.4f}, R-squared: {rnn_r2:.4f}')\n",
    "print(f'LSTM Test Loss: {lstm_test_loss:.4f}, MAE: {lstm_mae:.4f}, RMSE: {lstm_rmse:.4f}, R-squared: {lstm_r2:.4f}')\n",
    "print(f'GRU Test Loss: {gru_test_loss:.4f}, MAE: {gru_mae:.4f}, RMSE: {gru_rmse:.4f}, R-squared: {gru_r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
